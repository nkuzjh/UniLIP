import os
import json
import random
import logging
import numpy as np
from PIL import Image
import copy
from typing import Dict, Sequence
from dataclasses import dataclass

import torch
import torch.nn as nn
from torch.utils.data import Dataset
from torchvision import transforms
from torchvision.transforms import InterpolationMode
import transformers

from unilip import conversation as conversation_lib
from unilip.mm_utils import tokenizer_image_token
from unilip.constants import IGNORE_INDEX, DEFAULT_IMAGE_TOKEN, IMAGE_TOKEN_IDX

from csgo_datasets.utils import CoarseDropout, GridDropout
from csgo_datasets.random_erasing import RandomErasing



IMG_START_TOKEN = '<img>'
IMG_END_TOKEN = '</img>'
IMG_CONTEXT_TOKEN = '<IMG_CONTEXT>'



# ä»»åŠ¡ A: å®šä½ (Map + FPS -> Pose)
# ==========================================
# A.1 æç¤ºè¯æ¨¡æ¿ (Prompt Templates)
# ==========================================
def get_loc_prompt(map_name):
    LOC_PROMPT_TEMPLATE = (
        f"Task: The following visual data of CS2 map '{map_name}' has been fused and inserted into this sequence:\n"
        "1. First-Person View (FPV) Features.\n"
        "2. Overhead Radar Map (RADAR) Features.\n"
        "Analyze the spatial relationship between the FPV and the RADAR Map to determine the precise camera pose. "
        "Predict the 5D pose (x, y, z, pitch, yaw) in the required format.\n"
        "<image>\n<image>"
    )
    return LOC_PROMPT_TEMPLATE



# ä»»åŠ¡ B: ç”Ÿæˆ (Map + Pose -> FPS)
# ==========================================
# B.1 è¾…åŠ©å‡½æ•°: å›¾åƒ Padding (å¤ç”¨ UniLIP é€»è¾‘)
# ==========================================
def expand2square(pil_img, background_color):
    """å°†å›¾ç‰‡å¡«å……ä¸ºæ­£æ–¹å½¢ï¼ŒèƒŒæ™¯è‰²é€šå¸¸ä¸º image_mean"""
    width, height = pil_img.size
    if width == height:
        return pil_img
    elif width > height:
        result = Image.new(pil_img.mode, (width, width), background_color)
        result.paste(pil_img, (0, (width - height) // 2))
        return result
    else:
        result = Image.new(pil_img.mode, (height, height), background_color)
        result.paste(pil_img, ((height - width) // 2, 0))
        return result

def img_process(images, processor, image_aspect_ratio):
    """è°ƒç”¨ HuggingFace processor å¤„ç†å›¾åƒåˆ—è¡¨"""
    if image_aspect_ratio == "pad":
        # è®¡ç®—å¡«å……èƒŒæ™¯è‰² (åŸºäº processor çš„å‡å€¼)
        background_color = tuple(int(x * 255) for x in processor.image_mean)
        images = [expand2square(img, background_color) for img in images]
        # è½¬æ¢ä¸º Tensor
        images = processor.preprocess(images, return_tensors="pt")["pixel_values"]
    else:
        images = processor.preprocess(images, return_tensors="pt")["pixel_values"]
    return images

# ==========================================
# B.2 Prompt æ„å»ºå‡½æ•°
# ==========================================
def build_sft_instruction_custom(pose_5d, map_name, z_max, z_min):
    definition_text = (
        f"Task: Generate a First-Person View (FPV) image of CS2 map '{map_name}' based on the Radar Map and Camera Pose.\n"
        "Coordinate System Definition:\n"
        "- Map Size: 1024x1024 pixels.\n"
        "- Yaw: 0 degrees is East, increases Clockwise.\n"
        "- Pitch: 0 degrees is looking straight Down (at feet), 180 degrees is looking straight Up (at sky).\n"
        f"- Z-Height: Absolute vertical coordinate. Valid values are bounded by the map's global topology, ranging from the lowest point at {z_min:.2f} to the highest point at {z_max:.2f}."
    )
    # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨äº†ä¼ å…¥å­—å…¸çš„ keyï¼Œç¡®ä¿å’Œä½ ä¸‹é¢çš„ __getitem__ æ„é€ ä¸€è‡´
    pose_str = (
        f"Position(x={pose_5d['x']:.1f}, y={pose_5d['y']:.1f}, z={pose_5d['z']:.3f}), "
        f"Rotation(pitch={pose_5d['angle_v']:.1f}, yaw={pose_5d['angle_h']:.1f})"
    )
    full_instruction = f"{definition_text}\n\nCurrent Camera Pose: {pose_str}\n<image>"
    return full_instruction

def preprocess_multimodal(sources: Sequence[str]) -> Dict:
    # NOTE: default to 256 tokens for 448x448
    und_placeholder = f'{IMG_START_TOKEN}{IMG_CONTEXT_TOKEN * 256}{IMG_END_TOKEN}'
    gen_placeholder = ""
    # "[IMG]" + "<image>" * data_args.n_query + "[/IMG]"
    inst_type = None
    for source in sources:  # [instance]
        for sentence in source:
            if sentence["from"] == "human" and "<image>" in sentence["value"]:
                sentence["value"] = sentence["value"].replace(DEFAULT_IMAGE_TOKEN, und_placeholder).strip()
                inst_type = "und"
            elif sentence["from"] == "gpt" and "<image>" in sentence["value"]:
                sentence["value"] = sentence["value"].replace(DEFAULT_IMAGE_TOKEN, gen_placeholder).strip()
                inst_type = "gen"
    return sources, inst_type

def _add_speaker_and_signal(header, source, get_conversation=True):
    """Add speaker and start/end signal on each round."""
    BEGIN_SIGNAL = "### "
    END_SIGNAL = "\n"
    conversation = header
    for sentence in source:
        from_str = sentence["from"]
        if from_str.lower() == "human":
            from_str = conversation_lib.default_conversation.roles[0]
        elif from_str.lower() == "gpt":
            from_str = conversation_lib.default_conversation.roles[1]
        else:
            from_str = "unknown"
        sentence["value"] = BEGIN_SIGNAL + from_str + ": " + sentence["value"] + END_SIGNAL
        if get_conversation:
            conversation += sentence["value"]
    conversation += BEGIN_SIGNAL
    return conversation

def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:
    """Tokenize a list of strings."""
    tokenized_list = [
        tokenizer(
            text,
            return_tensors="pt",
            padding="longest",
            max_length=tokenizer.model_max_length,
            truncation=True,
        )
        for text in strings
    ]
    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]
    input_ids_lens = labels_lens = [tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list]
    return dict(
        input_ids=input_ids,
        labels=labels,
        input_ids_lens=input_ids_lens,
        labels_lens=labels_lens,
    )

def _mask_targets(target, tokenized_lens, speakers):
    # cur_idx = 0
    cur_idx = tokenized_lens[0]
    tokenized_lens = tokenized_lens[1:]
    target[:cur_idx] = IGNORE_INDEX
    for tokenized_len, speaker in zip(tokenized_lens, speakers):
        if speaker == "human":
            target[cur_idx + 2 : cur_idx + tokenized_len] = IGNORE_INDEX
        cur_idx += tokenized_len

def preprocess_internvl(sources, tokenizer: transformers.PreTrainedTokenizer, has_image: bool = False, max_len=2048, system_message: str = "ä½ æ˜¯ä¹¦ç”ŸÂ·ä¸‡è±¡ï¼Œè‹±æ–‡åæ˜¯InternVLï¼Œæ˜¯ç”±ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤ã€æ¸…åå¤§å­¦åŠå¤šå®¶åˆä½œå•ä½è”åˆå¼€å‘çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€‚") -> Dict:
    roles = {"human": "user", "gpt": "assistant"}

    tokenizer = copy.deepcopy(tokenizer)
    chat_template = "{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n'}}{% if message['content'] is string %}{{ message['content'] }}{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' %}{{ '<IMG_CONTEXT>\n' }}{% elif content['type'] == 'video' %}{{ '<video>\n' }}{% elif content['type'] == 'text' %}{{ content['text'] }}{% endif %}{% endfor %}{% endif %}{{'<|im_end|>\n'}}{% endfor %}{% if add_generation_prompt %}{{'<|im_start|>assistant\n' }}{% endif %}"
    tokenizer.chat_template = chat_template

    # Apply prompt templates
    input_ids, targets = [], []
    for i, source in enumerate(sources):
        if roles[source[0]["from"]] != roles["human"]:
            source = source[1:]

        input_id, target = [], []

        # New version, use apply chat template
        # Build system message for each sentence
        # input_id += tokenizer.apply_chat_template([{"role" : "system", "content" : system_message}])
        target += [IGNORE_INDEX] * len(input_id)

        for conv in source:
            try:
                role = conv["role"]
                content = conv["content"]
            except:
                role = conv["from"]
                content = conv["value"]

            role =  roles.get(role, role)

            conv = [{"role" : role, "content" : content}]
            encode_id = tokenizer.apply_chat_template(conv)
            input_id += encode_id
            if role in ["user", "system"]:
                target += [IGNORE_INDEX] * len(encode_id)
            else:
                target += encode_id



        assert len(input_id) == len(target), f"{len(input_id)} != {len(target)}"

        input_ids.append(input_id)
        targets.append(target)
    input_ids = torch.tensor(input_ids, dtype=torch.long)
    targets = torch.tensor(targets, dtype=torch.long)

    input_ids = input_ids[:, :-2]
    targets = targets[:, :-2]
    return dict(
        input_ids=input_ids,  # tensor(bs x seq_len)
        labels=targets,  # tensor(bs x seq_len)
    )


def preprocess_qwen(sources, tokenizer: transformers.PreTrainedTokenizer, has_image: bool = False, max_len=2048, system_message: str = "You are a helpful assistant.") -> Dict:
    roles = {"human": "user", "gpt": "assistant"}

    tokenizer = copy.deepcopy(tokenizer)
    chat_template = "{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}"
    tokenizer.chat_template = chat_template

    # Apply prompt templates
    input_ids, targets = [], []
    for i, source in enumerate(sources):
        if roles[source[0]["from"]] != roles["human"]:
            source = source[1:]

        input_id, target = [], []

        # New version, use apply chat template
        # Build system message for each sentence
        input_id += tokenizer.apply_chat_template([{"role" : "system", "content" : system_message}])
        target += [IGNORE_INDEX] * len(input_id)

        for conv in source:
            try:
                role = conv["role"]
                content = conv["content"]
            except:
                role = conv["from"]
                content = conv["value"]

            role =  roles.get(role, role)

            conv = [{"role" : role, "content" : content}]
            encode_id = tokenizer.apply_chat_template(conv)
            input_id += encode_id
            if role in ["user", "system"]:
                target += [IGNORE_INDEX] * len(encode_id)
            else:
                target += encode_id



        assert len(input_id) == len(target), f"{len(input_id)} != {len(target)}"

        input_ids.append(input_id)
        targets.append(target)
    input_ids = torch.tensor(input_ids, dtype=torch.long)
    targets = torch.tensor(targets, dtype=torch.long)

    return dict(
        input_ids=input_ids,  # tensor(bs x seq_len)
        labels=targets,  # tensor(bs x seq_len)
    )




def preprocess_llama3(
    sources,
    tokenizer: transformers.PreTrainedTokenizer,
    has_image: bool = False,
    max_len=2048,
    system_message: str = "You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.",
) -> Dict:
    # roles = {"human": "<|start_header_id|>user<|end_header_id|>", "gpt": "<|start_header_id|>assistant<|end_header_id|>"}
    roles = {"human": "user", "gpt": "assistant"}

    # Add image tokens to tokenizer as a special tokens
    # Use a deepcopy of tokenizer so that we don't modify on the tokenizer
    tokenizer = copy.deepcopy(tokenizer)
    # When there is actually an image, we add the image tokens as a special token
    if has_image:
        tokenizer.add_tokens(["<image>"], special_tokens=True)
    image_token_index = tokenizer.convert_tokens_to_ids("<image>")
    bos_token_id = tokenizer.convert_tokens_to_ids("<|begin_of_text|>")
    start_header_id = tokenizer.convert_tokens_to_ids("<|start_header_id|>")
    end_header_id = tokenizer.convert_tokens_to_ids("<|end_header_id|>")
    eot_id = tokenizer.convert_tokens_to_ids("<|eot_id|>")

    unmask_tokens = ["<|begin_of_text|>", "<|start_header_id|>", "<|end_header_id|>", "<|eot_id|>", "\n\n"]
    unmask_tokens_idx = [tokenizer.convert_tokens_to_ids(tok) for tok in unmask_tokens]

    # After update, calling tokenizer of llama3 will
    # auto add bos id for the tokens. ãƒ½(ï½€âŒ’Â´)ï¾‰
    def safe_tokenizer_llama3(text):
        input_ids = tokenizer(text).input_ids
        if input_ids[0] == bos_token_id:
            input_ids = input_ids[1:]
        return input_ids

    nl_tokens = tokenizer.convert_tokens_to_ids("\n\n")
    # Apply prompt templates
    input_ids, targets = [], []
    for i, source in enumerate(sources):
        if roles[source[0]["from"]] != roles["human"]:
            source = source[1:]

        input_id, target = [], []

        # New version, use apply chat template
        # Build system message for each sentence
        input_id += tokenizer.apply_chat_template([{"role" : "system", "content" : system_message}])
        target += [IGNORE_INDEX] * len(input_id)

        for conv in source:
            try:
                role = conv["role"]
                content = conv["content"]
            except:
                role = conv["from"]
                content = conv["value"]

            role =  roles.get(role, role)

            conv = [{"role" : role, "content" : content}]
            # First is bos token we don't need here
            encode_id = tokenizer.apply_chat_template(conv)[1:]
            input_id += encode_id
            if role in ["user", "system"]:
                target += [IGNORE_INDEX] * len(encode_id)
            else:
                target += encode_id



        assert len(input_id) == len(target), f"{len(input_id)} != {len(target)}"
        for idx, encode_id in enumerate(input_id):
            if encode_id in unmask_tokens_idx:
                target[idx] = encode_id
            if encode_id == image_token_index:
                input_id[idx] = IMAGE_TOKEN_INDEX
        input_ids.append(input_id)
        targets.append(target)
    input_ids = torch.tensor(input_ids, dtype=torch.long)
    targets = torch.tensor(targets, dtype=torch.long)

    return dict(
        input_ids=input_ids,  # tensor(bs x seq_len)
        labels=targets,  # tensor(bs x seq_len)
    )



def preprocess_plain(
    sources: Sequence[str],
    tokenizer: transformers.PreTrainedTokenizer,
) -> Dict:
    # add end signal and concatenate together
    conversations = []
    for source in sources:
        assert len(source) == 2
        # assert DEFAULT_IMAGE_TOKEN in source[0]['value'] or DEFAULT_IMAGE_TOKEN in source[1]['value']
        conversation = source[0]["value"] + source[1]["value"] + conversation_lib.default_conversation.sep
        conversations.append(conversation)
    # tokenize conversations
    input_ids = [tokenizer_image_token(prompt, tokenizer, return_tensors="pt") for prompt in conversations]
    targets = copy.deepcopy(input_ids)
    for target, source in zip(targets, sources):
        tokenized_len = len(tokenizer_image_token(source[0]["value"], tokenizer))
        target[:tokenized_len] = IGNORE_INDEX

    return dict(input_ids=input_ids, labels=targets)

def preprocess(
    sources: Sequence[str],
    tokenizer: transformers.PreTrainedTokenizer,
    has_image: bool = False,
) -> Dict:
    """
    Given a list of sources, each is a conversation list. This transform:
    1. Add signal '### ' at the beginning each sentence, with end signal '\n';
    2. Concatenate conversations together;
    3. Tokenize the concatenated conversation;
    4. Make a deepcopy as the target. Mask human words with IGNORE_INDEX.
    """
    if conversation_lib.default_conversation.sep_style == conversation_lib.SeparatorStyle.PLAIN:
        return preprocess_plain(sources, tokenizer)
    if conversation_lib.default_conversation.version == "llama3":
        return preprocess_llama3(sources, tokenizer, has_image=has_image)
    if conversation_lib.default_conversation.version == "qwen":
        return preprocess_qwen(sources, tokenizer, has_image=has_image)
    if conversation_lib.default_conversation.version == "internvl":
        return preprocess_internvl(sources, tokenizer, has_image=has_image)
    # add end signal and concatenate together
    conversations = []
    for source in sources:
        header = f"{conversation_lib.default_conversation.system}\n\n"
        conversation = _add_speaker_and_signal(header, source)
        conversations.append(conversation)
    logging.info(f"conversations: {conversations}")
    # tokenize conversations
    def get_tokenize_len(prompts):
        return [len(tokenizer_image_token(prompt, tokenizer)) for prompt in prompts]

    if has_image:
        input_ids = [tokenizer_image_token(prompt, tokenizer, return_tensors="pt") for prompt in conversations]
    else:
        conversations_tokenized = _tokenize_fn(conversations, tokenizer)
        input_ids = conversations_tokenized["input_ids"]

    targets = copy.deepcopy(input_ids)
    for target, source in zip(targets, sources):
        if has_image:
            tokenized_lens = get_tokenize_len([header] + [s["value"] for s in source])
        else:
            tokenized_lens = _tokenize_fn([header] + [s["value"] for s in source], tokenizer)["input_ids_lens"]
        speakers = [sentence["from"] for sentence in source]
        _mask_targets(target, tokenized_lens, speakers)

    return dict(input_ids=input_ids, labels=targets)



# ==========================================
# C æ•°æ®é›†ä¿¡æ¯
# ==========================================
map_path_dict = {
    'de_dust2': 'de_dust2_radar_psd.png',
    'de_inferno': 'de_inferno_radar_psd.png',
    'de_mirage': 'de_mirage_radar_psd.png',
    'de_nuke': 'de_nuke_blended_radar_psd.png',
    'de_ancient': 'de_ancient_radar_psd.png',
    'de_anubis': 'de_anubis_radar_psd.png',
    'de_golden': 'de_golden_radar_tga.png',
    'de_overpass': 'de_overpass_radar_psd.png',
    'de_palacio': 'de_palacio_radar_tga.png',
    'de_train': 'de_train_blended_radar_psd.png',
    'de_vertigo': 'de_vertigo_blended_radar_psd.png',
    'cs_agency': 'cs_agency_radar_tga.png',
    'cs_italy': 'cs_italy_radar_psd.png',
    'cs_office': 'cs_office_radar_psd.png',
}

map_to_id_dict = {
    'de_dust2': 0,
    'de_inferno': 1,
    'de_mirage': 2,
    'de_nuke': 3,
    'de_ancient':4,
    'de_anubis': 5,
    'de_golden': 6,
    'de_overpass': 7,
    'de_palacio': 8,
    'de_train': 9,
    'de_vertigo': 10,
    'cs_agency': 11,
    'cs_italy': 12,
    'cs_office': 13
}

id_to_map_dict = {k: i for i, k in enumerate(map_to_id_dict.keys())}



# ==========================================
# D å¤šä»»åŠ¡æ•°æ®é›†ç±»
# ==========================================
class UniLIPMultiTaskDataset(Dataset):
    def __init__(self, config, tokenizer, data_args):
        super().__init__()
        self.config = config
        self.tokenizer = tokenizer
        self.data_args = data_args

        # ä»»åŠ¡æ··åˆæ¯”ä¾‹: 0.5 è¡¨ç¤º 50% æ¦‚ç‡å®šä½, 50% æ¦‚ç‡ç”Ÿæˆ
        self.mix_ratio = config.get('task_mix_ratio', 0.5)

        self.data_entries = []
        self.map_z_range = {}

        logging.info("ğŸ”„ Loading Multi-Task CS2 Dataset...")

        # --- 1. åŠ è½½æ•°æ®ç´¢å¼• (é€»è¾‘å¤ç”¨ CsgoTrainDataset_IT) ---
        for map_name in config["train_maps"]:
            position_data_path = f"{config['data_dir']}/{map_name}/splits_20000_5000/train_split.json"
            if config['debug']:
                position_data_path = f"{config['data_dir']}/{map_name}/splits_20000_5000/test_split.json"

            logging.info(f"Loading CS2 Data Split {position_data_path}...")
            with open(position_data_path, "r", encoding="utf-8") as f:
                positions_data = json.load(f)

            # è®¡ç®— Z è½´èŒƒå›´
            max_z, min_z = -float('inf'), float('inf')
            for data in positions_data:
                if data['z'] > max_z: max_z = data['z']
                if data['z'] < min_z: min_z = data['z']
            self.map_z_range[map_name] = {'max_z': max_z, 'min_z': min_z}

            # è½½å…¥æ•°æ®
            for pos_data in positions_data:
                entry = {
                    'map': map_name,
                    'file_frame': pos_data['file_frame'],
                    'x': pos_data['x'],
                    'y': pos_data['y'],
                    'z': pos_data['z'],
                    'angle_v': pos_data['angle_v'], # Radian
                    'angle_h': pos_data['angle_h'], # Radian
                }
                self.data_entries.append(entry)

        # Debug é‡‡æ ·
        if config.get('debug', False):
            sampled_num = config.get('debug_num_train_data', 100)
            # self.data_entries = self.data_entries[:sampled_num]
            self.data_entries = random.sample(self.data_entries, sampled_num)
            logging.info([(data['map'], data['file_frame']) for data in self.data_entries])

        logging.info(f"âœ… Total entries: {len(self.data_entries)}")
        self.list_data_dict = {
            "type": ["CS2_Multi_Task"] * len(self.data_entries),
            "id": [str(entry['map'] + "_" + entry['file_frame']) for entry in self.data_entries]
        }


    def __len__(self):
        return len(self.data_entries)

    def __getitem__(self, idx):
        # 1. è·å–åŸºç¡€æ•°æ®
        data = self.data_entries[idx]
        map_name = data['map']

        # 2. å†³å®šå½“å‰æ ·æœ¬çš„ä»»åŠ¡ç±»å‹
        # task_id = 0 -> Localization (å®šä½)
        # task_id = 1 -> Generation (ç”Ÿæˆ)
        task_id = 0 if random.random() < self.mix_ratio else 1

        # 3. åŠ è½½å›¾åƒèµ„æº
        # Map Image
        map_filename = map_path_dict.get(map_name, 'de_dust2_radar_psd.png')
        map_path = f"{self.config['data_dir']}/{map_name}/{map_filename}"
        map_img_pil = Image.open(map_path).convert('RGB')

        # FPS Image
        ext = ".jpg" if self.config['data_dir'] == 'data/preprocessed_data' else ".png"
        fps_path = f"{self.config['data_dir']}/{map_name}/imgs/{data['file_frame']}{ext}"
        fps_img_pil = Image.open(fps_path).convert('RGB')

        # 4. è®¡ç®—å½’ä¸€åŒ–åæ ‡ (Common for both tasks)
        z_info = self.map_z_range[map_name]
        x_norm = data['x'] / 1024.0
        y_norm = data['y'] / 1024.0
        z_norm = (data['z'] - z_info['min_z']) / (z_info['max_z'] - z_info['min_z'] + 1e-6)
        v_norm = data['angle_v'] / (2 * np.pi) # Pitch 0-1
        pitch_deg = (data['angle_v'] / (2 * np.pi)) * 360.0
        h_norm = data['angle_h'] / (2 * np.pi) # Yaw 0-1
        yaw_deg = (data['angle_h'] / (2 * np.pi)) * 360.0

        # Ground Truth Tensor for Localization Head
        loc_coords_norm = torch.tensor([x_norm, y_norm, z_norm, v_norm, h_norm], dtype=torch.bfloat16)
        pose_dict = {
                    'x': data['x'],
                    'y': data['y'],
                    'z': data['z'],
                    'angle_v': pitch_deg,
                    'angle_h': yaw_deg,
                }

        # 5. ä»»åŠ¡åˆ†æµå¤„ç†
        if task_id == 0:
            # =========================================
            # Task: LOCALIZATION (Map + FPS -> Pose)
            # =========================================
            # Text Prompt
            user_text = get_loc_prompt(map_name)
            sources = {
                "conversations": [
                    {"from": "human", "value": user_text},
                    {"from": "gpt", "value": ""} # Assistant å›å¤ä½ç½®Token
                ]
            }
            sources, _ = preprocess_multimodal(copy.deepcopy([sources["conversations"]]))
            preprocess_dict = preprocess(sources, self.tokenizer, has_image=True)
            input_ids = preprocess_dict["input_ids"][0]
            labels = preprocess_dict["labels"][0]

            # FPS Image & MAP Image
            if self.config.get('is_fps_dropout', False):
                fps_img_pil = self.loc_fps_transform(self.config, fps_img_pil)
            all_images = [fps_img_pil, map_img_pil]
            process_images = img_process(
                all_images,
                self.data_args.image_processor,
                self.data_args.image_aspect_ratio
            ) # shape: [2, C, H, W]
            und_image = process_images[:-1] # [1, C, H, W]
            aux_image = process_images[-1:] # [1, C, H, W]

            # Target Image for Generator (DiT)
            # å®šä½ä»»åŠ¡ä¸éœ€è¦ç”Ÿæˆï¼Œæ‰€ä»¥ç»™ä¸€ä¸ªå…¨é»‘çš„æˆ–è€…éšæœºçš„å ä½ç¬¦ï¼ŒLoss Mask ä¼šæŠŠå®ƒå¿½ç•¥
            gen_image = torch.zeros_like(und_image)

            # Head Mask: å¼€å¯ Pose Headï¼Œå…³é—­ Gen Head
            # [Loc_Loss_Weight, Gen_Loss_Weight]
            loss_mask = torch.tensor([1.0, 0.0], dtype=torch.float32)

        else:
            # =========================================
            # Task: GENERATION (Map + Pose -> FPS)
            # =========================================
            # æ¨¡æ‹Ÿ Classifier-Free Guidance (CFG) è®­ç»ƒ
            # 10% æ¦‚ç‡ç»™ç©º Prompt ("Generate...")ï¼Œ90% æ¦‚ç‡ç»™å®Œæ•´ Prompt
            if random.random() > 0.1:
                # Positive Prompt
                user_text = build_sft_instruction_custom(pose_dict, map_name, z_info['max_z'], z_info['min_z'])
            else:
                # CFG Negative/Generic Prompt
                # æ³¨æ„ï¼šå¿…é¡»åŒ…å« <image> ä¸”ä½ç½®è¦å’Œ Positive Prompt é‡Œçš„ä½ç½®ä¸€è‡´(è¿™é‡Œéƒ½åœ¨æœ€å)
                user_text = "Generate the view.\n<image>"

            # æ„å»º UniLIP æ ‡å‡†å¯¹è¯æ ¼å¼
            sources = {
                "conversations": [
                    {"from": "human", "value": user_text},
                    {"from": "gpt", "value": "<image>"} # Assistant å›å¤ç”Ÿæˆçš„å›¾ç‰‡ Token
                ]
            }
            # å¤„ç† <image> token æ›¿æ¢
            sources, _ = preprocess_multimodal(copy.deepcopy([sources["conversations"]]))
            # Tokenize æ–‡æœ¬å¾—åˆ° input_ids å’Œ labels(å·²æ›¿æ¢IGNORE_INDEX); has_image=True é˜²æ­¢å¤šæ¨¡æ€è¾“å…¥æ—¶, tokenizer æŠ¥é”™
            preprocess_dict = preprocess(sources, self.tokenizer, has_image=True)
            input_ids = preprocess_dict["input_ids"][0]
            labels = preprocess_dict["labels"][0]

            # å°† Radar å’Œ FPS æ‰“åŒ…ä¸€èµ·å¤„ç†ï¼Œç¡®ä¿ç»è¿‡ç›¸åŒçš„ Preprocess
            # é¡ºåºï¼š[Radar(Und), FPS(Gen)]
            all_images = [map_img_pil, fps_img_pil]
            process_images = img_process(
                all_images,
                self.data_args.image_processor,
                self.data_args.image_aspect_ratio
            ) # shape: [2, C, H, W]
            und_image = process_images[:-1] # [1, C, H, W]
            gen_image = process_images[-1:] # [1, C, H, W]

            aux_image = torch.zeros_like(und_image)

            loss_mask = torch.tensor([0.0, 1.0], dtype=torch.float32)

        # 6. è¿”å›å­—å…¸
        # å…¼å®¹ UniLIP å’Œ OpenPI çš„å­—æ®µå‘½å
        return {
            "task_id": task_id,             # 0=Loc, 1=Gen
            "ids": data['file_frame'],
            "und_image": und_image,         # ç†è§£æµè¾“å…¥ - å®šä½=fps ç”Ÿæˆ=map
            "aux_image": aux_image,         # è¾…åŠ©è¾“å…¥   - å®šä½=map ç”Ÿæˆ=Empty
            "gen_image": gen_image,         # ç”Ÿæˆæµè¾“å‡º - å®šä½=Empty ç”Ÿæˆ=fps
            "input_ids": input_ids,
            "labels": labels,
            "raw_prompt": user_text,
            "actions": loc_coords_norm.unsqueeze(0), # [1, 5] å®šä½çœŸå€¼ (ç”Ÿæˆä»»åŠ¡æ—¶æ­¤å€¼å­˜åœ¨ä½†loss_maskä¸º0)
            "loss_mask": loss_mask,         # [1.0, 0.0] or [0.0, 1.0]
            "map_id": map_to_id_dict.get(map_name, 0),
            "map_name": map_name,
            "pose_dict": pose_dict,
        }

    def loc_fps_transform(self, config, fps_img_pil):
        fps_transform = transforms.Compose([
            transforms.ToTensor(),
            CoarseDropout(max_holes=8, max_height=16, max_width=16, p=0.5),
            GridDropout(grid_size=4, p=0.3),
            RandomErasing(probability=config['erasing_p'], mean=[0.0, 0.0, 0.0])
        ])
        fps_img = fps_transform(fps_img_pil)
        fps_img = np.array(fps_img.permute(1,2,0))
        fps_img = Image.fromarray((fps_img*255).astype(np.uint8))

        return fps_img





@dataclass
class DataCollatorForUniLIPMultiTaskDataset(object):
    """
    Collate examples for UniLIP Multi-Task (Localization + Generation).
    Adapts inputs based on task_id.
    """
    tokenizer: transformers.PreTrainedTokenizer

    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:
        # 1. æå–åŸºç¡€æ•°æ®
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å¤„ç† task_id æ¥å†³å®šæ˜¯å¦æ·»åŠ ç”Ÿæˆä¸“ç”¨çš„å ä½ç¬¦ token
        task_id_list = []
        input_ids_list = []
        labels_list = []
        ids_list = []

        # æ–°å¢å­—æ®µçš„å®¹å™¨
        batch_gen_images = []
        batch_und_images = []
        batch_aux_images = []
        batch_actions = []
        batch_loss_masks = []
        batch_map_ids = []

        batch_raw_prompt_list = []
        batch_map_name_list = []
        batch_pose_dict_list = []

        # 2. é€æ ·æœ¬å¤„ç†
        for instance in instances:
            task_id = instance.get("task_id", 1) # é»˜è®¤ä¸º1(ç”Ÿæˆ)ä»¥é˜²ä¸‡ä¸€
            _input_id = instance["input_ids"]
            _label = instance["labels"]
            _id = instance.get("ids", "unknown")

            # === Token å¤„ç†é€»è¾‘ ===
            # ä¸ºäº†é˜²æ­¢è¶…è¿‡æ¨¡å‹æœ€å¤§é•¿åº¦ï¼Œå…ˆåšæˆªæ–­ (é¢„ç•™ 257 ä¸ªä½ç½®ç»™ç”Ÿæˆ Token)
            # UniLIP åŸé€»è¾‘ï¼šinput_id[: max_len - 257]
            safe_len = self.tokenizer.model_max_length - 257
            _input_id = _input_id[:safe_len]
            _label = _label[:safe_len]

            # [å…³é”®åˆ†æ”¯]
            if task_id == 1:
                # ==> ç”Ÿæˆä»»åŠ¡ (Generation)
                # éœ€è¦åœ¨æœ«å°¾è¿½åŠ  257 ä¸ª Latent Query Tokens ä¾› DiT ä½¿ç”¨
                # æ„é€  Image Token åºåˆ—
                img_tokens = torch.full((257,), IMAGE_TOKEN_IDX, dtype=_input_id.dtype, device=_input_id.device)
                # UniLIP ç‰¹æ®Šæ“ä½œï¼šç¬¬ä¸€ä¸ª Token è®¾ä¸º 151665 (å¯èƒ½æ˜¯ç‰¹å®šçš„ Start Token æˆ– Separator)
                # è¯·ç¡®ä¿è¿™ä¸ª ID åœ¨æ‚¨çš„ Tokenizer ä¸­æ˜¯åˆæ³•çš„ï¼Œæˆ–è€…æ²¿ç”¨åŸä»£ç çš„ç¡¬ç¼–ç 
                img_tokens[0] = 151665

                # æ‹¼æ¥åˆ° input_ids
                _input_id = torch.cat([_input_id, img_tokens])

                # æ„é€  Label (å¯¹äºç”Ÿæˆéƒ¨åˆ†ï¼ŒLabel ä¹Ÿæ˜¯ IMAGE_TOKEN_IDXï¼Œä½†åœ¨ Model å†…éƒ¨ä¼šè¢« Mask æ‰ä¸è®¡ç®— CE Loss)
                img_labels = torch.full((257,), IMAGE_TOKEN_IDX, dtype=_label.dtype, device=_label.device)
                img_labels[0] = 151665
                _label = torch.cat([_label, img_labels])

            else:
                # ==> å®šä½ä»»åŠ¡ (Localization)
                # ä¸éœ€è¦è¿½åŠ  Latent Queriesï¼Œå› ä¸ºæˆ‘ä»¬æ˜¯ç”¨ Action Head åŸºäºå‰é¢çš„æ–‡æœ¬ç‰¹å¾é¢„æµ‹
                # ä¿æŒ input_ids åŸæ · (å·²ç»åŒ…å« <image> placeholders for input images)
                pass

            task_id_list.append(task_id)
            input_ids_list.append(_input_id)
            labels_list.append(_label)
            ids_list.append(_id)

            # === æ”¶é›† Tensor æ•°æ® ===
            if "gen_image" in instance and instance["gen_image"] is not None:
                batch_gen_images.append(instance["gen_image"])

            if "und_image" in instance and instance["und_image"] is not None:
                batch_und_images.append(instance["und_image"])

            # [NEW] æ”¶é›† Aux Image (è¾…åŠ©å›¾/Map)
            if "aux_image" in instance and instance["aux_image"] is not None:
                batch_aux_images.append(instance["aux_image"])

            # [NEW] æ”¶é›† Actions
            if "actions" in instance and instance["actions"] is not None:
                batch_actions.append(instance["actions"]) # [1, 5]

            # [NEW] æ”¶é›† Loss Mask
            if "loss_mask" in instance and instance["loss_mask"] is not None:
                batch_loss_masks.append(instance["loss_mask"]) # [2]

            if "map_id" in instance:
                batch_map_ids.append(instance["map_id"])

            if "raw_prompt" in instance:
                batch_raw_prompt_list.append(instance["raw_prompt"])
            if "map_name" in instance:
                batch_map_name_list.append(instance["map_name"])
            if "pose_dict" in instance:
                batch_pose_dict_list.append(instance["pose_dict"])


        # 3. Padding (Pad Input Ids & Labels)
        # batch_first=True -> [BS, Seq]
        input_ids = torch.nn.utils.rnn.pad_sequence(
            input_ids_list, batch_first=True, padding_value=self.tokenizer.pad_token_id
        )
        labels = torch.nn.utils.rnn.pad_sequence(
            labels_list, batch_first=True, padding_value=IGNORE_INDEX
        )

        # å†æ¬¡æ£€æŸ¥æœ€å¤§é•¿åº¦ (Padding åå¯èƒ½ä¼šå˜é•¿)
        if input_ids.shape[1] > self.tokenizer.model_max_length:
            logging.warning(f"Input length {input_ids.shape[1]} > {self.tokenizer.model_max_length}, truncating.")
            input_ids = input_ids[:, :self.tokenizer.model_max_length]
            labels = labels[:, :self.tokenizer.model_max_length]

        # 4. æ„å»º Batch å­—å…¸
        batch = dict(
            input_ids=input_ids,
            labels=labels,
            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),
            ids=ids_list,
        )

        # 5. å †å å›¾åƒ (Stack Images)
        # è¾…åŠ©å‡½æ•°ï¼šå®‰å…¨å †å 
        def stack_images(img_list):
            if len(img_list) == 0:
                return None
            # æ£€æŸ¥å½¢çŠ¶ä¸€è‡´æ€§
            if all(x is not None and x.shape == img_list[0].shape for x in img_list):
                return torch.cat(img_list, dim=0) # [BS, C, H, W] (å› ä¸º Dataset è¿”å›çš„æ˜¯ [1, C, H, W])
            else:
                logging.warning("Image shapes inconsistent in batch, returning list instead of tensor.")
                return img_list

        batch["gen_image"] = stack_images(batch_gen_images)
        batch["und_image"] = stack_images(batch_und_images)
        batch["aux_image"] = stack_images(batch_aux_images)

        # 6. å †å å…¶ä»– Tensor (Actions, Mask)
        if len(batch_actions) > 0:
            batch["actions"] = torch.stack(batch_actions, dim=0) # [BS, 1, 5]
        else:
            batch["actions"] = None

        if len(batch_loss_masks) > 0:
            batch["loss_mask"] = torch.stack(batch_loss_masks, dim=0) # [BS, 2]
        else:
            batch["loss_mask"] = None

        if len(batch_map_ids) > 0:
            batch["map_id"] = torch.tensor(batch_map_ids, dtype=torch.long)

        if len(task_id_list) > 0:
            batch["task_id"] = torch.tensor(task_id_list, dtype=torch.long)

        if len(batch_raw_prompt_list) > 0:
            batch["raw_prompt"] = batch_raw_prompt_list
        if len(batch_map_name_list) > 0:
            batch["map_name"] = batch_map_name_list
        if len(batch_pose_dict_list) > 0:
            batch["pose_dict"] = batch_pose_dict_list

        return batch





import torch
import matplotlib.pyplot as plt
import numpy as np
import textwrap

def denormalize_image(tensor):
    """åå½’ä¸€åŒ– CLIP å›¾ç‰‡ Tensor -> Numpy Uint8"""
    if tensor is None: return None
    # ç¡®ä¿åœ¨ CPU
    tensor = tensor.cpu().detach()

    # CLIP Mean/Std
    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).view(3, 1, 1)
    std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).view(3, 1, 1)

    # åå½’ä¸€åŒ–
    tensor = tensor * std + mean
    tensor = torch.clamp(tensor, 0, 1)

    # [C, H, W] -> [H, W, C]
    return (tensor.permute(1, 2, 0).numpy() * 255).astype(np.uint8)

def visualize_batch(batch, tokenizer, max_samples=5, save_path="_vis_results/_trainner_batch.jpg"):
    """
    å¯è§†åŒ– Batch ä¸­çš„å‰ N ä¸ªæ ·æœ¬ã€‚
    åŒ…å«ï¼šæ–‡æœ¬ä¿¡æ¯é¢æ¿ + Und Image + Aux Image + Gen Image
    """
    batch_size = len(batch['input_ids'])
    limit = min(batch_size, max_samples)

    # åˆ›å»ºå¤§å›¾ï¼šæ¯ä¸ªæ ·æœ¬å ä¸€è¡Œ (é«˜åº¦6)ï¼Œå®½24
    fig, axes = plt.subplots(limit, 4, figsize=(30, 8 * limit))
    if limit == 1: axes = axes.reshape(1, -1) # å¤„ç†å•æ ·æœ¬æƒ…å†µ

    plt.subplots_adjust(hspace=0.4, wspace=0.2)

    for i in range(limit):
        # --- 1. è·å–åŸºç¡€ä¿¡æ¯ ---
        # å°è¯•ä» tensor æˆ– list ä¸­è·å–ï¼Œå…¼å®¹ä¸¤ç§æ ¼å¼
        def get_item(key, idx):
            if key not in batch or batch[key] is None: return "N/A"
            item = batch[key]
            if isinstance(item, torch.Tensor):
                return item[idx].cpu().detach()
            elif isinstance(item, list):
                return item[idx]
            return "Unknown Type"

        task_id = get_item("task_id", i)
        task_name = "GENERATION" if task_id == 1 else "LOCALIZATION"

        sample_id = get_item("ids", i)
        loss_mask = get_item("loss_mask", i)
        actions = get_item("actions", i)
        raw_prompt = get_item("raw_prompt", i)
        pose_dict = get_item("pose_dict", i)

        # --- 2. ç»˜åˆ¶æ–‡æœ¬é¢æ¿ (ç¬¬ä¸€åˆ—) ---
        ax_text = axes[i, 0]
        ax_text.axis('off')

        # æ„å»ºä¿¡æ¯å­—ç¬¦ä¸²
        info_str = (
            f"Index: {i} | ID: {sample_id}\n"
            f"----------------------------------------\n"
            f"Task: {task_name} (ID: {task_id})\n"
            f"Loss Mask: {loss_mask}\n"
            f"Actions (GT): {actions}\n"
            f"Pose Dict: {pose_dict}\n"
            f"----------------------------------------\n"
            f"Raw Prompt:\n{textwrap.fill(str(raw_prompt), width=80)}\n"
        )

        # æ˜¾ç¤º Input IDs è§£ç ç‰‡æ®µ
        input_ids = batch['input_ids'][i]
        decoded = tokenizer.decode(input_ids, skip_special_tokens=False)
        # åªæ˜¾ç¤ºæœ€å 200 ä¸ªå­—ç¬¦é¿å…å¤ªé•¿
        info_str += f"----------------------------------------\n"
        info_str += f"Tokenized (Fist 500 chars ... Last 500 chars):\n{textwrap.fill(decoded[:500] + ' ... ... ' + decoded[-500:], width=80)}"

        # æ˜¾ç¤º Labels è§£ç ç‰‡æ®µ
        labels = batch['labels'][i]
        decoded_labels = tokenizer.decode(labels[labels!=-100], skip_special_tokens=False)
        # åªæ˜¾ç¤ºæœ€å 200 ä¸ªå­—ç¬¦é¿å…å¤ªé•¿
        info_str += f"----------------------------------------\n"
        info_str += f"Tokenized Labels (Fist 100 chars ... Last 100 chars):\n{textwrap.fill(decoded_labels[:100] + ' ... ... ' + decoded_labels[-100:], width=80)}"

        ax_text.text(0, 1, info_str, fontsize=10, verticalalignment='top', fontfamily='monospace')

        # --- 3. ç»˜åˆ¶å›¾ç‰‡ (åä¸‰åˆ—) ---
        img_keys = [
            ("und_image", "Und Image (Input)"),
            ("aux_image", "Aux Image (Map/Wrist)"),
            ("gen_image", "Gen Image (Target)")
        ]

        for col_idx, (key, title) in enumerate(img_keys):
            ax_img = axes[i, col_idx + 1]

            if key in batch and batch[key] is not None:
                # æ£€æŸ¥ batch[key] çš„å½¢çŠ¶ï¼Œå¦‚æœæ˜¯ [BS, C, H, W]
                img_tensor = batch[key][i]

                # æ£€æŸ¥æ˜¯å¦å…¨é»‘/å…¨é›¶ (è¡¨ç¤ºæœªä½¿ç”¨çš„å ä½ç¬¦)
                if torch.all(img_tensor == 0):
                    ax_img.text(0.5, 0.5, "Empty / Masked", ha='center', va='center')
                    ax_img.set_facecolor("#f0f0f0") # ç°è‰²èƒŒæ™¯
                else:
                    img_np = denormalize_image(img_tensor)
                    ax_img.imshow(img_np)
            else:
                ax_img.text(0.5, 0.5, "None", ha='center', va='center')

            ax_img.set_title(title, fontsize=12, fontweight='bold')
            ax_img.axis('off')

    plt.tight_layout()
    plt.savefig(save_path)
    print(f"âœ… Batch visualization saved to {save_path}")
    plt.close()

# # --- ä½¿ç”¨ç¤ºä¾‹ ---
# if __name__ == "__main__":
#     # å‡è®¾ä½ å·²ç»è¿è¡Œäº† collator å¾—åˆ°äº† batch
#     # from transformers import AutoTokenizer
#     # tokenizer = AutoTokenizer.from_pretrained("...")
#     # visualize_batch(batch, tokenizer)
#     # visualize_batch(batch, model.text_tokenizer)

#     pass





#### Debug at huggingface.transformers
# /home/jiahao/miniconda3/envs/UniLIP/lib/python3.11/site-packages/transformers/trainer.py
# Line 2579
# epoch_dataloader = train_dataloader
# batch = next(iter(epoch_dataloader))
    # batch.keys()
    # dict_keys(['input_ids', 'labels', 'attention_mask', 'ids', 'gen_image', 'und_image', 'aux_image', 'actions', 'loss_mask', 'map_id', 'task_id'])

# visualize_batch(batch, model.text_tokenizer)

# Line 2618
# batch_samples, num_items_in_batch = self.get_batch_samples(epoch_iterator, num_batches, args.device)
    # len(batch_samples)
    # 1
    # batch_samples[0].keys()
    # dict_keys(['input_ids', 'labels', 'attention_mask', 'ids', 'gen_image', 'und_image', 'aux_image', 'actions', 'loss_mask', 'map_id', 'task_id', 'raw_prompt', 'map_name', 'pose_dict'])

# Line 4110
    # outputs = model(**inputs)
    # inputs.keys()
    # dict_keys(['input_ids', 'labels', 'attention_mask', 'ids', 'gen_image', 'und_image', 'aux_image', 'actions', 'loss_mask', 'map_id', 'task_id', 'raw_prompt', 'map_name', 'pose_dict', 'num_items_in_batch'])

