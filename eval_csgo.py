import argparse
import os
import json
import torch
import numpy as np
import yaml
import random
import datetime
from PIL import Image
from tqdm import tqdm
from torch.utils.data import DataLoader, Dataset
from transformers import AutoProcessor
import matplotlib.pyplot as plt

# å¼•å…¥ UniLIP æ ¸å¿ƒæ¨¡å—
from unilip.utils import disable_torch_init
from unilip.model.builder import load_pretrained_model_general
from unilip.pipeline_edit import CustomEditPipeline
from unilip.mm_utils import get_model_name_from_path



def set_seed(seed=42):
    # 1. Python å†…ç½® random
    random.seed(seed)
    # 2. æ“ä½œç³»ç»Ÿç¯å¢ƒ (è¿™å¯¹æŸäº›å“ˆå¸Œæ“ä½œæ˜¯å¿…é¡»çš„ï¼Œå¦‚ set/dict çš„é¡ºåº)
    os.environ['PYTHONHASHSEED'] = str(seed)
    # 3. NumPy
    np.random.seed(seed)
    # 4. PyTorch CPU
    torch.manual_seed(seed)
    # 5. PyTorch GPU (å¦‚æœå¯ç”¨)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed) # å¦‚æœæœ‰å¤šå¼ æ˜¾å¡ï¼Œä¸ºæ‰€æœ‰æ˜¾å¡è®¾ç½®
    # 6. è®¾ç½® CuDNN åç«¯ä»¥ç¡®ä¿ç¡®å®šæ€§ (ä¼šé™ä½æ€§èƒ½)
    # å¦‚æœä½ éå¸¸çœ‹é‡ç»“æœçš„é€ä½ä¸€è‡´æ€§ï¼Œå¿…é¡»å¼€å¯ deterministic
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    print(f"éšæœºç§å­å·²è®¾ç½®ä¸º: {seed}")


# ==========================================
# 1. å¤ç”¨è¾…åŠ©å‡½æ•° (Prompt æ„å»º & Padding)
# ==========================================
def expand2square(pil_img, background_color):
    width, height = pil_img.size
    if width == height:
        return pil_img
    elif width > height:
        result = Image.new(pil_img.mode, (width, width), background_color)
        result.paste(pil_img, (0, (width - height) // 2))
        return result
    else:
        result = Image.new(pil_img.mode, (height, height), background_color)
        result.paste(pil_img, ((height - width) // 2, 0))
        return result

def build_sft_instruction_custom(pose_5d, map_name, z_max, z_min):
    # ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´çš„ Prompt æ¨¡æ¿
    definition_text = (
        f"Task: Generate a First-Person View (FPV) image of CS2 map '{map_name}' based on the Radar Map and Camera Pose.\n"
        "Coordinate System Definition:\n"
        "- Map Size: 1024x1024 pixels.\n"
        "- Yaw: 0 degrees is East, increases Clockwise.\n"
        "- Pitch: 0 degrees is looking straight Down (at feet), 180 degrees is looking straight Up (at sky).\n"
        f"- Z-Height: Absolute vertical coordinate. Valid values are bounded by the map's global topology, ranging from the lowest point at {z_min:.2f} to the highest point at {z_max:.2f}."
    )
    pose_str = (
        f"Position(x={pose_5d['x']:.1f}, y={pose_5d['y']:.1f}, z={pose_5d['z']:.3f}), "
        f"Rotation(pitch={pose_5d['angle_v']:.1f}, yaw={pose_5d['angle_h']:.1f})"
    )
    full_instruction = f"{definition_text}\n\nCurrent Camera Pose: {pose_str}\n<image>"
    return full_instruction

def add_template_for_inference(prompt_text):
    # å°† SFT æŒ‡ä»¤åŒ…è£…æˆå¯¹è¯æ ¼å¼
    instruction = ('<|im_start|>user\n{input}<|im_end|>\n'
                   '<|im_start|>assistant\n<img>')

    # Positive Prompt: ä½ çš„ SFT æŒ‡ä»¤
    pos_prompt = instruction.format(input=prompt_text)

    # Negative/CFG Prompt: ä¿æŒè®­ç»ƒæ—¶çš„é€šç”¨æŒ‡ä»¤
    # æ³¨æ„ï¼šè¿™é‡Œ <image> ä¹Ÿè¦åŒ…å«
    cfg_prompt = instruction.format(input="Generate the view.\n<image>")

    return [pos_prompt, cfg_prompt]

# ==========================================
# 2. è½»é‡çº§æ¨ç†æ•°æ®é›† (InferenceDataset)
# ==========================================
class CSGOInferenceDataset(Dataset):
    def __init__(self, config, map_path_dict):
        self.config = config
        self.data_dir = config['data_dir']
        self.map_names = config['val_maps']
        self.map_path_dict = map_path_dict

        self.data_entries = []
        self.map_z_range = {}
        print("ğŸ”„ Loading Test Data...")
        for map_name in self.map_names:
            # è¯»å–æµ‹è¯•é›† split
            # æ³¨æ„ï¼šè¿™é‡Œå¼ºåˆ¶è¯»å– test_split.json
            split_path = f"{self.data_dir}/{map_name}/splits_20000_5000/test_split.json"

            with open(split_path, "r", encoding="utf-8") as f:
                positions_data = json.load(f)

            # è®¡ç®— Z èŒƒå›´ (å¿…é¡»åŸºäºå…¨é›†æˆ– Train é›†çš„ç»Ÿè®¡ï¼Œè¿™é‡Œç®€åŒ–ä¸ºå½“å‰ Split çš„ç»Ÿè®¡ï¼Œå»ºè®®æœ€å¥½ç¡¬ç¼–ç æˆ–è¯»å– train_split ç»Ÿè®¡)
            # ä¸ºäº†ä¸¥è°¨ï¼Œè¿™é‡Œåº”è¯¥è¯»å– train_split æ¥è·å– z_min/z_maxï¼Œé˜²æ­¢ test æ•°æ®æº¢å‡º
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šç›´æ¥éå† test set (ç”Ÿäº§ç¯å¢ƒå»ºè®®è¯»å– metadata)
            zs = [d['z'] for d in positions_data]
            self.map_z_range[map_name] = {'max_z': max(zs), 'min_z': min(zs)}

            for pos_data in positions_data:
                entry = {
                    'map': map_name,
                    'file_frame': pos_data['file_frame'],
                    'x': pos_data['x'],
                    'y': pos_data['y'],
                    'z': pos_data['z'],
                    'angle_v': pos_data['angle_v'],
                    'angle_h': pos_data['angle_h'],
                }
                self.data_entries.append(entry)

        if config['debug'] and config.get('debug_num_train_data', False):
            sampled_num = config.get('debug_num_train_data', len(self.data_entries))
            self.data_entries = self.data_entries[:sampled_num]
        elif config['debug'] and config.get('debug_num_train_data', False) == False:
            indices = [335, 535, 707, 288, 21, 240, 20, 30, 809, 423, 857, 459, 557, 882, 893, 406, 24, 477, 407, 427, 453, 923, 925, 399, 752, 867, 547, 563, 424, 217, 789, 681]
            self.data_entries = [self.data_entries[i] for i in indices]
        elif config['debug']==False and config.get('debug_num_train_data', False):
            sampled_num = config.get('debug_num_train_data', len(self.data_entries))
            self.data_entries = random.sample(self.data_entries, sampled_num)

        # ä»…å–å‰Nä¸ªåšæµ‹è¯•ï¼Œé¿å…è·‘å¤ªä¹… (å¯é€‰)
        # self.data_entries = self.data_entries[:50]
        print(f"âœ… Loaded {len(self.data_entries)} test samples.")

    def __len__(self):
        return len(self.data_entries)

    def __getitem__(self, i):
        data = self.data_entries[i]
        map_name = data['map']

        # 1. åŠ è½½ Radar (Input Condition)
        map_filename = self.map_path_dict.get(map_name, 'de_dust2_radar_psd.png')
        radar_path = f"{self.data_dir}/{map_name}/{map_filename}"
        radar_img = Image.open(radar_path).convert('RGB')

        # 2. åŠ è½½ GT FPS (Ground Truth for Vis)
        # æ³¨æ„åç¼€ï¼Œå¦‚æœæ˜¯ preprocessed_data å¯èƒ½æ˜¯ .jpg
        ext = ".jpg" if "preprocessed" in self.data_dir else ".png"
        fps_path = f"{self.data_dir}/{map_name}/imgs/{data['file_frame']}{ext}"
        gt_img = Image.open(fps_path).convert('RGB')

        # 3. å‡†å¤‡ Prompt å‚æ•°
        z_min = self.map_z_range[map_name]['min_z']
        z_max = self.map_z_range[map_name]['max_z']

        # å½’ä¸€åŒ– Z (0-1) ç”¨äº Pose æ•°å€¼å±•ç¤º
        z_norm = (data['z'] - z_min) / (z_max - z_min + 1e-6)

        # å¼§åº¦è½¬è§’åº¦
        pitch_deg = (data['angle_v'] / (2 * np.pi)) * 180.0
        yaw_deg = (data['angle_h'] / (2 * np.pi)) * 360.0

        pose_dict = {
            'x': data['x'], 'y': data['y'], 'z': data['z'],
             'angle_v': pitch_deg, 'angle_h': yaw_deg
        }

        # 4. æ„å»º Prompt
        # æ³¨æ„ï¼šè¿™é‡Œ z_max, z_min ä¼ å…¥çœŸå®ç‰©ç†å€¼ç”¨äºå®šä¹‰
        raw_prompt = build_sft_instruction_custom(pose_dict, map_name, z_max, z_min)

        return {
            "radar_img": radar_img,
            "gt_img": gt_img,
            "raw_prompt": raw_prompt,
            "file_frame": data['file_frame'],
            "pose_info": pose_dict
        }

def collate_fn(batch):
    return batch # ç®€å•çš„ list è¿”å›ï¼Œä¸ç”± DataLoader è‡ªåŠ¨ stack tensor

map_path_dict = {
    'de_dust2': 'de_dust2_radar_psd.png',
    'de_inferno': 'de_inferno_radar_psd.png',
    'de_mirage': 'de_mirage_radar_psd.png',
    'de_nuke': 'de_nuke_blended_radar_psd.png',
    'de_ancient': 'de_ancient_radar_psd.png',
    'de_anubis': 'de_anubis_radar_psd.png',
    'de_golden': 'de_golden_radar_tga.png',
    'de_overpass': 'de_overpass_radar_psd.png',
    'de_palacio': 'de_palacio_radar_tga.png',
    'de_train': 'de_train_blended_radar_psd.png',
    'de_vertigo': 'de_vertigo_blended_radar_psd.png',
    'cs_agency': 'cs_agency_radar_tga.png',
    'cs_italy': 'cs_italy_radar_psd.png',
    'cs_office': 'cs_office_radar_psd.png',
}

# ==========================================
# 3. ä¸»æ¨ç†é€»è¾‘
# ==========================================
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--csgo_config", type=str, required=True)
    args = parser.parse_args()

    with open(args.csgo_config, 'r') as f:
        csgo_config = yaml.safe_load(f)
    print("csgo_config: ", csgo_config)

    # è®¾ç½®éšæœºç§å­
    set_seed()

    cur_time_str = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = f"outputs_eval/{args.csgo_config.split('/')[-1][:-5]}/test_{cur_time_str}"
    os.makedirs(output_dir, exist_ok=True)

    # 1. åŠ è½½æ¨¡å‹
    disable_torch_init()
    model_name = get_model_name_from_path(csgo_config["ckpt_path"])
    print(f"ğŸš€ Loading model from {csgo_config['ckpt_path']}...")
    tokenizer, model, context_len = load_pretrained_model_general(
        'UniLIP_InternVLForCausalLM', csgo_config["ckpt_path"], None, model_name
    )

    image_processor = AutoProcessor.from_pretrained(model.config.mllm_hf_path).image_processor

    # åˆå§‹åŒ– Pipeline
    pipe = CustomEditPipeline(multimodal_encoder=model, tokenizer=tokenizer, image_processor=image_processor)

    test_dataset = CSGOInferenceDataset(
        csgo_config,
        map_path_dict
    )

    dataloader = DataLoader(test_dataset, batch_size=csgo_config["batch_size"], shuffle=False, collate_fn=collate_fn)

    # 3. æ¨ç†å¾ªç¯
    generator = torch.Generator(device=model.device).manual_seed(42)
    print("ğŸš€ Starting Inference...")

    vis_data = [] # å­˜å‚¨ç¬¬ä¸€æ‰¹æ¬¡ç”¨äºå¯è§†åŒ–

    for batch_idx, batch in enumerate(tqdm(dataloader)):
        # æ‰¹æ¬¡å†…çš„æ¯ä¸ªæ ·æœ¬é€ä¸ªå¤„ç† (å› ä¸º Pipe æ¥å£é€šå¸¸æ¥å— List[Prompt] ä½†å¯¹åº”å•å¼ å›¾ç‰‡è¾“å…¥)
        # ä¸ºäº†å…¼å®¹ CustomEditPipeline çš„é€»è¾‘ (multimodal_prompts list ç»“æ„)

        for sample in batch:
            radar_img = sample['radar_img']
            raw_prompt = sample['raw_prompt']
            file_frame = sample['file_frame']

            # æ„é€  UniLIP æ ¼å¼çš„ multimodal prompts
            # [Positive_Prompt, Negative_Prompt, Image]
            multimodal_prompts = add_template_for_inference(raw_prompt)
            multimodal_prompts.append(radar_img) # å¿…é¡» append PIL Image å¯¹è±¡

            # æ‰§è¡Œç”Ÿæˆ
            with torch.no_grad():
                gen_img = pipe(
                    multimodal_prompts,
                    guidance_scale=csgo_config["guidance_scale"],
                    generator=generator
                )

            # ä¿å­˜å•å¼ å›¾ç‰‡
            save_name = f"{file_frame}.png"
            gen_img.save(os.path.join(output_dir, save_name))

            # æ”¶é›†æ•°æ®ç”¨äºå¯è§†åŒ–
            if len(vis_data) < 4:
                vis_data.append({
                    "radar": radar_img,
                    "gt": sample['gt_img'],
                    "gen": gen_img,
                    "pose": sample['pose_info'],
                    "prompt": raw_prompt
                })

    # 4. å¯è§†åŒ–å¯¹æ¯”å›¾ (Radar | GT | Gen)
    if len(vis_data) > 0:
        print("ğŸ“Š Generating Visualization for the first batch...")
        fig, axes = plt.subplots(len(vis_data), 3, figsize=(15, 5 * len(vis_data)))
        if len(vis_data) == 1: axes = [axes]

        for i, item in enumerate(vis_data):
            # Radar
            axes[i][0].imshow(item['radar'])
            axes[i][0].set_title("Input: Radar Map")
            axes[i][0].axis('off')

            # GT FPS
            axes[i][1].imshow(item['gt'])
            axes[i][1].set_title("Ground Truth (FPS)")
            axes[i][1].axis('off')

            # Generated FPS
            axes[i][2].imshow(item['gen'])

            # æå– Pose å­—ç¬¦ä¸²ç”¨äºå±•ç¤º
            p = item['pose']
            title_str = f"Generated\nPos: ({p['x']:.1f}, {p['y']:.1f}, {p['z']:.2f})\nAng: ({p['angle_v']:.1f}, {p['angle_h']:.1f})"
            axes[i][2].set_title(title_str, color='blue', fontsize=10)
            axes[i][2].axis('off')

        plt.tight_layout()
        vis_save_path = os.path.join(output_dir, "vis_batch_0.png")
        plt.savefig(vis_save_path, dpi=150)
        print(f"âœ¨ Visualization saved to {vis_save_path}")

    print(f"âœ… Inference finished. Results saved to {output_dir}")

if __name__ == "__main__":
    main()



# python eval_csgo.py --csgo_config csgo_configs/test/exp0.yaml