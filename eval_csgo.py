import argparse
import os
import json
import torch
import numpy as np
import yaml
import random
import datetime
from PIL import Image
from tqdm import tqdm
from torch.utils.data import DataLoader, Dataset
from transformers import AutoProcessor
import matplotlib.pyplot as plt
from safetensors.torch import load_file as safe_load_file
import textwrap

# å¼•å…¥ UniLIP æ ¸å¿ƒæ¨¡å—
from unilip.utils import disable_torch_init
from unilip.pipeline_edit import CustomEditPipeline
from unilip.mm_utils import get_model_name_from_path
from unilip.model.builder import load_pretrained_model_general


def set_seed(seed=42):
    # 1. Python å†…ç½® random
    random.seed(seed)
    # 2. æ“ä½œç³»ç»Ÿç¯å¢ƒ (è¿™å¯¹æŸäº›å“ˆå¸Œæ“ä½œæ˜¯å¿…é¡»çš„ï¼Œå¦‚ set/dict çš„é¡ºåº)
    os.environ['PYTHONHASHSEED'] = str(seed)
    # 3. NumPy
    np.random.seed(seed)
    # 4. PyTorch CPU
    torch.manual_seed(seed)
    # 5. PyTorch GPU (å¦‚æœå¯ç”¨)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed) # å¦‚æœæœ‰å¤šå¼ æ˜¾å¡ï¼Œä¸ºæ‰€æœ‰æ˜¾å¡è®¾ç½®
    # 6. è®¾ç½® CuDNN åç«¯ä»¥ç¡®ä¿ç¡®å®šæ€§ (ä¼šé™ä½æ€§èƒ½)
    # å¦‚æœä½ éå¸¸çœ‹é‡ç»“æœçš„é€ä½ä¸€è‡´æ€§ï¼Œå¿…é¡»å¼€å¯ deterministic
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    print(f"éšæœºç§å­å·²è®¾ç½®ä¸º: {seed}")


# ==========================================
# 1. å¤ç”¨è¾…åŠ©å‡½æ•° (Prompt æ„å»º & Padding)
# ==========================================
def expand2square(pil_img, background_color):
    width, height = pil_img.size
    if width == height:
        return pil_img
    elif width > height:
        result = Image.new(pil_img.mode, (width, width), background_color)
        result.paste(pil_img, (0, (width - height) // 2))
        return result
    else:
        result = Image.new(pil_img.mode, (height, height), background_color)
        result.paste(pil_img, ((height - width) // 2, 0))
        return result

def build_sft_instruction_custom(pose_5d, map_name, z_max, z_min):
    # ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´çš„ Prompt æ¨¡æ¿
    definition_text = (
        f"Task: Generate a First-Person View (FPV) image of CS2 map '{map_name}' based on the Radar Map and Camera Pose.\n"
        "Coordinate System Definition:\n"
        "- Map Size: 1024x1024 pixels.\n"
        "- Yaw: 0 degrees is East, increases Clockwise.\n"
        "- Pitch: 0 degrees is looking straight Down (at feet), 180 degrees is looking straight Up (at sky).\n"
        f"- Z-Height: Absolute vertical coordinate. Valid values are bounded by the map's global topology, ranging from the lowest point at {z_min:.2f} to the highest point at {z_max:.2f}."
    )
    pose_str = (
        f"Position(x={pose_5d['x']:.1f}, y={pose_5d['y']:.1f}, z={pose_5d['z']:.3f}), "
        f"Rotation(pitch={pose_5d['angle_v']:.1f}, yaw={pose_5d['angle_h']:.1f})"
    )
    full_instruction = f"{definition_text}\n\nCurrent Camera Pose: {pose_str}\n<image>"
    return full_instruction

def add_template_for_inference(prompt_text):
    # å°† SFT æŒ‡ä»¤åŒ…è£…æˆå¯¹è¯æ ¼å¼
    instruction = ('<|im_start|>user\n{input}<|im_end|>\n'
                   '<|im_start|>assistant\n<img>')

    # Positive Prompt: ä½ çš„ SFT æŒ‡ä»¤
    pos_prompt = instruction.format(input=prompt_text)

    # Negative/CFG Prompt: ä¿æŒè®­ç»ƒæ—¶çš„é€šç”¨æŒ‡ä»¤
    # æ³¨æ„ï¼šè¿™é‡Œ <image> ä¹Ÿè¦åŒ…å«
    cfg_prompt = instruction.format(input="Generate the view.\n<image>")

    return [pos_prompt, cfg_prompt]

# ==========================================
# 2. è½»é‡çº§æ¨ç†æ•°æ®é›† (InferenceDataset)
# ==========================================
class CSGOInferenceDataset(Dataset):
    def __init__(self, config, map_path_dict):
        self.config = config
        self.data_dir = config['data_dir']
        self.map_names = config['val_maps']
        self.map_path_dict = map_path_dict

        self.data_entries = []
        self.map_z_range = {}
        print("ğŸ”„ Loading Test Data...")
        for map_name in self.map_names:
            # è¯»å–æµ‹è¯•é›† split
            # æ³¨æ„ï¼šè¿™é‡Œå¼ºåˆ¶è¯»å– test_split.json
            if self.config.get("is_conti_gen", False):
                split_path = f"{self.data_dir}/{map_name}/splits_20000_5000/continuous_unseen_clips.json"
            else:
                split_path = f"{self.data_dir}/{map_name}/splits_20000_5000/test_split.json"

            with open(split_path, "r", encoding="utf-8") as f:
                positions_data = json.load(f)

            # è®¡ç®— Z èŒƒå›´ (å¿…é¡»åŸºäºå…¨é›†æˆ– Train é›†çš„ç»Ÿè®¡ï¼Œè¿™é‡Œç®€åŒ–ä¸ºå½“å‰ Split çš„ç»Ÿè®¡ï¼Œå»ºè®®æœ€å¥½ç¡¬ç¼–ç æˆ–è¯»å– train_split ç»Ÿè®¡)
            # ä¸ºäº†ä¸¥è°¨ï¼Œè¿™é‡Œåº”è¯¥è¯»å– train_split æ¥è·å– z_min/z_maxï¼Œé˜²æ­¢ test æ•°æ®æº¢å‡º
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šç›´æ¥éå† test set (ç”Ÿäº§ç¯å¢ƒå»ºè®®è¯»å– metadata)
            zs = [d['z'] for d in positions_data]
            self.map_z_range[map_name] = {'max_z': max(zs), 'min_z': min(zs)}

            for pos_data in positions_data:
                entry = {
                    'map': map_name,
                    'file_frame': pos_data['file_frame'],
                    'x': pos_data['x'],
                    'y': pos_data['y'],
                    'z': pos_data['z'],
                    'angle_v': pos_data['angle_v'],
                    'angle_h': pos_data['angle_h'],
                }
                self.data_entries.append(entry)

        if config['debug'] and config.get('debug_num_train_data', False):
            sampled_num = config.get('debug_num_train_data', len(self.data_entries))
            self.data_entries = self.data_entries[:sampled_num]
            print([data['file_frame'] for data in self.data_entries])
        elif config['debug'] and config.get('debug_num_train_data', False) == False:
            indices = [335, 535, 707, 288, 21, 240, 20, 30, 809, 423, 857, 459, 557, 882, 893, 406, 24, 477, 407, 427, 453, 923, 925, 399, 752, 867, 547, 563, 424, 217, 789, 681]
            self.data_entries = [self.data_entries[i] for i in indices]
            print([data['file_frame'] for data in self.data_entries])
        elif config['debug']==False and config.get('debug_num_train_data', False):
            sampled_num = config.get('debug_num_train_data', len(self.data_entries))
            self.data_entries = random.sample(self.data_entries, sampled_num)
            print([data['file_frame'] for data in self.data_entries])

        # ä»…å–å‰Nä¸ªåšæµ‹è¯•ï¼Œé¿å…è·‘å¤ªä¹… (å¯é€‰)
        # self.data_entries = self.data_entries[:50]
        print(f"âœ… Loaded {len(self.data_entries)} test samples.")

    def __len__(self):
        return len(self.data_entries)

    def __getitem__(self, i):
        data = self.data_entries[i]
        map_name = data['map']

        # 1. åŠ è½½ Radar (Input Condition)
        map_filename = self.map_path_dict.get(map_name, 'de_dust2_radar_psd.png')
        radar_path = f"{self.data_dir}/{map_name}/{map_filename}"
        radar_img = Image.open(radar_path).convert('RGB')

        # 2. åŠ è½½ GT FPS (Ground Truth for Vis)
        # æ³¨æ„åç¼€ï¼Œå¦‚æœæ˜¯ preprocessed_data å¯èƒ½æ˜¯ .jpg
        ext = ".jpg" if "preprocessed" in self.data_dir else ".png"
        fps_path = f"{self.data_dir}/{map_name}/imgs/{data['file_frame']}{ext}"
        gt_img = Image.open(fps_path).convert('RGB')

        # 3. å‡†å¤‡ Prompt å‚æ•°
        z_min = self.map_z_range[map_name]['min_z']
        z_max = self.map_z_range[map_name]['max_z']

        # å½’ä¸€åŒ– Z (0-1) ç”¨äº Pose æ•°å€¼å±•ç¤º
        z_norm = (data['z'] - z_min) / (z_max - z_min + 1e-6)

        # å¼§åº¦è½¬è§’åº¦
        pitch_deg = (data['angle_v'] / (2 * np.pi)) * 180.0
        yaw_deg = (data['angle_h'] / (2 * np.pi)) * 360.0

        pose_dict = {
            'x': data['x'], 'y': data['y'], 'z': data['z'],
             'angle_v': pitch_deg, 'angle_h': yaw_deg
        }

        # 4. æ„å»º Prompt
        # æ³¨æ„ï¼šè¿™é‡Œ z_max, z_min ä¼ å…¥çœŸå®ç‰©ç†å€¼ç”¨äºå®šä¹‰
        raw_prompt = build_sft_instruction_custom(pose_dict, map_name, z_max, z_min)

        return {
            "map_name": map_name,
            "radar_img": radar_img,
            "gt_img": gt_img,
            "raw_prompt": raw_prompt,
            "file_frame": data['file_frame'],
            "pose_info": pose_dict
        }

def collate_fn(batch):
    return batch # ç®€å•çš„ list è¿”å›ï¼Œä¸ç”± DataLoader è‡ªåŠ¨ stack tensor

map_path_dict = {
    'de_dust2': 'de_dust2_radar_psd.png',
    'de_inferno': 'de_inferno_radar_psd.png',
    'de_mirage': 'de_mirage_radar_psd.png',
    'de_nuke': 'de_nuke_blended_radar_psd.png',
    'de_ancient': 'de_ancient_radar_psd.png',
    'de_anubis': 'de_anubis_radar_psd.png',
    'de_golden': 'de_golden_radar_tga.png',
    'de_overpass': 'de_overpass_radar_psd.png',
    'de_palacio': 'de_palacio_radar_tga.png',
    'de_train': 'de_train_blended_radar_psd.png',
    'de_vertigo': 'de_vertigo_blended_radar_psd.png',
    'cs_agency': 'cs_agency_radar_tga.png',
    'cs_italy': 'cs_italy_radar_psd.png',
    'cs_office': 'cs_office_radar_psd.png',
}

# ==========================================
# 0. å‡çº§ç‰ˆï¼šæƒé‡åŠ è½½è¾…åŠ©å‡½æ•° (æ”¯æŒ .bin å’Œ .safetensors)
# ==========================================
def load_custom_checkpoint(model, ckpt_path):
    """
    æ™ºèƒ½åŠ è½½æƒé‡ï¼š
    1. åŒºåˆ† .bin å’Œ .safetensors
    2. æ”¯æŒåŠ è½½ pytorch_model.bin / model.safetensors
    3. æ”¯æŒåŠ è½½åˆ†ç¦»ä¿å­˜çš„ mm_projector
    """
    print(f"ğŸš€ Processing checkpoint path: {ckpt_path}")
    ckpt_path = os.path.abspath(ckpt_path)

    state_dict = None

    # --- æƒ…å†µ A: è¿™æ˜¯ä¸€ä¸ªæ–‡ä»¶ (ä¾‹å¦‚ model.safetensors) ---
    if os.path.isfile(ckpt_path):
        print(f"   -> Loading directly from file {ckpt_path} ...")
        if ckpt_path.endswith(".safetensors"):
            state_dict = safe_load_file(ckpt_path, device="cpu")
        else:
            state_dict = torch.load(ckpt_path, map_location="cpu")

        msg = model.load_state_dict(state_dict, strict=False)
        print(f"   -> Direct load msg: {msg}")
        return # è¿™æ˜¯ä¸€ä¸ªå®Œæ•´æƒé‡æ–‡ä»¶ï¼ŒåŠ è½½å®Œç›´æ¥è¿”å›

    # --- æƒ…å†µ B: è¿™æ˜¯ä¸€ä¸ªæ–‡ä»¶å¤¹ (ä¾‹å¦‚ checkpoint-1000) ---
    # 1. å°è¯•åŠ è½½åŸºç¡€æƒé‡ (Base Weights)
    potential_base_files = ["model.safetensors", "pytorch_model.bin"]
    for f in potential_base_files:
        full_path = os.path.join(ckpt_path, f)
        if os.path.exists(full_path):
            print(f"   -> Found base weights: {full_path}")
            if f.endswith(".safetensors"):
                state_dict = safe_load_file(full_path, device="cpu")
            else:
                state_dict = torch.load(full_path, map_location="cpu")

            msg = model.load_state_dict(state_dict, strict=False)
            print(f"   -> Base load msg: {msg}")
            break

    # 2. å°è¯•åŠ è½½ Projector (å¦‚æœè®­ç»ƒä»£ç å°†å…¶å•ç‹¬ä¿å­˜äº†)
    # é€»è¾‘ï¼šæ£€æŸ¥å½“å‰ç›®å½•æˆ–ä¸Šçº§ç›®å½•çš„ mm_projector æ–‡ä»¶å¤¹
    folder_name = os.path.basename(ckpt_path)
    parent_dir = os.path.dirname(ckpt_path)

    projector_types = ["mm_projector", "gen_projector"]

    for proj_type in projector_types:
        candidates = []
        # å€™é€‰ 1: åœ¨å½“å‰ç›®å½•å†…
        candidates.append(os.path.join(ckpt_path, f"{proj_type}.bin"))
        candidates.append(os.path.join(ckpt_path, f"{proj_type}.safetensors"))

        # å€™é€‰ 2: åœ¨ä¸Šçº§å¹³è¡Œç›®å½• (é’ˆå¯¹ checkpoint-xxx ç»“æ„)
        if folder_name.startswith("checkpoint-"):
            candidates.append(os.path.join(parent_dir, proj_type, f"{folder_name}.bin"))
            candidates.append(os.path.join(parent_dir, proj_type, f"{folder_name}.safetensors"))

        loaded_proj = False
        for p_path in candidates:
            if os.path.exists(p_path):
                print(f"   -> Found {proj_type} at: {p_path}")
                if p_path.endswith(".safetensors"):
                    proj_weights = safe_load_file(p_path, device="cpu")
                else:
                    proj_weights = torch.load(p_path, map_location="cpu")

                # åŠ è½½
                model.load_state_dict(proj_weights, strict=False)
                loaded_proj = True
                print(f"      âœ… Loaded {proj_type} successfully.")
                break

        if not loaded_proj and state_dict is None:
             print(f"âš ï¸ Warning: Did not find separate {proj_type} file and no base model loaded.")

# ==========================================
# 3. ä¸»æ¨ç†é€»è¾‘
# ==========================================
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--csgo_config", type=str, required=True)
    args = parser.parse_args()

    with open(args.csgo_config, 'r') as f:
        csgo_config = yaml.safe_load(f)
    print("csgo_config: ", csgo_config)

    # è®¾ç½®éšæœºç§å­
    set_seed()

    cur_time_str = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = f"outputs_eval/{args.csgo_config.split('/')[-1][:-5]}/test_{cur_time_str}"
    os.makedirs(output_dir+"/gen_imgs", exist_ok=True)

    # 1. åŠ è½½æ¨¡å‹
    disable_torch_init()
    # tokenizer, model, context_len = load_pretrained_model_general(
    #     'UniLIP_InternVLForCausalLM', csgo_config['ckpt_path']
    # )
    disable_torch_init()
    tokenizer, model, context_len = load_pretrained_model_general(
        'UniLIP_InternVLForCausalLM', 'UniLIP-1B'
    )

    ckpt_path = csgo_config['ckpt_path']
    print(f"ğŸš€ Loading model from {csgo_config['ckpt_path']}...")
    load_custom_checkpoint(model, ckpt_path)

    image_processor = AutoProcessor.from_pretrained(model.config.mllm_hf_path).image_processor

    # åˆå§‹åŒ– Pipeline
    pipe = CustomEditPipeline(multimodal_encoder=model, tokenizer=tokenizer, image_processor=image_processor)

    test_dataset = CSGOInferenceDataset(
        csgo_config,
        map_path_dict
    )

    dataloader = DataLoader(test_dataset, batch_size=csgo_config["batch_size"], shuffle=False, collate_fn=collate_fn)

    # 3. æ¨ç†å¾ªç¯
    generator = torch.Generator(device=model.device).manual_seed(42)
    print("ğŸš€ Starting Inference...")

    vis_data = [] # å­˜å‚¨ç¬¬ä¸€æ‰¹æ¬¡ç”¨äºå¯è§†åŒ–
    is_vis = True
    vis_data_num = 30

    for batch_idx, batch in enumerate(tqdm(dataloader)):
        # æ‰¹æ¬¡å†…çš„æ¯ä¸ªæ ·æœ¬é€ä¸ªå¤„ç† (å› ä¸º Pipe æ¥å£é€šå¸¸æ¥å— List[Prompt] ä½†å¯¹åº”å•å¼ å›¾ç‰‡è¾“å…¥)
        # ä¸ºäº†å…¼å®¹ CustomEditPipeline çš„é€»è¾‘ (multimodal_prompts list ç»“æ„)

        for sample in batch:
            radar_img = sample['radar_img']
            raw_prompt = sample['raw_prompt']
            file_frame = sample['file_frame']

            # æ„é€  UniLIP æ ¼å¼çš„ multimodal prompts
            # [Positive_Prompt, Negative_Prompt, Image]
            multimodal_prompts = add_template_for_inference(raw_prompt)
            multimodal_prompts.append(radar_img) # å¿…é¡» append PIL Image å¯¹è±¡

            # æ‰§è¡Œç”Ÿæˆ
            with torch.no_grad():
                gen_img = pipe(
                    multimodal_prompts,
                    guidance_scale=csgo_config["guidance_scale"],
                    generator=generator
                )

            os.makedirs(output_dir+f"/gen_imgs/{sample['map_name']}", exist_ok=True)
            save_name = f"gen_imgs/{sample['map_name']}/{file_frame}.jpg"
            gen_img.save(os.path.join(output_dir, save_name))

            # æ”¶é›†æ•°æ®ç”¨äºå¯è§†åŒ–
            if len(vis_data) < vis_data_num:
                # ä¿å­˜å•å¼ å›¾ç‰‡

                vis_data.append({
                    "map_name": sample['map_name'],
                    "radar": radar_img,
                    "gt": sample['gt_img'],
                    "gen": gen_img,
                    "pose": sample['pose_info'],
                    "prompt": raw_prompt
                })

            if is_vis and len(vis_data) == vis_data_num:
                print(f"ğŸ“Š Generating Visualization for {len(vis_data)} samples...")
                batch_size = 5
                # æŒ‰æ­¥é•¿ batch_size å¾ªç¯
                for batch_start in range(0, len(vis_data), batch_size):
                    batch_end = min(batch_start + batch_size, len(vis_data))
                    batch_items = vis_data[batch_start:batch_end]

                    # åˆ›å»ºç”»å¸ƒï¼š5è¡Œ4åˆ—
                    # figsize éœ€è¦è®¾ç½®å®½ä¸€ç‚¹ä»¥å®¹çº³æ–‡æœ¬ï¼Œé«˜ä¸€ç‚¹ä»¥å®¹çº³5è¡Œ
                    fig, axes = plt.subplots(batch_size, 4, figsize=(24, 6 * batch_size))

                    # å¦‚æœåªæœ‰ä¸€è¡Œï¼ˆbatch_size=1çš„æƒ…å†µï¼‰ï¼Œaxesæ˜¯ä¸€ç»´æ•°ç»„ï¼Œå¼ºåˆ¶è½¬ä¸ºäºŒç»´
                    if batch_size == 1:
                        axes = [axes]
                    # å¦‚æœæœ€åä¸æ»¡5ä¸ªï¼Œaxesä»ç„¶æ˜¯5è¡Œï¼Œéœ€è¦å¤„ç†ç©ºè¡Œ
                    elif len(batch_items) < batch_size:
                        pass

                    for i in range(batch_size):
                        # è·å–å½“å‰è¡Œå¯¹åº”çš„ axes
                        ax_row = axes[i]

                        # å¦‚æœå½“å‰æ‰¹æ¬¡çš„æ•°æ®å·²ç»ç”¨å®Œï¼ˆå¤„ç†æœ€åä½™æ•°çš„æƒ…å†µï¼‰ï¼Œéšè—å‰©ä¸‹çš„ç©ºå›¾
                        if i >= len(batch_items):
                            for ax in ax_row:
                                ax.axis('off')
                            continue

                        item = batch_items[i]

                        # --- ç¬¬1åˆ—ï¼šPrompt Text ---
                        # ä½¿ç”¨ textwrap è‡ªåŠ¨æ¢è¡Œï¼Œé˜²æ­¢æ–‡æœ¬æº¢å‡º
                        wrapped_prompt = "\n".join(textwrap.wrap(item['prompt'], width=40))

                        ax_row[0].text(0, 1, wrapped_prompt, ha='left', va='top', fontsize=18, wrap=True)
                        ax_row[0].axis('off') # å…³é—­åæ ‡è½´
                        if i == 0: ax_row[0].set_title("Input Prompt", fontsize=14, fontweight='bold')

                        # --- ç¬¬2åˆ—ï¼šRadar ---
                        ax_row[1].imshow(item['radar'])
                        ax_row[1].axis('off')
                        if i == 0: ax_row[1].set_title("Input Radar", fontsize=14, fontweight='bold')

                        # --- ç¬¬3åˆ—ï¼šGT ---
                        ax_row[2].imshow(item['gt'])
                        ax_row[2].axis('off')
                        if i == 0: ax_row[2].set_title("Ground Truth", fontsize=14, fontweight='bold')

                        # --- ç¬¬4åˆ—ï¼šGenerated ---
                        ax_row[3].imshow(item['gen'])
                        ax_row[3].axis('off')

                        # æŠŠå…·ä½“çš„ Pose æ•°å€¼æ”¾åœ¨ç”Ÿæˆå›¾çš„æ ‡é¢˜ä¸Šï¼Œä½œä¸ºè¡¥å……ä¿¡æ¯
                        p = item['pose']
                        pose_str = f"{item['map_name']}, Pred View\nPos:({p['x']:.0f},{p['y']:.0f},{p['z']:.0f}) Ang:({p['angle_v']:.0f},{p['angle_h']:.0f})"
                        ax_row[3].set_title(pose_str if i > 0 else f"Generated\n{pose_str}", fontsize=10, color='blue')

                    plt.tight_layout()

                    # ä¿å­˜æ–‡ä»¶åï¼švis_0_5.jpg, vis_5_10.jpg ...
                    vis_save_path = os.path.join(output_dir, f"vis_batch_{batch_start}_{batch_end}.jpg")
                    plt.savefig(vis_save_path, dpi=100) # dpi 100 è¶³å¤Ÿæ¸…æ™°ä¸”ä½“ç§¯é€‚ä¸­
                    plt.close(fig) # æå…¶é‡è¦ï¼šå¾ªç¯ç”»å›¾å¿…é¡» closeï¼Œå¦åˆ™å†…å­˜æ³„æ¼

                    print(f"   -> Saved visualization batch: {vis_save_path}")

                is_vis = False

    print(f"âœ… Inference finished. Results saved to {output_dir}")

if __name__ == "__main__":
    main()



# python eval_csgo.py --csgo_config csgo_configs/test/exp0.yaml