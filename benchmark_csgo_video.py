import os
import argparse
import re
import torch
import numpy as np
from PIL import Image
from tqdm import tqdm
from glob import glob
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms

# å¼•å…¥æŒ‡æ ‡åº“
from torchmetrics.image.fid import FrechetInceptionDistance
from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity
from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure
# æ–°å¢: FVD
from torchmetrics.video import FrechetVideoDistance

from transformers import CLIPProcessor, CLIPModel

# æ–°å¢: DreamSim (å°è¯•å¯¼å…¥ï¼Œé¿å…æœªå®‰è£…æŠ¥é”™)
try:
    from dreamsim import dreamsim
    DREAMSIM_AVAILABLE = True
except ImportError:
    DREAMSIM_AVAILABLE = False
    print("âš ï¸ DreamSim library not found. Install via 'pip install dreamsim' to use --dreamsim")

# ---------------------------------------------------------
# Utils: æ•°æ®åŠ è½½
# ---------------------------------------------------------

class PairedImageDataset(Dataset):
    """
    ç”¨äº PSNR, SSIM, LPIPS, DreamSim, CLIP-I ç­‰éœ€è¦ä¸€ä¸€å¯¹åº”è®¡ç®—çš„æŒ‡æ ‡
    """
    def __init__(self, gt_dir, pred_dir, size=(224, 224)):
        self.gt_dir = gt_dir
        self.pred_dir = pred_dir
        self.gt_images = sorted(os.listdir(gt_dir))
        self.pred_images = sorted(os.listdir(pred_dir))
        self.filenames = [f for f in self.gt_images if f in self.pred_images]

        if len(self.filenames) == 0:
            raise ValueError(f"No common filenames found between {gt_dir} and {pred_dir}")

        self.transform = transforms.Compose([
            transforms.Resize(size),
            transforms.ToTensor(),
        ])

    def __len__(self):
        return len(self.filenames)

    def __getitem__(self, idx):
        filename = self.filenames[idx]
        gt_path = os.path.join(self.gt_dir, filename)
        pred_path = os.path.join(self.pred_dir, filename)

        gt_img = Image.open(gt_path).convert('RGB')
        pred_img = Image.open(pred_path).convert('RGB')

        return self.transform(gt_img), self.transform(pred_img)

class SingleImageDataset(Dataset):
    """ç”¨äº Aesthetic Score æˆ– FID"""
    def __init__(self, img_dir, filter_list=None, size=(224, 224)):
        self.img_dir = img_dir
        all_files = sorted(os.listdir(img_dir))

        # å¦‚æœæä¾›äº† filter_list (æ¯”å¦‚åªè®¡ç®—é…å¯¹çš„å›¾ç‰‡)ï¼Œåˆ™è¿‡æ»¤
        if filter_list:
            self.filenames = [f for f in all_files if f in filter_list]
        else:
            self.filenames = all_files

        self.transform = transforms.Compose([
            transforms.Resize(size),
            transforms.ToTensor(),
            transforms.ConvertImageDtype(torch.uint8)
        ])

        self.raw_transform = transforms.Compose([
             transforms.Resize(size),
             transforms.ToTensor()
        ])

    def __len__(self):
        return len(self.filenames)

    def __getitem__(self, idx):
        filename = self.filenames[idx]
        path = os.path.join(self.img_dir, filename)
        img = Image.open(path).convert('RGB')
        return self.transform(img), self.raw_transform(img)

# --- æ–°å¢: è§†é¢‘ç‰‡æ®µæ•°æ®é›† (ç”¨äº FVD) ---
class VideoClipDataset(Dataset):
    """
    å°†æ•£è£…çš„å¸§å›¾ç‰‡è‡ªåŠ¨ç»„è£…æˆè§†é¢‘ç‰‡æ®µ (Clip)ã€‚
    FVD (I3D) é€šå¸¸è¦æ±‚è¾“å…¥ä¸º 16 å¸§çš„ç‰‡æ®µã€‚
    """
    def __init__(self, img_dir, clip_length=16, size=(224, 224)):
        self.img_dir = img_dir
        self.clip_length = clip_length
        self.size = size

        # 1. æ‰«æå¹¶è§£ææ‰€æœ‰æ–‡ä»¶
        files = []
        for f in os.listdir(img_dir):
            if f.lower().endswith(('.jpg', '.png', '.jpeg')):
                match = re.search(r'file_num(\d+)_frame_(\d+)', f)
                if match:
                    files.append({
                        'path': os.path.join(img_dir, f),
                        'file_num': int(match.group(1)),
                        'frame_id': int(match.group(2))
                    })

        # 2. æ’åº
        files.sort(key=lambda x: (x['file_num'], x['frame_id']))

        # 3. åˆ†ç»„ä¸º Clips
        self.clips = []
        current_clip = []
        for i, item in enumerate(files):
            if not current_clip:
                current_clip.append(item)
                continue

            last_item = current_clip[-1]
            # è¿ç»­æ€§æ£€æŸ¥: åŒæ–‡ä»¶ & å¸§å·è¿ç»­ (å…è®¸è·³å¸§é˜ˆå€¼2)
            if item['file_num'] == last_item['file_num'] and \
               (item['frame_id'] - last_item['frame_id'] <= 2):
                current_clip.append(item)
            else:
                # è¿ç»­æ€§ä¸­æ–­ï¼Œé‡ç½®
                current_clip = [item]

            # å¦‚æœå‡‘å¤Ÿäº† clip_length å¸§ï¼Œä¿å­˜ä¸ºä¸€ä¸ªæ ·æœ¬
            if len(current_clip) == self.clip_length:
                self.clips.append(current_clip)
                # æ»‘åŠ¨çª—å£: å¦‚æœæƒ³éé‡å åˆ‡åˆ†ï¼Œè¿™é‡Œæ¸…ç©º current_clip
                # å¦‚æœæƒ³é‡å åˆ‡åˆ†(æ•°æ®æ›´å¤š)ï¼Œè¿™é‡Œ current_clip.pop(0)
                # FVD è®¡ç®—é€šå¸¸ä½¿ç”¨éé‡å ç‰‡æ®µå³å¯
                current_clip = []

        print(f"ğŸ¬ Found {len(self.clips)} valid video clips (len={clip_length}) in {img_dir}")

        self.transform = transforms.Compose([
            transforms.Resize(size),
            transforms.PILToTensor(), # FVD éœ€è¦ uint8 Tensor [0, 255]
        ])

    def __len__(self):
        return len(self.clips)

    def __getitem__(self, idx):
        clip_info = self.clips[idx]
        frames = []
        for info in clip_info:
            img = Image.open(info['path']).convert('RGB')
            # transform è¿”å› (C, H, W) uint8
            frames.append(self.transform(img))

        # Stack -> (T, C, H, W)
        video_tensor = torch.stack(frames)

        # FVD è¦æ±‚è¾“å…¥æ ¼å¼: (B, C, T, H, W) æˆ– (B, T, C, H, W)
        # torchmetrics FVD æ–‡æ¡£å»ºè®® (B, C, T, H, W)ï¼Œè¿™é‡Œè¿”å› (C, T, H, W) ç»™ loader å †å 
        return video_tensor.permute(1, 0, 2, 3)


# ---------------------------------------------------------
# Metric 1: FID
# ---------------------------------------------------------
def compute_fid(gt_dir, pred_dir, batch_size, device):
    print(f"ğŸ”„ Calculating FID on {device}...")
    fid = FrechetInceptionDistance(feature=2048).to(device)

    # è·å–æ–‡ä»¶åäº¤é›†åˆ—è¡¨
    gt_files = set(os.listdir(gt_dir))
    pred_files = set(os.listdir(pred_dir))
    common_files = sorted(list(gt_files.intersection(pred_files)))

    dataset_gt = SingleImageDataset(gt_dir, filter_list=common_files, size=(299, 299))
    dataset_pred = SingleImageDataset(pred_dir, filter_list=common_files, size=(299, 299))

    loader_gt = DataLoader(dataset_gt, batch_size=batch_size, num_workers=4)
    loader_pred = DataLoader(dataset_pred, batch_size=batch_size, num_workers=4)

    for batch_uint8, _ in tqdm(loader_gt, desc="FID (GT)"):
        fid.update(batch_uint8.to(device), real=True)

    for batch_uint8, _ in tqdm(loader_pred, desc="FID (Pred)"):
        fid.update(batch_uint8.to(device), real=False)

    fid_score = fid.compute()
    print(f"âœ… FID Score: {fid_score.item():.4f}")
    return fid_score.item()

# ---------------------------------------------------------
# Metric 2: LPIPS
# ---------------------------------------------------------
def compute_lpips(gt_dir, pred_dir, batch_size, device):
    print(f"ğŸ”„ Calculating LPIPS on {device}...")
    lpips = LearnedPerceptualImagePatchSimilarity(net_type='alex').to(device)
    dataset = PairedImageDataset(gt_dir, pred_dir)
    loader = DataLoader(dataset, batch_size=batch_size, num_workers=4)

    total_lpips = 0.0
    count = 0
    with torch.no_grad():
        for gt_batch, pred_batch in tqdm(loader, desc="LPIPS"):
            gt_batch = (gt_batch.to(device) * 2.0 - 1.0)
            pred_batch = (pred_batch.to(device) * 2.0 - 1.0)
            score = lpips(pred_batch, gt_batch)
            total_lpips += score.item() * gt_batch.size(0)
            count += gt_batch.size(0)

    avg = total_lpips / count
    print(f"âœ… LPIPS Score: {avg:.4f}")
    return avg

# ---------------------------------------------------------
# Metric 3 & 4: PSNR & SSIM
# ---------------------------------------------------------
def compute_psnr_ssim(gt_dir, pred_dir, batch_size, device):
    print(f"ğŸ”„ Calculating PSNR & SSIM on {device}...")
    psnr_metric = PeakSignalNoiseRatio(data_range=1.0).to(device)
    ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)
    dataset = PairedImageDataset(gt_dir, pred_dir)
    loader = DataLoader(dataset, batch_size=batch_size, num_workers=4)

    total_psnr = 0.0
    total_ssim = 0.0
    count = 0
    with torch.no_grad():
        for gt_batch, pred_batch in tqdm(loader, desc="PSNR/SSIM"):
            gt_batch, pred_batch = gt_batch.to(device), pred_batch.to(device)
            total_psnr += psnr_metric(pred_batch, gt_batch).item() * gt_batch.size(0)
            total_ssim += ssim_metric(pred_batch, gt_batch).item() * gt_batch.size(0)
            count += gt_batch.size(0)

    print(f"âœ… PSNR: {total_psnr/count:.4f} | SSIM: {total_ssim/count:.4f}")
    return total_psnr/count, total_ssim/count

# ---------------------------------------------------------
# Metric 5: CLIP Score
# ---------------------------------------------------------
def compute_clip_score(gt_dir, pred_dir, batch_size, device):
    print(f"ğŸ”„ Calculating CLIP Score on {device}...")
    model_name = "openai/clip-vit-base-patch32"
    model = CLIPModel.from_pretrained(model_name).to(device)
    dataset = PairedImageDataset(gt_dir, pred_dir)
    loader = DataLoader(dataset, batch_size=batch_size, num_workers=4)

    total_score = 0.0
    count = 0
    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).view(1, 3, 1, 1).to(device)
    std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).view(1, 3, 1, 1).to(device)

    with torch.no_grad():
        for gt_batch, pred_batch in tqdm(loader, desc="CLIP Score"):
            gt_norm = (gt_batch.to(device) - mean) / std
            pred_norm = (pred_batch.to(device) - mean) / std

            gt_emb = model.get_image_features(pixel_values=gt_norm)
            pred_emb = model.get_image_features(pixel_values=pred_norm)

            gt_emb = gt_emb / gt_emb.norm(dim=1, keepdim=True)
            pred_emb = pred_emb / pred_emb.norm(dim=1, keepdim=True)

            total_score += (gt_emb * pred_emb).sum(dim=1).sum().item()
            count += gt_batch.size(0)

    avg = total_score / count
    print(f"âœ… CLIP Score: {avg:.4f}")
    return avg

# ---------------------------------------------------------
# Metric 6: Aesthetic Score
# ---------------------------------------------------------
class AestheticPredictor(torch.nn.Module):
    def __init__(self, input_size):
        super().__init__()
        self.layers = torch.nn.Sequential(
            torch.nn.Linear(input_size, 1024), torch.nn.Dropout(0.2),
            torch.nn.Linear(1024, 128), torch.nn.Dropout(0.2),
            torch.nn.Linear(128, 64), torch.nn.Dropout(0.1),
            torch.nn.Linear(64, 16), torch.nn.Linear(16, 1)
        )
    def forward(self, x): return self.layers(x)

def compute_aesthetic_score(pred_dir, batch_size, device):
    print(f"ğŸ”„ Calculating Aesthetic Score on {device}...")
    model_name = "openai/clip-vit-large-patch14"
    clip_model = CLIPModel.from_pretrained(model_name).to(device)

    weight_url = "https://github.com/christophschuhmann/improved-aesthetic-predictor/raw/main/sac+logos+ava1-l14-linearMSE.pth"
    weight_path = "aesthetic_model.pth"
    if not os.path.exists(weight_path):
        torch.hub.download_url_to_file(weight_url, weight_path)

    predictor = AestheticPredictor(768)
    predictor.load_state_dict(torch.load(weight_path, map_location=device))
    predictor.to(device).eval()

    dataset = SingleImageDataset(pred_dir, size=(224, 224))
    loader = DataLoader(dataset, batch_size=batch_size, num_workers=4)

    total = 0.0
    count = 0
    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).view(1, 3, 1, 1).to(device)
    std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).view(1, 3, 1, 1).to(device)

    with torch.no_grad():
        for _, img_batch in tqdm(loader, desc="Aesthetic"):
            img_norm = (img_batch.to(device) - mean) / std
            features = clip_model.get_image_features(pixel_values=img_norm)
            features = features / features.norm(dim=1, keepdim=True)
            total += predictor(features.float()).sum().item()
            count += img_batch.size(0)

    avg = total / count
    print(f"âœ… Aesthetic Score: {avg:.4f}")
    return avg

# ---------------------------------------------------------
# New Metric 7: DreamSim (Perceptual)
# ---------------------------------------------------------
def compute_dreamsim(gt_dir, pred_dir, batch_size, device):
    if not DREAMSIM_AVAILABLE:
        print("âŒ DreamSim not installed. Skipping.")
        return 0.0

    print(f"ğŸ”„ Calculating DreamSim on {device}...")
    # DreamSim model loading (pretrained=True loads OpenCLIP-ViT-B-32 variant by default)
    model, preprocess = dreamsim(pretrained=True, device=device)

    dataset = PairedImageDataset(gt_dir, pred_dir)
    loader = DataLoader(dataset, batch_size=batch_size, num_workers=4)

    total_dist = 0.0
    count = 0

    with torch.no_grad():
        for gt_batch, pred_batch in tqdm(loader, desc="DreamSim"):
            # DreamSim handles normalization internally if using their preprocess,
            # but here we have standard tensors [0,1].
            # DreamSim forward expects tensors.
            gt_batch = gt_batch.to(device)
            pred_batch = pred_batch.to(device)

            # DreamSim returns distance for each pair
            dist = model(pred_batch, gt_batch)
            total_dist += dist.sum().item()
            count += gt_batch.size(0)

    avg = total_dist / count
    print(f"âœ… DreamSim Score: {avg:.4f} (Lower is better)")
    return avg

# ---------------------------------------------------------
# New Metric 8: FVD (FrÃ©chet Video Distance)
# ---------------------------------------------------------
def compute_fvd(gt_dir, pred_dir, batch_size, device):
    print(f"ğŸ”„ Calculating FVD on {device}...")
    # I3D æ˜¯ FVD çš„æ ‡å‡†ç‰¹å¾æå–å™¨
    fvd = FrechetVideoDistance(feature_extractor="i3d400", reset_real_features=False, reset_fake_features=False).to(device)

    # è§†é¢‘æ•°æ®å¤„ç†: 16å¸§ä¸ºä¸€ä¸ªClip
    # FVD éœ€è¦è§†é¢‘è¾“å…¥ï¼Œæˆ‘ä»¬éœ€è¦æŠŠå›¾ç‰‡ç»„åˆæˆè§†é¢‘ç‰‡æ®µ
    clip_len = 16
    dataset_gt = VideoClipDataset(gt_dir, clip_length=clip_len, size=(224, 224))
    dataset_pred = VideoClipDataset(pred_dir, clip_length=clip_len, size=(224, 224))

    if len(dataset_gt) == 0 or len(dataset_pred) == 0:
        print(f"âŒ Not enough contiguous frames for FVD (Need {clip_len} frames per clip). Skipping.")
        return 0.0

    # FVD æ¯”è¾ƒè€—æ˜¾å­˜ï¼Œå»ºè®® batch_size è°ƒå°
    vid_batch_size = max(1, batch_size // 4)

    loader_gt = DataLoader(dataset_gt, batch_size=vid_batch_size, num_workers=4)
    loader_pred = DataLoader(dataset_pred, batch_size=vid_batch_size, num_workers=4)

    print(f"  - Found {len(dataset_gt)} Real Clips, {len(dataset_pred)} Fake Clips.")

    # Update GT (Real)
    for batch_vid in tqdm(loader_gt, desc="FVD (Real Clips)"):
        # batch_vid shape: (B, C, T, H, W), uint8 [0, 255]
        fvd.update(batch_vid.to(device), real=True)

    # Update Pred (Fake)
    for batch_vid in tqdm(loader_pred, desc="FVD (Fake Clips)"):
        fvd.update(batch_vid.to(device), real=False)

    fvd_score = fvd.compute()
    print(f"âœ… FVD Score: {fvd_score.item():.4f} (Lower is better)")
    return fvd_score.item()


# ---------------------------------------------------------
# Main
# ---------------------------------------------------------
def main():
    parser = argparse.ArgumentParser(description="Evaluate Image/Video Generation Metrics")
    parser.add_argument("--gt", type=str, required=True, help="Path to GT images")
    parser.add_argument("--pred", type=str, required=True, help="Path to Generated images")
    parser.add_argument("--batch_size", type=int, default=32)
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu")

    parser.add_argument("--all", action="store_true", help="Run ALL metrics")
    parser.add_argument("--fid", action="store_true")
    parser.add_argument("--lpips", action="store_true")
    parser.add_argument("--psnr_ssim", action="store_true")
    parser.add_argument("--clip", action="store_true")
    parser.add_argument("--aesthetic", action="store_true")
    # New flags
    parser.add_argument("--dreamsim", action="store_true", help="Calculate DreamSim (Perceptual)")
    parser.add_argument("--fvd", action="store_true", help="Calculate FVD (Video)")

    args = parser.parse_args()
    results = {}

    if args.all or args.psnr_ssim:
        p, s = compute_psnr_ssim(args.gt, args.pred, args.batch_size, args.device)
        results['PSNR'], results['SSIM'] = p, s

    if args.all or args.lpips:
        results['LPIPS'] = compute_lpips(args.gt, args.pred, args.batch_size, args.device)

    if args.all or args.dreamsim:
        results['DreamSim'] = compute_dreamsim(args.gt, args.pred, args.batch_size, args.device)

    if args.all or args.fid:
        results['FID'] = compute_fid(args.gt, args.pred, args.batch_size, args.device)

    if args.all or args.fvd:
        results['FVD'] = compute_fvd(args.gt, args.pred, args.batch_size, args.device)

    if args.all or args.clip:
        results['CLIP'] = compute_clip_score(args.gt, args.pred, args.batch_size, args.device)

    if args.all or args.aesthetic:
        results['Aesthetic'] = compute_aesthetic_score(args.pred, args.batch_size, args.device)

    print("\n" + "="*30)
    print("ğŸ“Š Final Results")
    print("="*30)
    for k, v in results.items():
        print(f"{k}: {v:.4f}")
    print("="*30)

if __name__ == "__main__":
    main()