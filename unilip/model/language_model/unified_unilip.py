from abc import ABC, abstractmethod
import math
import copy
from typing import List, Optional, Tuple, Union, Dict
import numpy as np
import cv2
import logging

import torch
import torch.nn as nn
import torch.nn.functional as F
from omegaconf import OmegaConf

# Transformers & Diffusers
from transformers.models.qwen2.modeling_qwen2 import Qwen2RMSNorm
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, AutoConfig, InternVLConfig, InternVLModel, InternVLForConditionalGeneration
from transformers.modeling_outputs import CausalLMOutputWithPast
from transformers.generation.utils import GenerateOutput
from diffusers.schedulers import FlowMatchEulerDiscreteScheduler, DPMSolverMultistepScheduler
from diffusers.training_utils import compute_density_for_timestep_sampling
from diffusers.utils.torch_utils import randn_tensor

# Local Imports (Assuming these exist in your project structure)
from ..sana import build_sana
from ..vae_modules import DCAE_Decoder
from unilip.constants import DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IMAGE_TOKEN_IDX, DEFAULT_IM_START_TOKEN_IDX, DEFAULT_IM_END_TOKEN_IDX, UND_IMAGE_TOKEN_IDX

from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training, PeftModel


# [NEW] Helper Functions from Pi0.5 (Ported)
def get_safe_dtype(target_dtype, device_type):
    if device_type == "cpu":
        if target_dtype == torch.bfloat16:
            return torch.float32
        if target_dtype == torch.float64:
            return torch.float64
    return target_dtype

def create_sinusoidal_pos_embedding(
    time: torch.tensor, dimension: int, min_period: float, max_period: float, device="cpu"
) -> torch.Tensor:
    if dimension % 2 != 0:
        raise ValueError(f"dimension ({dimension}) must be divisible by 2")
    if time.ndim != 1:
        raise ValueError("The time tensor is expected to be of shape `(batch_size, )`.")
    dtype = get_safe_dtype(torch.float64, device.type)
    fraction = torch.linspace(0.0, 1.0, dimension // 2, dtype=dtype, device=device)
    period = min_period * (max_period / min_period) ** fraction
    scaling_factor = 1.0 / period * 2 * math.pi
    sin_input = scaling_factor[None, :] * time[:, None]
    return torch.cat([torch.sin(sin_input), torch.cos(sin_input)], dim=1)

def sample_beta(alpha, beta, bsize, device):
    alpha_t = torch.as_tensor(alpha, dtype=torch.float32, device=device)
    beta_t = torch.as_tensor(beta, dtype=torch.float32, device=device)
    dist = torch.distributions.Beta(alpha_t, beta_t)
    return dist.sample((bsize,))
# é€è¡Œè§£é‡Š
#   alpha_t = torch.as_tensor(alpha, ...)å°† alpha å‚æ•°ï¼ˆé€šå¸¸æ˜¯ä¸€ä¸ª Python æµ®ç‚¹æ•°ï¼Œä¾‹å¦‚ 0.5ï¼‰è½¬æ¢ä¸ºä¸€ä¸ª PyTorch å¼ é‡ã€‚
#   beta_t = torch.as_tensor(beta, ...)å°† beta å‚æ•°è½¬æ¢ä¸ºä¸€ä¸ª PyTorch å¼ é‡ã€‚
#   dist = torch.distributions.Beta(alpha_t, beta_t)è¿™æ˜¯æ ¸å¿ƒä»£ç ã€‚å®ƒåˆ›å»ºäº†ä¸€ä¸ª Beta åˆ†å¸ƒçš„å®ä¾‹ (object)ã€‚è¿™ä¸ªå®ä¾‹ "çŸ¥é“" å®ƒçš„å½¢çŠ¶æ˜¯ç”± alpha å’Œ beta å®šä¹‰çš„ã€‚
#   return dist.sample((bsize,))è°ƒç”¨è¯¥åˆ†å¸ƒå®ä¾‹çš„ .sample() æ–¹æ³•ï¼Œä»ä¸­æŠ½å– bsize ä¸ªéšæœºæ ·æœ¬ã€‚è¿”å›çš„å¼ é‡å½¢çŠ¶ä¸º [bsize]ï¼Œå…¶ä¸­çš„æ¯ä¸ªå€¼éƒ½æ˜¯ 0.0 åˆ° 1.0 ä¹‹é—´çš„ä¸€ä¸ªæµ®ç‚¹æ•°ã€‚
# $\alpha$ å’Œ $\beta$ çš„ä½œç”¨
    # alpha å’Œ beta æ§åˆ¶ç€åˆ†å¸ƒçš„â€œåå¥½â€ï¼š
    #   alpha = 1, beta = 1:è¿™æ˜¯å‡åŒ€åˆ†å¸ƒ (Uniform Distribution)ã€‚dist.sample() çš„è¡Œä¸ºä¸ torch.rand(bsize) ç›¸åŒï¼ˆ0 åˆ° 1 ä¹‹é—´çš„å®Œå…¨éšæœºæ•°ï¼‰ã€‚
    #   alpha > 1, beta > 1:åˆ†å¸ƒå‘ˆé’Ÿå½¢ï¼Œæ ·æœ¬ä¼šèšé›†åœ¨ä¸­é—´ï¼ˆä¾‹å¦‚ï¼Œalpha=5, beta=5 æ—¶ï¼Œæ ·æœ¬ä¼šèšé›†åœ¨ 0.5 é™„è¿‘ï¼‰ã€‚
    #   alpha < 1, beta < 1:åˆ†å¸ƒå‘ˆ U å½¢ï¼ˆæµ´ç¼¸å½¢çŠ¶ï¼‰ï¼Œæ ·æœ¬ä¼šèšé›†åœ¨ä¸¤ç«¯ï¼ˆ0.0 æˆ– 1.0 é™„è¿‘ï¼‰ã€‚
    #   alpha > beta (ä¾‹å¦‚ alpha=5, beta=1):åˆ†å¸ƒå‘å³åç§»ï¼Œæ ·æœ¬ä¼šèšé›†åœ¨ 1.0 é™„è¿‘ã€‚
    #   alpha < beta (ä¾‹å¦‚ alpha=1, beta=5):åˆ†å¸ƒå‘å·¦åç§»ï¼Œæ ·æœ¬ä¼šèšé›†åœ¨ 0.0 é™„è¿‘ã€‚
# å¸¸è§ç”¨é€”: è¿™ä¸ªå‡½æ•°ç»å¸¸ç”¨äºåƒ MixUp è¿™æ ·çš„æ•°æ®å¢å¼ºæŠ€æœ¯ä¸­ï¼Œç”¨æ¥ç”Ÿæˆä¸€ä¸ªéå‡åŒ€çš„æ··åˆæ¯”ä¾‹ï¼ˆ$\lambda$ï¼‰ã€‚

# æ‰‹åŠ¨åˆå§‹åŒ–æƒé‡ (ä½¿ç”¨ Xavier æˆ– Small Normal)
def init_weights(m):
    if isinstance(m, nn.Linear):
        # ä½¿ç”¨è¾ƒå°çš„æ ‡å‡†å·®ï¼Œç¡®ä¿åˆå§‹è¾“å‡ºæ¥è¿‘ 0
        nn.init.normal_(m.weight, std=0.02)
        if m.bias is not None:
            nn.init.zeros_(m.bias)

# Aution_AE.decoderåˆå§‹åŒ–0.002
def small_init_weights(m):
    if isinstance(m, nn.Linear):
        # ä½¿ç”¨è¾ƒå°çš„æ ‡å‡†å·®ï¼Œç¡®ä¿åˆå§‹è¾“å‡ºæ¥è¿‘ 0
        nn.init.normal_(m.weight, std=0.001)
        if m.bias is not None:
            nn.init.zeros_(m.bias)

# ==========================================
# 1. MetaModel: å®šä¹‰ç»„ä»¶ (Connectors & Heads)
# ==========================================
class Unified_UniLIP_InternVL_MetaModel:

    def __init__(self, config):
        super(Unified_UniLIP_InternVL_MetaModel, self).__init__(config)

        # --- A. Existing Logic (Vision Tower & LLM Setup) ---
        if hasattr(config, "n_query"):
            path = config.mllm_path
            internvl_model = AutoModel.from_pretrained(
                path,
                torch_dtype=torch.bfloat16, # Assuming bf16 training
                low_cpu_mem_usage=True,
                trust_remote_code=True
            )
            self.vision_tower = internvl_model.vision_model
            self.multi_modal_projector = internvl_model.mlp1

            # Disable dropout for vision tower
            for layer in self.vision_tower.encoder.layers:
                try:
                    layer.drop_path1.drop_prob = 0.0
                    layer.drop_path2.drop_prob = 0.0
                except:
                    continue
            self.vision_tower.eval()
            self.multi_modal_projector.eval()

            # Latent Queries for Generation Task
            if 'hidden_size' in self.config:
                hidden_size = self.config.hidden_size
            else:
                hidden_size = self.config.text_config.hidden_size

            self.latent_queries = nn.Parameter(torch.randn(1, config.n_query, hidden_size))

            # --- B. Generation Path (DiT & Connector) ---
            self.dit, self.vae, self.noise_scheduler = build_sana(config.dit_path)

            # VAE Decoder for Latent Space
            vae_config_dict = {'model': {'dc_ae_path': config.vae_path}}
            vae_config_omega = OmegaConf.create(vae_config_dict)
            llm_hidden_size = self.multi_modal_projector[-1].weight.shape[-1]
            self.vae_decoder = DCAE_Decoder(vae_config_omega, llm_hidden_size)

            # LLM Connector (Slice of InternVL for DiT)
            path = config.mllm_hf_path
            internvl_model = AutoModel.from_pretrained(
                path,
                torch_dtype=torch.bfloat16,
                low_cpu_mem_usage=True,
                trust_remote_code=True,
                attn_implementation="eager"
            )
            self.llm_connector = copy.deepcopy(internvl_model.language_model)
            del self.llm_connector.layers[:-self.config.connect_layer]
            del self.llm_connector.embed_tokens
            self.projector = nn.Linear(llm_hidden_size, self.dit.config.caption_channels)

            # --- C. [NEW] Localization Path (Action Connector & Flow Matching Heads) ---
            # if getattr(self.config, "is_loc_learnable_query", False):
            #     self.loc_learnable_query = nn.Parameter(torch.randn(1, 1, llm_hidden_size))
            # if getattr(self.config, "is_action_dit_projector", False):
            #     self.action_dit_projector = nn.Sequential(
            #         nn.Linear(llm_hidden_size, llm_hidden_size*4, bias=True),
            #         nn.GELU(),
            #         nn.Linear(llm_hidden_size*4, llm_hidden_size*2, bias=True),
            #         nn.GELU(),
            #         nn.Linear(llm_hidden_size*2, llm_hidden_size, bias=True),
            #     )
            # è¾“å…¥action_ditå‰çš„featureé¦–å…ˆè¿›è¡Œnormalize
            self.action_dit_norm = Qwen2RMSNorm(llm_hidden_size, eps=1e-6)

            # 1. Action Dit
            # è¿™é‡Œçš„ config.action_dit_layer å¯ä»¥åœ¨ model_args ä¸­å®šä¹‰ï¼Œé»˜è®¤æ¯”å¦‚ 3 æˆ– 6
            action_layers = getattr(config, "action_dit_layer", 3)
            # åŒæ ·ä½¿ç”¨ InternVL çš„åå‡ å±‚åˆ‡ç‰‡ (å¤ç”¨ llm_connector çš„æ€è·¯)
            internvl_model2 = AutoModel.from_pretrained(
                path,
                torch_dtype=torch.bfloat16,
                low_cpu_mem_usage=True,
                trust_remote_code=True,
                attn_implementation="eager"
            )
            self.action_dit = copy.deepcopy(internvl_model2.language_model)#.to(torch.bfloat16)
            del self.action_dit.layers[:-action_layers]
            del self.action_dit.embed_tokens # ä¸éœ€è¦ Embedding å±‚ï¼Œç›´æ¥åƒ Hidden States

            # 2. Flow Matching Heads
            # å°† Action (5D Pose) æ˜ å°„åˆ° LLM Hidden Size
            self.action_dim = getattr(config, "action_dim", 5) # x, y, z, pitch, yaw
            # self.action_norm = nn.LayerNorm(llm_hidden_size, eps=1e-6)#.to(torch.bfloat16)
            self.action_in_proj = nn.Linear(self.action_dim, llm_hidden_size)#.to(torch.bfloat16)

            # 3. æ—¶é—´æ­¥ MLP
            self.time_mlp_in = nn.Linear(llm_hidden_size, llm_hidden_size)#.to(torch.bfloat16)
            self.time_mlp_out = nn.Linear(llm_hidden_size, llm_hidden_size)#.to(torch.bfloat16)

            # 4. è¾“å‡ºæŠ•å½± (Hidden -> Action Velocity)
            self.action_out_proj = nn.Linear(llm_hidden_size, self.action_dim)#.to(torch.bfloat16)

            # if self.config.is_loc_learnable_query:
            #     self.loc_learnable_query.apply(init_weights)
            # if self.config.is_action_dit_projector:
            #     self.action_dit_projector.apply(init_weights)
            if getattr(self.config, "is_aciton_dit_vae_small_init", False):
                self.action_in_proj.apply(small_init_weights)
            else:
                self.action_in_proj.apply(init_weights)
            self.time_mlp_in.apply(init_weights)
            self.time_mlp_out.apply(init_weights)
            if getattr(self.config, "is_aciton_dit_vae_small_init", False):
                self.action_out_proj.apply(small_init_weights)
            else:
                self.action_out_proj.apply(init_weights)


    def initialize_vision_modules(self, model_args, fsdp=None):
        # ... (Existing initialization logic for Vision, VAE, DiT) ...
        unilip_path = model_args.unilip_path
        self.unilip_path = unilip_path
        self.unilip_factor = model_args.unilip_factor
        self.fix_dit = model_args.fix_dit
        self.fix_connect = model_args.fix_connect
        logging.info(f"fix connect {self.fix_connect}")
        logging.info(f"fix dit {self.fix_dit}")
        if getattr(self, 'vae_decoder', None) is None:
            # replace hf structure with original internvl structure
            path = model_args.mllm_path
            internvl_model = AutoModel.from_pretrained(
                path,
                torch_dtype=torch.float32,
                low_cpu_mem_usage=True,
                trust_remote_code=True)
            self.vision_tower = internvl_model.vision_model
            self.multi_modal_projector = internvl_model.mlp1

            for layer in self.vision_tower.encoder.layers:
                try:
                    layer.drop_path1.drop_prob = 0.0
                    layer.drop_path2.drop_prob = 0.0
                except:
                    continue
            logging.info(f"should no drop out {self.vision_tower}")
            self.vision_tower.eval()

            # load unilip pretrain weight
            logging.info(f"load from unilip path {unilip_path}, factor {self.unilip_factor}")
            unilip_ckpt = torch.load(unilip_path)
            encoder_state = {}
            for key, value in unilip_ckpt.items():
                if 'encoder.' in key:
                    newkey = key[len('encoder.'):]
                    encoder_state[newkey] = value

            msg = self.vision_tower.load_state_dict(encoder_state)
            for p in self.vision_tower.parameters():
                p.requires_grad = False
            logging.info(f"load unilip vision encoder {msg}")

            mlp1_state = {}
            for key, value in unilip_ckpt.items():
                if 'mlp1.' in key:
                    newkey = key[len('mlp1.'):]
                    mlp1_state[newkey] = value
            msg = self.multi_modal_projector.load_state_dict(mlp1_state)
            for p in self.multi_modal_projector.parameters():
                p.requires_grad = False
            logging.info(f"load unilip mlp1 {msg}")

            # load vae decoder
            vae_config = {
                'model':{
                    'dc_ae_path': model_args.vae_path
                }
            }
            vae_config = OmegaConf.create(vae_config)
            llm_hidden_size = self.multi_modal_projector[-1].weight.shape[-1]
            self.vae_decoder = DCAE_Decoder(vae_config, llm_hidden_size)
            for name in list(unilip_ckpt.keys()):
                if 'regressor' in name:
                    del unilip_ckpt[name]
                else:
                    if 'decoder' in name or 'down' in name:
                        continue
                    else:
                        del unilip_ckpt[name]
            msg = self.vae_decoder.load_state_dict(unilip_ckpt)
            for p in self.vae_decoder.parameters():
                p.requires_grad = False
            logging.info(f"load unilip decoder {msg}")
        else:
            logging.info("unilip load from checkpoint!!!")
            self.vision_tower.eval()
            for p in self.vision_tower.parameters():
                p.requires_grad = False
            for p in self.multi_modal_projector.parameters():
                p.requires_grad = False
            for p in self.vae_decoder.parameters():
                p.requires_grad = False

        if getattr(self, 'dit', None) is None:
            logging.info("random initiation the DiT !!!")
            self.dit, self.vae, self.noise_scheduler = build_sana(model_args.dit_path)
        else:
            logging.info("DiT load from checkpoint!!!")
            for p in self.dit.parameters():
                p.requires_grad = True
        if self.fix_dit:
            for p in self.dit.parameters():
                p.requires_grad = False

        if getattr(self, 'llm_connector', None) is None:
            logging.info("initialize the llm connector !!!")
            path = model_args.mllm_hf_path
            internvl_model = AutoModel.from_pretrained(
                path,
                torch_dtype=self.vision_tower.dtype,
                low_cpu_mem_usage=True,
                trust_remote_code=True,
                attn_implementation="eager") # for bidr attention
            self.llm_connector = deepcopy(internvl_model.language_model)
            del self.llm_connector.layers[:-model_args.connect_layer]
            del self.llm_connector.embed_tokens
            self.projector = nn.Linear(llm_hidden_size, self.dit.config.caption_channels)
        else:
            logging.info("Connector load from checkpoint!!!")
            for p in self.llm_connector.parameters():
                p.requires_grad = True
            for p in self.projector.parameters():
                p.requires_grad = True

        self.config.n_query = model_args.n_query
        self.config.connect_layer = model_args.connect_layer
        self.config.mllm_path = model_args.mllm_path
        self.config.mllm_hf_path = model_args.mllm_hf_path
        self.config.vae_path = model_args.vae_path
        self.config.dit_path = model_args.dit_path
        self.config.unilip_factor = model_args.unilip_factor

        if getattr(self, 'latent_queries', None) is None:
            logging.info("random initiation the latent_queries !!!")
            if 'hidden_size' in self.config:
                hidden_size = self.config.hidden_size
            else:
                hidden_size = self.config.text_config.hidden_size
            self.latent_queries = nn.Parameter(torch.randn(1, self.config.n_query, hidden_size))
        else:
            logging.info("latent_queries load from checkpoint!!!")
            self.latent_queries.requires_grad = True

        connect_require_grad = not self.fix_connect
        for p in self.llm_connector.parameters():
            p.requires_grad = connect_require_grad
        for p in self.projector.parameters():
            p.requires_grad = connect_require_grad
        self.latent_queries.requires_grad = connect_require_grad

        ### æ˜¯å¦å¼€å¯LoRAï¼Œé‡æ–°é…ç½®å¯å­¦ä¹ å‚æ•°
        self.is_lora = getattr(self.config, 'is_lora', False)
        # 1. Vision Tower & Multi-modal Projector
        if not model_args.fix_vit:
            if self.is_lora:
                # LoRAæ¨¡å¼ï¼šVision Towerä¸»ä½“å†»ç»“ï¼Œç”±PEFTæ¥ç®¡ï¼›Projectoré€šå¸¸å…¨é‡è®­ç»ƒ(ä½œä¸ºmodules_to_save)
                for p in self.vision_tower.parameters(): p.requires_grad = False
                for p in self.multi_modal_projector.parameters(): p.requires_grad = True
            else:
                # å…¨é‡å¾®è°ƒæ¨¡å¼
                for p in self.vision_tower.parameters(): p.requires_grad = True
                for p in self.multi_modal_projector.parameters(): p.requires_grad = True

        # 2. LLM Backbone
        if not model_args.fix_llm:
            if self.is_lora:
                # LoRAæ¨¡å¼ï¼šLLMä¸»ä½“å†»ç»“ï¼Œç”±PEFTæ¥ç®¡
                for p in self.model.language_model.parameters(): p.requires_grad = False
            else:
                for p in self.model.language_model.parameters(): p.requires_grad = True

        # 3. LLM Connector (Gen Branch)
        if not self.fix_connect:
            if self.is_lora:
                # Connectoræ˜¯InternVLåˆ‡ç‰‡ï¼Œè§†ä¸ºBackboneï¼Œç”¨LoRAè®­ç»ƒ
                for p in self.llm_connector.parameters(): p.requires_grad = False
                # Projector æ˜¯Linearæ˜ å°„å±‚ï¼Œå»ºè®®å…¨é‡è®­ç»ƒ
                for p in self.projector.parameters(): p.requires_grad = True
                self.latent_queries.requires_grad = True
            else:
                for p in self.llm_connector.parameters(): p.requires_grad = True
                for p in self.projector.parameters(): p.requires_grad = True
                self.latent_queries.requires_grad = True

        # 4. SANA DiT (Gen Branch)
        if not self.fix_dit:
            if self.is_lora:
                # DiT ä¹Ÿæ˜¯å¤§æ¨¡å‹ï¼Œç”¨ LoRA
                for p in self.dit.parameters(): p.requires_grad = False
            else:
                for p in self.dit.parameters(): p.requires_grad = True


    def initialize_localization_modules(self, model_args):
        # [Simulating previous code structure for brevity]
        self.config.action_horizon = getattr(model_args, 'action_horizon', 1) # CS2 Pose is typically single step
        self.config.action_dim = getattr(model_args, 'action_dim', 5)
        self.config.is_action_dit_dense_timestep = getattr(model_args, 'is_action_dit_dense_timestep', False)
        llm_hidden_size = self.multi_modal_projector[-1].weight.shape[-1]
        # [NEW] Initialize Action Connector & Heads
        # if getattr(self, 'action_dit', None) is None:
        if 1:
            if getattr(self.config, "is_loc_learnable_query", False):
                self.loc_learnable_query = nn.Parameter(torch.randn(1, 1, llm_hidden_size))
            if getattr(self.config, "is_action_dit_projector", False):
                self.action_dit_projector = nn.Sequential(
                    nn.Linear(llm_hidden_size, llm_hidden_size*4, bias=True),
                    nn.GELU(),
                    nn.Linear(llm_hidden_size*4, llm_hidden_size*2, bias=True),
                    nn.GELU(),
                    nn.Linear(llm_hidden_size*2, llm_hidden_size, bias=True),
                )
            self.action_dit_norm = Qwen2RMSNorm(llm_hidden_size, eps=1e-6)

            path = model_args.mllm_hf_path
            logging.info(f"Initializing Action Connector from {path} slice...")
            internvl_model = AutoModel.from_pretrained(
                path,
                torch_dtype=self.vision_tower.dtype,
                low_cpu_mem_usage=True,
                trust_remote_code=True,
                attn_implementation="eager"
            )
            self.action_dit = copy.deepcopy(internvl_model.language_model)
            del self.action_dit.layers[:-getattr(model_args, "action_dit_layer", 3)]
            del self.action_dit.embed_tokens
            logging.info("Action DiT weights initialized successfully!")

            if getattr(model_args, 'is_action_dit_dense_timestep', False):
                # æ›¿æ¢ Norm å±‚ä¸º AdaRMS
                logging.info("Replacing action_dit norms with AdaRMS...")
                # 1. æ›¿æ¢æ¯ä¸€å±‚çš„ Norm
                for layer in self.action_dit.layers:
                    # æ›¿æ¢ input_layernorm
                    old_norm = layer.input_layernorm
                    new_norm = Qwen2RMSNormAdaRMS(llm_hidden_size, cond_dim=llm_hidden_size, eps=old_norm.variance_epsilon)
                    new_norm.weight.data = old_norm.weight.data # ç»§æ‰¿é¢„è®­ç»ƒæƒé‡
                    layer.input_layernorm = new_norm

                    # æ›¿æ¢ post_attention_layernorm
                    old_post_norm = layer.post_attention_layernorm
                    new_post_norm = Qwen2RMSNormAdaRMS(llm_hidden_size, cond_dim=llm_hidden_size, eps=old_post_norm.variance_epsilon)
                    new_post_norm.weight.data = old_post_norm.weight.data
                    layer.post_attention_layernorm = new_post_norm

                # 2. æ›¿æ¢ Final Norm
                old_final_norm = self.action_dit.norm
                new_final_norm = Qwen2RMSNormAdaRMS(llm_hidden_size, cond_dim=llm_hidden_size, eps=old_final_norm.variance_epsilon)
                new_final_norm.weight.data = old_final_norm.weight.data
                self.action_dit.norm = new_final_norm

            # Initialize Heads
            llm_hidden_size = self.config.text_config.hidden_size
            # self.action_norm = nn.LayerNorm(llm_hidden_size, eps=1e-6)#.to(torch.bfloat16)
            self.action_in_proj = nn.Linear(self.config.action_dim, llm_hidden_size)#.to(torch.bfloat16)
            self.time_mlp_in = nn.Linear(llm_hidden_size, llm_hidden_size)#.to(torch.bfloat16)
            self.time_mlp_out = nn.Linear(llm_hidden_size, llm_hidden_size)#.to(torch.bfloat16)
            self.action_out_proj = nn.Linear(llm_hidden_size, self.config.action_dim)#.to(torch.bfloat16)

        ### æ˜¯å¦å¼€å¯LoRAï¼Œé‡æ–°é…ç½®å¯å­¦ä¹ å‚æ•°
        # 1. Action DiT (Pi0.5 GemmaExpertModel)
        self.is_lora = getattr(self.config, 'is_lora', False)
        if self.is_lora:
            # Backbone å†»ç»“ï¼Œç­‰å¾… LoRA æ³¨å…¥
            for p in self.action_dit.parameters(): p.requires_grad = False
        else:
            for p in self.action_dit.parameters(): p.requires_grad = True

        # 2. Heads & Projectors
        # Enable Gradients for Action Path
        if getattr(self.config, "is_loc_learnable_query", False):
            for p in self.loc_learnable_query.parameters(): p.requires_grad = True
        if getattr(self.config, "is_action_dit_projector", False):
            for p in self.action_dit_projector.parameters(): p.requires_grad = True
        for p in self.action_dit_norm.parameters(): p.requires_grad = True
        for p in self.action_in_proj.parameters(): p.requires_grad = True
        for p in self.time_mlp_in.parameters(): p.requires_grad = True
        for p in self.time_mlp_out.parameters(): p.requires_grad = True
        for p in self.action_out_proj.parameters(): p.requires_grad = True

        # ç›´æ¥ç§»æ¤pi05çš„action_ditæ¨¡å‹å’Œæƒé‡ä½œä¸ºå®šä½head,æ— éœ€åˆå§‹åŒ–æƒé‡
        if getattr(self.config, "use_pi05_action_dit", False):
            logging.info(f"Use Pi0.5 Action DiT without Init, LoRA Enabled: {self.is_lora}")
        else:
            # Init Weights
            if getattr(self.config, "is_loc_learnable_query", False):
                self.loc_learnable_query.apply(init_weights)
            if getattr(self.config, "is_action_dit_projector", False):
                self.action_dit_projector.apply(init_weights)
            # self.action_dit_norm.apply(init_weights) # NOrmå±‚ä¸éœ€è¦æ‰‹åŠ¨åˆå§‹åŒ–ï¼Œä¿æŒåŸåˆå§‹weightså³å¯
            if getattr(self.config, "is_aciton_dit_vae_small_init", False):
                self.action_in_proj.apply(small_init_weights)
            else:
                self.action_in_proj.apply(init_weights)
            self.time_mlp_in.apply(init_weights)
            self.time_mlp_out.apply(init_weights)
            if getattr(self.config, "is_aciton_dit_vae_small_init", False):
                self.action_out_proj.apply(small_init_weights)
            else:
                self.action_out_proj.apply(init_weights)
            logging.info("Custom Action DiT weights initialized, LoRA Enabled: {self.is_lora}")

        # if getattr(model_args, "gradient_checkpointing", False):
        #     self.action_dit.gradient_checkpointing_enable()


def split_image_tokens(input_ids, image_token_idx):
    # 1. åˆ›å»ºåŸºç¡€æ©ç ï¼šæ ‡è®°æ‰€æœ‰æ˜¯å›¾ç‰‡ Token çš„ä½ç½®
    # Shape: [Batch, Seq_Len] (å¸ƒå°”å€¼)
    all_img_mask = (input_ids == image_token_idx)

    # 2. è®¡ç®—ç´¯åŠ å’Œï¼šç»™æ¯ä¸ªå›¾ç‰‡ Token ç¼–å· (1, 2, 3...)
    # éå›¾ç‰‡ä½ç½®è™½ç„¶ä¹Ÿæœ‰æ•°å€¼ï¼Œä½†ä¼šè¢«åŸºç¡€æ©ç è¿‡æ»¤æ‰ï¼Œæ‰€ä»¥ä¸ç”¨æ‹…å¿ƒ
    # Shape: [Batch, Seq_Len]
    img_cumsum = all_img_mask.cumsum(dim=1)

    # 3. è®¡ç®—æ¯ä¸€è¡Œå›¾ç‰‡ Token çš„æ€»æ•°
    # Shape: [Batch, 1] (ä¿æŒç»´åº¦ä»¥ä¾¿å¹¿æ’­)
    total_imgs = all_img_mask.sum(dim=1, keepdim=True)

    # 4. è®¡ç®—åˆ†å‰²ç‚¹ï¼šæ€»æ•°çš„ä¸€åŠ
    # Shape: [Batch, 1]
    half_point = total_imgs // 2

    # 5. ç”Ÿæˆå‰ä¸€åŠçš„æ©ç  (und_image_idx)
    # æ¡ä»¶ï¼šæ˜¯å›¾ç‰‡Token ä¸” å½“å‰ç´¯è®¡åºå· <= ä¸€åŠ
    und_image_idx = all_img_mask & (img_cumsum <= half_point)

    # 6. ç”Ÿæˆåä¸€åŠçš„æ©ç  (aux_image_idx)
    # æ¡ä»¶ï¼šæ˜¯å›¾ç‰‡Token ä¸” å½“å‰ç´¯è®¡åºå· > ä¸€åŠ
    aux_image_idx = all_img_mask & (img_cumsum > half_point)

    return und_image_idx, aux_image_idx

class Unified_UniLIP_InternVL_MetaForCausalLM(ABC):
    @abstractmethod
    def get_model(self):
        pass

    def get_vision_tower(self):
        return self.get_model().get_vision_tower()

    def get_n_query(self):
        return self.get_model().config.n_query

    # [NEW] Flow Matching Logic Helper
    def embed_action_suffix(self, noisy_actions, timestep, llm_hidden_size, device, dtype):
        """
        Embed noisy_actions and timestep to prepare for Action Connector processing.
        Inspired by Pi0.5 embed_suffix but adapted for UniLIP architecture.
        """
        # 1. Embed Timestep (Sinusoidal -> MLP)
        # self.action_in_proj.out_features is llm_hidden_size
        time_emb = create_sinusoidal_pos_embedding(
            timestep, llm_hidden_size, min_period=4e-3, max_period=4.0, device=device
        )
        time_emb = time_emb.to(dtype=dtype)

        # MLP Processing for Time (AdaRMS style condition)
        time_emb = self.get_model().time_mlp_in(time_emb)
        time_emb = F.silu(time_emb)
        time_emb = self.get_model().time_mlp_out(time_emb)
        adarms_cond = F.silu(time_emb) # [BS, Hidden]

        # 2. Embed Noisy Actions
        # noisy_actions: [BS, 1, Action_Dim]
        action_emb = self.get_model().action_in_proj(noisy_actions) # [BS, 1, Hidden]

        if not getattr(self.config, 'is_action_dit_dense_timestep', False):
            # è¿™é‡Œaction_emb + adarms_condï¼Œæ˜¾å¼åœ°å°†timeä¼ å…¥action_embedï¼Œé¿å…ä¿®æ”¹action_dit(internvl)çš„ä»£ç 
            action_emb = action_emb + adarms_cond.unsqueeze(1)

        return action_emb, adarms_cond

    def get_sigmas(self, timesteps, device, n_dim=4, dtype=torch.float32):
        # ... (Existing logic) ...
        sigmas = self.get_model().noise_scheduler.sigmas.to(device=device, dtype=dtype)
        schedule_timesteps = self.get_model().noise_scheduler.timesteps.to(device=device)
        timesteps = timesteps.to(device)
        step_indices = [(schedule_timesteps == t).nonzero().item() for t in timesteps]
        sigma = sigmas[step_indices].flatten()
        while len(sigma.shape) < n_dim:
            sigma = sigma.unsqueeze(-1)
        return sigma

    def prepare_inputs_labels_for_multimodal(
        self, input_ids, position_ids, attention_mask, past_key_values, labels,
        gen_images, combined_und_images, grid_thw, i_s_pos, image_sizes,task_id
    ):
        # ... (Existing Logic: Process Images, Replace Tokens) ...
        # This part remains largely same as original code provided
        # Specifically handling und_images embedding replacement and latent_queries.

        #ç‰¹å¾å¯¹é½ï¼šæŠŠè¾“å…¥çš„å›¾ç‰‡åƒç´ å˜æˆå‘é‡ï¼Œå¹¶åŒºåˆ†æ˜¯â€œè¾“å…¥å›¾ï¼ˆç†è§£ï¼‰â€è¿˜æ˜¯â€œç›®æ ‡å›¾ï¼ˆç”Ÿæˆï¼‰â€ã€‚
        #åµŒå…¥æ›¿æ¢ï¼šåœ¨æ–‡æœ¬ Embedding åºåˆ—ä¸­â€œæŒ–å‘â€ï¼ŒæŠŠæ–‡æœ¬å ä½ç¬¦ [IMG] æ›¿æ¢æˆçœŸå®çš„å›¾ç‰‡ç‰¹å¾ï¼ˆå¯¹äºç†è§£ä»»åŠ¡ï¼‰æˆ–å¯å­¦ä¹ çš„ Query å‘é‡ï¼ˆå¯¹äºç”Ÿæˆä»»åŠ¡ï¼‰ã€‚
        #è®­ç»ƒç›®æ ‡è®¾å®šï¼šå±è”½ LLM å¯¹å›¾ç‰‡ä½ç½®çš„åˆ†ç±» Lossï¼Œå‡†å¤‡å¥½å›å½’ç”¨çš„ Target Embeddingsã€‚

        # forwardä¸­ä½¿ç”¨ combined_und_images = torch.cat([und_image, aux_image], dim=0) å¯¹é½ä¼ å‚æ ¼å¼
        und_images = combined_und_images[:combined_und_images.size(0)//2, ...] #torch.Size([128, 3, 448, 448])
        aux_images = combined_und_images[combined_und_images.size(0)//2:, ...] #torch.Size([128, 3, 448, 448])

        # [Simulated Abbreviation for Brevity - Keeping Core Logic]
        if (gen_images is None and und_images is None) or (aux_images is None and und_images is None) or input_ids.shape[1] == 1:
             return input_ids, position_ids, attention_mask, past_key_values, None, labels, None, None, None

        vision_feature_layer = self.config.vision_feature_layer
        vision_feature_select_strategy = self.config.vision_feature_select_strategy

        # 1. Process Gen Images (Target) - used for DiT Loss
        target_image_embeds = None
        if gen_images is not None and gen_images.sum() != 0: # Check if not empty (masked)
            with torch.no_grad():
                prompt_image_embeds = self.model.get_image_features(
                    pixel_values=gen_images,
                    vision_feature_layer=vision_feature_layer,
                    vision_feature_select_strategy=vision_feature_select_strategy,
                    image_sizes=image_sizes,
                )# bs, 256, 896 # ä¸ºäº†å¯¹é½unilipé¢„è®­ç»ƒçš„latent_queries=256ï¼Œgen_imageä»ä½¿ç”¨448ã€‚å› æ­¤æœ‰è¾“å…¥ç«¯ï¼šå®šä½fps=224ã€map=224ï¼›ç”Ÿæˆmap=224ã€‚è¾“å‡ºç«¯ï¼šç”Ÿæˆfps=448
                # (B, HW, C) -> (B, C, H, W), assume H==W
                prompt_image_embeds = self.model.vae_decoder.clip_down(prompt_image_embeds)
            target_image_embeds = torch.clone(prompt_image_embeds).detach()
            target_image_embeds = target_image_embeds.mul_(self.model.unilip_factor) #torch.Size([128, 32, 16, 16]) # bs, 32, 8, 8

        # 2. Process Und Images (Input) - used for Understanding/Localization
        und_image_embeds = None
        und_img_idx = None
        if und_images is not None:
             und_image_embeds = self.model.get_image_features(
                pixel_values=und_images,
                vision_feature_layer=vision_feature_layer,
                vision_feature_select_strategy=vision_feature_select_strategy,
                image_sizes=image_sizes,
            )#torch.Size([128, 256, 896])

        # 3. Process Aux Images (Input) - used for Understanding/Localization
        aux_image_embeds = None
        aux_img_idx = None
        if aux_images is not None:
             aux_image_embeds = self.model.get_image_features(
                pixel_values=aux_images,
                vision_feature_layer=vision_feature_layer,
                vision_feature_select_strategy=vision_feature_select_strategy,
                image_sizes=image_sizes,
            )#torch.Size([128, 256, 896])

        # 4. Text Embeddings & Replacements
        und_image_idx, aux_image_idx = split_image_tokens(input_ids, IMAGE_TOKEN_IDX) # yiyongç”Ÿæˆä»»åŠ¡çš„æœ€åä¹Ÿæ‹¼æ¥äº†256ä¸ª<IMG_CONTEXT> token #und_image_idx=torch.Size([128, 707]) #aux_image_idx=torch.Size([128, 707])
        gen_image_idx = (input_ids == IMAGE_TOKEN_IDX)
        # combined_image_idx = (input_ids == UND_IMAGE_TOKEN_IDX) # ä¸ºäº†ä¸æ”¹å˜åŸæœ‰æ¨¡å‹çš„token_vocabulary, ç›´æ¥ä½¿ç”¨UND_IMAGE_TOKEN_IDXä½œä¸ºundå’Œaux image tokençš„idxã€‚
        text_embeds = self.get_model().language_model.embed_tokens(input_ids)

        # 5. Replace gen_token with Latent Queries
        is_gen_task = (task_id == 1)
        latent_queries = self.get_model().latent_queries.repeat(input_ids.shape[0], 1, 1) #torch.Size([128, 256, 896])
        valid_queries = latent_queries[is_gen_task] # å½¢çŠ¶å˜æ›´ä¸º [N_Gen, 256, H] #èµ‹å€¼ï¼šç°åœ¨ valid_queries.numel() == gen_img_idx.sum() * Hï¼Œå½¢çŠ¶å®Œç¾åŒ¹é…
        H = valid_queries.shape[-1]
        valid_queries = valid_queries.contiguous().view(-1, H) #torch.Size([32768, 896])

        # Only replace if labels indicate generation task (output_indicator)
        output_indicator = labels != -100 #torch.Size([128, 707])
        gen_img_idx = torch.logical_and(output_indicator, gen_image_idx)
        text_embeds = text_embeds.clone() #torch.Size([128, 707, 896])
        if gen_img_idx.any(): #gen_img_idx.sum()=tensor(18176, device='cuda:0') #latent_queries=torch.Size([32768, 896]) #valid_queries.shape=torch.Size([18176, 896])
            text_embeds[gen_img_idx] = valid_queries.to(text_embeds.dtype)

        # 6. Replace Und & Aux Images
        is_loc_task = (task_id == 0)
        input_indicator = labels == -100
        und_img_idx = torch.logical_and(input_indicator, und_image_idx)
        aux_img_idx = torch.logical_and(input_indicator, aux_image_idx) #åœ¨ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œåˆ©ç”¨split_image_tokenså¾—åˆ°çš„aux_image_idxå®é™…æ˜¯gen_image_idxçš„ä½ç½®ï¼Œä½†labels!=-100ï¼Œæ‰€ä»¥è¿™é‡Œå–äº¤é›†aux_img_idxä¸ºç©º
        if und_images is not None and und_img_idx.any():
             text_embeds[und_img_idx] = und_image_embeds.to(text_embeds.device).flatten(0,1)
        if aux_images is not None and aux_img_idx.any():
             text_embeds[aux_img_idx] = aux_image_embeds[is_loc_task].to(text_embeds.device).flatten(0,1) #aux_image_embeds[is_loc_task]=[N_Loc, 256, H] #is_loc_task.sum()=tensor(57, device='cuda:0') #aux_img_idx.sum()=tensor(14592, device='cuda:0') #14592=57*256

        labels[gen_image_idx] = -100

        # Attention Mask for Bi-directional (if needed by Connector)
        bidr_attention_mask = attention_mask.unsqueeze(2) & attention_mask.unsqueeze(1)#attention_mask=torch.Size([128, 707])
        bidr_attention_mask = bidr_attention_mask.unsqueeze(1)
        bidr_attention_mask = (1-bidr_attention_mask.float())*-100000 #torch.Size([128, 1, 707, 707])

        combined_img_idx = torch.cat([und_img_idx, aux_img_idx], dim=0)#torch.Size([256, 707])
        combined_image_embeds = torch.cat([und_image_embeds, aux_image_embeds], dim=0)#torch.Size([256, 256, 896])
        return None, position_ids, attention_mask, past_key_values, text_embeds, labels, target_image_embeds, combined_img_idx, combined_image_embeds, bidr_attention_mask

    def initialize_vision_tokenizer(self, model_args, tokenizer):
        if model_args.mm_use_im_patch_token: #False
            tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)
            self.resize_token_embeddings(len(tokenizer))

        if model_args.mm_use_im_start_end: #False
            num_new_tokens = tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)
            self.resize_token_embeddings(len(tokenizer))

            if num_new_tokens > 0:
                input_embeddings = self.get_input_embeddings().weight.data
                output_embeddings = self.get_output_embeddings().weight.data

                input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(
                    dim=0, keepdim=True)
                output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(
                    dim=0, keepdim=True)

                input_embeddings[-num_new_tokens:] = input_embeddings_avg
                output_embeddings[-num_new_tokens:] = output_embeddings_avg

            if model_args.tune_mm_mlp_adapter:
                for p in self.get_input_embeddings().parameters():
                    p.requires_grad = True
                for p in self.get_output_embeddings().parameters():
                    p.requires_grad = False

            if model_args.pretrain_mm_mlp_adapter:
                mm_projector_weights = torch.load(model_args.pretrain_mm_mlp_adapter, map_location='cpu')
                embed_tokens_weight = mm_projector_weights['model.embed_tokens.weight']
                assert num_new_tokens == 2
                if input_embeddings.shape == embed_tokens_weight.shape:
                    input_embeddings[-num_new_tokens:] = embed_tokens_weight[-num_new_tokens:]
                elif embed_tokens_weight.shape[0] == num_new_tokens:
                    input_embeddings[-num_new_tokens:] = embed_tokens_weight
                else:
                    raise ValueError(f"Unexpected embed_tokens_weight shape. Pretrained: {embed_tokens_weight.shape}. Current: {input_embeddings.shape}. Numer of new tokens: {num_new_tokens}.")
        elif model_args.mm_use_im_patch_token: #False
            if model_args.tune_mm_mlp_adapter:
                for p in self.get_input_embeddings().parameters():
                    p.requires_grad = False
                for p in self.get_output_embeddings().parameters():
                    p.requires_grad = False


## transformers fix/lerobot_openpi branch
class GemmaRMSNorm(nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6, cond_dim: Optional[int] = None):
        super().__init__()
        self.eps = eps
        self.dim = dim
        self.cond_dim = cond_dim

        # Dense layer for adaptive normalization (if cond_dim is provided)
        if cond_dim is not None:
            #self.dense = nn.Linear(cond_dim, dim * 3, bias=True, dtype=torch.bfloat16)
            self.dense = nn.Linear(cond_dim, dim * 3, bias=True)
            # Initialize with zeros (matches source implementation)
            nn.init.zeros_(self.dense.weight)
        else:
            self.weight = nn.Parameter(torch.zeros(dim, dtype=torch.bfloat16))
            self.dense = None

    def _norm(self, x):
        # Compute variance in float32 (like the source implementation)
        var = torch.mean(torch.square(x.float()), dim=-1, keepdim=True)
        # Compute normalization in float32
        normed_inputs = x * torch.rsqrt(var + self.eps)
        return normed_inputs

    def forward(self, x, cond=None):
        dtype = x.dtype  # original dtype, could be half-precision
        normed_inputs = self._norm(x)

        if cond is None or self.dense is None:
            # regular RMSNorm
            # scale by learned parameter in float32 (matches source implementation)
            normed_inputs = normed_inputs * (1.0 + self.weight.float())
            return normed_inputs.to(dtype), None  # return in original dtype with None gate

        # adaptive RMSNorm (if cond is provided and dense layer exists)
        if cond.shape[-1] != self.cond_dim:
            raise ValueError(f"Expected cond dimension {self.cond_dim}, got {cond.shape[-1]}")

        #self.dense.to(dtype=torch.bfloat16).to(dtype=torch.float32)
        modulation = self.dense(cond)
        # Reshape modulation to broadcast properly: [batch, 1, features] for [batch, seq, features]
        if len(x.shape) == 3:  # [batch, seq, features]
            modulation = modulation.unsqueeze(1)

        scale, shift, gate = torch.chunk(modulation, 3, dim=-1)

        # Apply adaptive normalization: use model weight dtype to ensure compatibility
        # model_dtype = self.dense.weight.dtype  # Use the model's dtype (bfloat16)
        # scale = scale.to(model_dtype)
        # shift = shift.to(model_dtype)
        # gate = gate.to(model_dtype)
        # normed_inputs = normed_inputs.to(model_dtype)  # Convert normed_inputs to model dtype

        normed_inputs = normed_inputs * (1 + scale.to(torch.float32)) + shift.to(torch.float32)

        return normed_inputs.to(dtype), gate.to(dtype)

    def extra_repr(self):
        repr_str = f"{tuple(self.weight.shape)}, eps={self.eps}"
        if self.dense is not None:
            repr_str += f", adaptive=True, cond_dim={self.cond_dim}"
        return repr_str

# å°†æ­¤ç±»æ·»åŠ åˆ° unified_unilip.py
class Qwen2RMSNormAdaRMS(nn.Module):
    def __init__(self, hidden_size, cond_dim, eps=1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

        # [ä¿®æ”¹] è¾“å‡ºç»´åº¦å˜ä¸º 3å€ (Scale, Shift, Gate)
        self.linear = nn.Linear(cond_dim, hidden_size * 3)

        # é›¶åˆå§‹åŒ– (ä¿æŒ AdaLN-Zero çš„ç‰¹æ€§)
        nn.init.zeros_(self.linear.weight)
        nn.init.zeros_(self.linear.bias)

    def forward(self, hidden_states, cond):
        # 1. åŸºç¡€ RMSNorm è®¡ç®— (ä¸å¸¦ä»¿å°„å˜æ¢)
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)#torch.Size([2, 708, 896])
        variance = hidden_states.pow(2).mean(-1, keepdim=True)#torch.Size([2, 708, 1])
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)

        # ä½¿ç”¨é¢„è®­ç»ƒçš„ weight è¿›è¡Œç¼©æ”¾ (ä¿ç•™åŸæ¨¡å‹çŸ¥è¯†)
        normed = self.weight * hidden_states.to(input_dtype)

        # 2. AdaRMS è°ƒåˆ¶
        # æŠ•å½±: [BS, Cond_Dim] -> [BS, 3 * Hidden]
        modulation = self.linear(cond)#torch.Size([2, 2688])
        modulation = modulation.unsqueeze(1) # [BS, 1, 3 * Hidden]

        # åˆ‡åˆ†: Scale, Shift, Gate
        scale, shift, gate = torch.chunk(modulation, 3, dim=-1)#3 * torch.Size([2, 1, 896])

        # åº”ç”¨: Norm(x) * (1 + scale) + shift
        # å³ä½¿æ˜¯ RMSNormï¼Œåœ¨ Diffusion ä¸­ä¹Ÿä¹ æƒ¯åŠ ä¸Š Shift ä»¥å¢å¼ºè¡¨è¾¾èƒ½åŠ›
        normed = normed.to(input_dtype) * (1 + scale) + shift

        # [å…³é”®] è¿”å› å½’ä¸€åŒ–åçš„ç‰¹å¾ å’Œ é—¨æ§å€¼
        return normed, gate

# class Qwen2RMSNormAdaRMS(nn.Module):
#     def __init__(self, hidden_size, cond_dim, eps=1e-6):
#         super().__init__()
#         self.weight = nn.Parameter(torch.ones(hidden_size))
#         self.variance_epsilon = eps

#         # [AdaRMS æ ¸å¿ƒ] æ¡ä»¶æŠ•å½±å±‚
#         # å°† cond (æ—¶é—´æ­¥ç‰¹å¾) æ˜ å°„ä¸ºç¼©æ”¾ç³»æ•°
#         self.linear = nn.Linear(cond_dim, hidden_size)

#         # [å…³é”®] é›¶åˆå§‹åŒ–
#         # ç¡®ä¿åˆå§‹çŠ¶æ€ä¸‹ scale=0ï¼Œè¾“å‡ºç­‰äºåŸå§‹ RMSNormï¼Œä¸ç ´åé¢„è®­ç»ƒæƒé‡
#         nn.init.zeros_(self.linear.weight)
#         nn.init.zeros_(self.linear.bias)

#     def forward(self, hidden_states, cond):
#         # 1. æ ‡å‡† RMSNorm è®¡ç®—
#         input_dtype = hidden_states.dtype
#         hidden_states = hidden_states.to(torch.float32)
#         variance = hidden_states.pow(2).mean(-1, keepdim=True)
#         hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
#         normed = self.weight * hidden_states.to(input_dtype)

#         # 2. AdaRMS è°ƒåˆ¶
#         # cond: [BS, Cond_Dim] -> scale: [BS, Hidden]
#         scale = self.linear(cond)

#         # å¹¿æ’­ç»´åº¦: [BS, Hidden] -> [BS, 1, Hidden] ä»¥åŒ¹é…åºåˆ—é•¿åº¦
#         scale = scale.unsqueeze(1)

#         # è°ƒåˆ¶: Norm(x) * (1 + scale)
#         return normed * (1 + scale)


# ==========================================
# 2. Main Model Class (InternVL Structure)
# ==========================================
class Unified_UniLIP_InternVLConfig(InternVLConfig):
    model_type = "unified_unilip"

class Unified_UniLIP_InternVLModel(Unified_UniLIP_InternVL_MetaModel, InternVLModel):
    config_class = Unified_UniLIP_InternVLConfig
    def __init__(self, config: InternVLConfig):
        super(Unified_UniLIP_InternVLModel, self).__init__(config)

class Unified_UniLIP_InternVLForCausalLM(InternVLForConditionalGeneration, Unified_UniLIP_InternVL_MetaForCausalLM):
    config_class = Unified_UniLIP_InternVLConfig

    def __init__(self, config):
        InternVLForConditionalGeneration.__init__(self, config)
        config.model_type = "unified_unilip"
        self.model = Unified_UniLIP_InternVLModel(config)
        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)
        self.post_init()

    # é€šç”¨ LoRA æ³¨å…¥å‡½æ•°
    def _apply_lora_to_module(self, lora_r, lora_alpha, lora_dropout, target_modules, modules_to_save=None, module_name="submodule"):
        """
        Helper to inject LoRA adapters into specific sub-modules.
        """
        # 1. æ£€æŸ¥æ¨¡å—æ˜¯å¦å­˜åœ¨
        if not hasattr(self.model, module_name):
            logging.warning(f"âš ï¸ Module {module_name} not found in model, skipping LoRA injection.")
            return
        # 2. è·å–å­æ¨¡å—å¯¹è±¡
        module = getattr(self.model, module_name)
        logging.info(f"ğŸš€ Injecting LoRA into sub-module: {module_name}...")


        # =================================================================
        # [å…³é”®ä¿®å¤] å¼ºåˆ¶æ‰“è¡¥ä¸ (Force Monkey Patch)
        # =================================================================
        # é’ˆå¯¹ action_dit å’Œ llm_connector è¿™ç§llm slices åˆ é™¤äº† embedding çš„æ¨¡å—ï¼Œ
        # æ— è®ºå®ƒä»¬æ˜¯å¦å¼€å¯ Gradient Checkpointingï¼Œéƒ½å¼ºåˆ¶ç»™ä¸€ä¸ªå‡çš„ get_input_embeddingsã€‚
        # è¿™æ ·å¯ä»¥ä¸€åŠ³æ°¸é€¸åœ°è§£å†³ PEFT çš„è‡ªåŠ¨æ£€æŸ¥æŠ¥é”™ã€‚
        if module_name in ["llm_connector", "action_dit"]:
            import types
            def _get_input_embeddings_shim(self_obj):
                if not hasattr(self_obj, "_dummy_embedding"):
                    self_obj._dummy_embedding = torch.nn.Identity().to(self_obj.device)
                    self_obj._dummy_embedding.weight = torch.tensor([0.0], requires_grad=True, device=self_obj.device)
                return self_obj._dummy_embedding

            # å¼ºåˆ¶æ›¿æ¢å®ä¾‹æ–¹æ³• (ä¸è¦åšä»»ä½•æ£€æŸ¥ï¼Œç›´æ¥è¦†ç›–ï¼)
            logging.info(f"ğŸ”§ Patching get_input_embeddings for {module_name} to bypass PEFT check.")
            module.get_input_embeddings = types.MethodType(_get_input_embeddings_shim, module)


        # 3. é…ç½® LoRA
        lora_config = LoraConfig(
            r=lora_r,
            lora_alpha=lora_alpha,
            target_modules=target_modules,
            lora_dropout=lora_dropout,
            bias="none",
            task_type=None, # å¯¹äºå­æ¨¡å—é€šå¸¸ä¸éœ€è¦æŒ‡å®š TaskTypeï¼Œä½œä¸ºé€šç”¨ Module å¤„ç†
            modules_to_save=modules_to_save # æŠ•å½±å±‚æˆ‘ä»¬æ‰‹åŠ¨è®¾ç½® requires_grad
        )
        logging.info(f"Applying LoRA to {module_name} with target_modules: {target_modules}, modules_to_save: {modules_to_save}")
        # 4. åŒ…è£…å¹¶åŸåœ°æ›¿æ¢
        peft_module = get_peft_model(module, lora_config)
        setattr(self.model, module_name, peft_module)
        # 5. æ‰“å°å¯è®­ç»ƒå‚æ•°é‡ä»¥éªŒè¯
        logging.info(f"ğŸ“Š {module_name} Adapter Config:")
        peft_module.print_trainable_parameters()

        # 6. [Hack] ç¡®ä¿ modules_to_save ä¸­çš„å‚æ•° requires_grad=True
        # æœ‰æ—¶ get_peft_model å¯¹è‡ªå®šä¹‰åµŒå¥—æ¨¡å—çš„ modules_to_save å¤„ç†ä¸å®Œç¾
        if modules_to_save is not None:
            for name, param in self.model.named_parameters():
                if any(m in name for m in modules_to_save):
                    param.requires_grad = True

        return peft_module

    def inject_lora_to_sub_module(self, model_args, training_args):
        if not getattr(training_args, 'is_lora', False):
            return

        logging.info("ğŸŒŸ Starting Modular LoRA Injection for Unified UniLIP...")

        # =========================================================
        # 1. Vision Tower (InternVisionModel)
        # =========================================================
        # ç»“æ„: attn.qkv, attn.proj, mlp.fc1, mlp.fc2
        if not model_args.fix_vit:
            self._apply_lora_to_module(
                lora_r=training_args.lora_r // 2,
                lora_alpha=training_args.lora_alpha,
                lora_dropout=training_args.lora_dropout,
                target_modules=["qkv", "proj", "fc1", "fc2",], #["qkv", "proj", "fc1", "fc2"]
                # modules_to_save=["multi_modal_projector"],
                module_name="vision_tower"
            )
        # =========================================================
        # 2. LLM Backbone (Qwen2Model)
        # =========================================================
        # ç»“æ„: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj
        if not model_args.fix_llm:
            self._apply_lora_to_module(
                lora_r=training_args.lora_r,
                lora_alpha=training_args.lora_alpha,
                lora_dropout=training_args.lora_dropout,
                target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
                # modules_to_save=["lm_head", "embed_tokens"],
                module_name="language_model"
            )
        # =========================================================
        # 3. LLM Connector (Qwen2Model Slice)
        # =========================================================
        # ç»“æ„åŒ LLM
        if not self.get_model().fix_connect:
            self._apply_lora_to_module(
                lora_r=training_args.lora_r // 2,
                lora_alpha=training_args.lora_alpha,
                lora_dropout=training_args.lora_dropout,
                target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
                # modules_to_save=None,
                module_name="llm_connector"
            )
        # =========================================================
        # 4. Gen DiT (SanaTransformer2DModel)
        # =========================================================
        # ç»“æ„åˆ†æ:
        # attn1/attn2: to_q, to_k, to_v, to_out.0
        # PatchEmbed/Timestep: linear_1, linear_2
        # æ³¨æ„ï¼šSana çš„ GLUMBConv ä½¿ç”¨çš„æ˜¯ Conv2dï¼ŒLoRA é»˜è®¤ä¸è½¬ Conv2d é™¤éæŒ‡å®šã€‚
        # è¿™é‡Œæˆ‘ä»¬ä¸»è¦å¯¹ Attention å’Œ Timestep MLP åš LoRAã€‚
        # to_q/k/v åŒ¹é… Attention, linear_1/2 åŒ¹é… TimestepEmbedder & CaptionProjection
        if not self.get_model().fix_dit:
            self._apply_lora_to_module(
                lora_r=training_args.lora_r,
                lora_alpha=training_args.lora_alpha,
                lora_dropout=training_args.lora_dropout,
                target_modules=["to_q", "to_k", "to_v", "to_out.0", "linear_1", "linear_2"],
                # modules_to_save=None,
                module_name="dit"
            )
        # =========================================================
        # 5. Loc Action DiT (Qwen2Model Slice)
        # =========================================================
        # ç»“æ„åŒ LLM
        # æ³¨æ„ï¼šAction DiT å§‹ç»ˆé€šè¿‡ LoRA è®­ç»ƒ (é™¤éå®Œå…¨å†»ç»“)
        self._apply_lora_to_module(
            lora_r=training_args.lora_r,
            lora_alpha=training_args.lora_alpha,
            lora_dropout=training_args.lora_dropout,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
            # modules_to_save=None,
            module_name="action_dit"
        )

        # =========================================================
        # 6. [å…³é”®] ç»Ÿä¸€å¼€å¯é LoRA æ¨¡å— (Heads/Projectors) çš„æ¢¯åº¦
        # =========================================================
        # é»˜è®¤ modules_to_save=None (å› ä¸ºå®ƒæ˜¯é’ˆå¯¹å­æ¨¡å—å†…çš„)ï¼Œæ‰€ä»¥éœ€è¦æ‰‹åŠ¨å¼€å¯å¤–éƒ¨è¿æ¥å±‚çš„æ¢¯åº¦ã€‚

        logging.info("ğŸ”“ Unfreezing Projectors and Heads...")

        # å®šä¹‰éœ€è¦å…¨é‡è®­ç»ƒçš„æ¨¡å—å…³é”®è¯
        modules_to_train_fully = [
            # "multi_modal_projector",  # Vision -> LLM
            # "lm_head",                # LLM Output
            # "embed_tokens",           # LLM Input Embedding (å¦‚æœ resize äº†)
            # "projector",              # Connector -> DiT
            "latent_queries",         # Gen Query
            "action_dit_projector",   # LLM -> Action DiT
            "action_dit_norm",        # Action DiT Norm (AdaRMS)
            "action_in_proj",         # Action Input
            "action_out_proj",        # Action Output
            "time_mlp_in",            # Timestep MLP
            "time_mlp_out",
            "loc_learnable_query"
        ]

        count_unfrozen = 0
        for name, param in self.model.named_parameters():
            # æ£€æŸ¥å‚æ•°åæ˜¯å¦åŒ…å«ä¸Šè¿°å…³é”®è¯
            if any(m in name for m in modules_to_train_fully):
                if param.requires_grad == False:
                    logging.info(name)
                    param.requires_grad = True
                    count_unfrozen += 1

        logging.info(f"âœ… LoRA Injection Complete. Manually unfroze {count_unfrozen} parameters for Heads/Projectors.")

    def get_model(self):
        return self.model

    def sample_noise(self, shape, device):
        return torch.normal(
            mean=0.0,
            std=1.0,
            size=shape,
            dtype=torch.bfloat16,
            device=device,
        )

    def sample_time(self, bsize, device):
        time_beta = sample_beta(1.5, 1.0, bsize, device)
        time = time_beta * 0.999 + 0.001
        return time.to(dtype=torch.bfloat16, device=device)

    # ==========================================
    # 3. [NEW] Forward Implementation
    # ==========================================
    def forward(
        self,
        input_ids: torch.LongTensor = None, #torch.Size([128, 707])
        attention_mask: Optional[torch.Tensor] = None, #torch.Size([128, 707])
        position_ids: Optional[torch.LongTensor] = None, #None
        past_key_values: Optional[List[torch.FloatTensor]] = None, #None
        inputs_embeds: Optional[torch.FloatTensor] = None, #None
        labels: Optional[torch.LongTensor] = None, #torch.Size([128, 707])
        ids: Optional[list] = None, #len=128 * file_frame
        i_s_pos: Optional[list] = None, #None
        use_cache: Optional[bool] = None, #None
        output_attentions: Optional[bool] = None, #output_attentions
        output_hidden_states: Optional[bool] = None, #None

        # Custom Inputs from Dataset
        gen_image: Optional[torch.FloatTensor] = None, #torch.Size([128, 3, 448, 448])
        und_image: Optional[torch.FloatTensor] = None, #torch.Size([128, 3, 448, 448])
        aux_image: Optional[torch.FloatTensor] = None, # [NEW] Support Aux Image (Wrist/Map) #torch.Size([128, 3, 448, 448])
        actions: Optional[torch.FloatTensor] = None,   # [NEW] GT 5D Pose [BS, 1, 5]  # torch.Size([128, 5])
        loss_mask: Optional[torch.FloatTensor] = None, # [NEW] [BS, 2] -> [Loc_Weight, Gen_Weight]  # torch.Size([128, 2])
        aux_loc_input_ids: torch.LongTensor = None,
        aux_loc_labels: Optional[torch.LongTensor] = None,
        aux_loc_attention_mask: Optional[torch.Tensor] = None,

        # Others
        grid_thw: Optional[torch.FloatTensor] = None, # None
        image_sizes: Optional[List[List[int]]] = None, # None
        return_dict: Optional[bool] = None, # None
        task_id: Optional[torch.FloatTensor] = None, # [NEW] [BS] -> 0: Loc, 1: Gen #torch.Size([128])
        **kwargs #dict_keys(['map_id', 'raw_prompt', 'map_name', 'pose_dict', 'num_items_in_batch']) #'num_items_in_batch': tensor(18631, device='cuda:0')
    ) -> Union[Tuple, CausalLMOutputWithPast]:
        # bs=8æ˜¾å­˜å ç”¨=5896MiB
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        loc_indices = (task_id == 0).nonzero(as_tuple=True)[0]
        gen_indices = (task_id == 1).nonzero(as_tuple=True)[0]

        # --- A. Input Preparation ---
        # Note: We merge und_image and aux_image logic.
        # In Dataset: Localization task has `und_image`(FPS) and `aux_image`(Map).
        # In Dataset: Generation task has `und_image`(Map) and `aux_image`(Empty).
        # We need to concat them for processing if both exist.

        combined_und_images = und_image
        if aux_image is not None:# and aux_image.sum() != 0:
            if combined_und_images is not None:
                # Assuming simple concat in batch dimension for processing, then splitting?
                # Or Dataset handles embedding replacement.
                # Given prepare_inputs logic, we rely on input_ids tokens.
                # If there are two <image> tokens in Loc Prompt, prepare_inputs handles replacement sequentially.
                # Just need to ensure `und_images` passed to prepare_inputs contains all pixel values.
                combined_und_images = torch.cat([und_image, aux_image], dim=0) # Careful with indexing mapping!
                # Actually, simpler: The dataset currently provides und_image and aux_image separately.
                # But input_ids has multiple <image> placeholders.
                # For simplicity in this adaptation, let's assume `und_image` arg passed to `prepare_inputs`
                # should contain ALL images referenced by input_ids (except gen target).
                # Your Dataset logic: und_image is [1, C, H, W], aux_image is [1, C, H, W].
                # If task=Loc, we have 2 images. We should stack them.

                # Check Batch Size:
                # If input_ids is [BS, Seq], und_image is [BS, 1, C, H, W] (from dataset collator likely [BS, 1, C, H, W])
                pass

        # For strict compatibility, let's pass `und_image` (which might be a stack of images if collated correctly).
        # Assuming the standard collator stacks images into [Total_Images_In_Batch, C, H, W].

        if inputs_embeds is None:
            ( # return None, position_ids, attention_mask, past_key_values, text_embeds, labels, target_image_embeds, combined_img_idx, combined_image_embeds, bidr_attention_mask
                _,
                position_ids,
                attention_mask,
                past_key_values,
                inputs_embeds,
                labels,
                target_image_embeds, #latents
                combined_img_idx,
                combined_image_embeds,
                bidr_attention_mask
            ) = self.prepare_inputs_labels_for_multimodal(
                input_ids,
                position_ids,
                attention_mask,
                past_key_values,
                labels,
                gen_image[gen_indices],
                combined_und_images, # Pass und_image (which contains all input visuals)
                grid_thw,
                i_s_pos,
                image_sizes,
                task_id,
            )
            und_img_idx = combined_img_idx[:combined_img_idx.size(0)//2, ...] #und_img_idx,sum()=32768 #32768/256=128.0
            aux_img_idx = combined_img_idx[combined_img_idx.size(0)//2:, ...]#aux_img_idx.sum()=tensor(14592, device='cuda:0') #14592/256=57
            und_image_embeds = combined_image_embeds[:combined_image_embeds.size(0)//2, ...]#torch.Size([128, 256, 896])
            aux_image_embeds = combined_image_embeds[combined_image_embeds.size(0)//2:, ...]#torch.Size([128, 256, 896])

        # --- B. Main LLM Forward (Understanding) ---

        position_ids = torch.cumsum(attention_mask, dim=1) - 1
        position_ids[position_ids < 0] = 0
        # bs=8æ˜¾å­˜å ç”¨=6186MiB
        # inputs_embeds=torch.Size([16, 515, 896])

        outputs = self.model.language_model(
            attention_mask=attention_mask, #torch.Size([128, 707])
            position_ids=position_ids, #torch.Size([128, 707])
            inputs_embeds=inputs_embeds, #torch.Size([128, 707, 896])
            output_hidden_states=True,
            return_dict=return_dict, #True
            use_cache=False
        )

        # Last Hidden State from LLM: [BS, Seq_Len, Hidden_Size]
        # This contains contextualized features of both text and images.
        hidden_states = outputs.hidden_states[-1] #torch.Size([128, 707, 896])
        # bs=8æ˜¾å­˜å ç”¨=6654MiB
        # # åœ¨ --- B. Main LLM Forward --- ä¹‹åæ’å…¥
        # logging.info(f"DEBUG Check:")
        # logging.info(f"  Input IDs Shape: {input_ids. shape}")
        # logging.info(f"  Hidden States Shape: {hidden_states.shape}")
        # logging.info(f"  Combined Img Idx Shape: {combined_img_idx.shape}") # å…³é”®ï¼çœ‹æ˜¯ä¸æ˜¯ 2*BS
        # logging.info(f"  Valid Lens Sample (0-5): {attention_mask.sum(dim=1)[:5]}")
        # logging.info(f"  Loss Mask Sum (Loc/Gen): {loss_mask.sum(dim=0)}")

        # Re-fill und_image embeddings (Skip Connection logic from UniLIP)
        if und_image_embeds is not None and und_img_idx is not None:
            hidden_states[und_img_idx] = und_image_embeds.to(hidden_states.device).flatten(0,1)

        is_loc_task = (task_id == 0)#is_loc_task.shape=torch.Size([128])#is_loc_task.sum()=tensor(57, device='cuda:0')
        if aux_image_embeds is not None and und_img_idx is not None: #aux_img_idx.sum()/128 = 57
            hidden_states[aux_img_idx] = aux_image_embeds[is_loc_task].to(hidden_states.device).flatten(0,1)#hidden_states[aux_img_idx].shape=torch.Size([14592, 896]) #aux_image_embeds[is_loc_task].shape=torch.Size([57, 256, 896])

        # --- C. Task Branching based on Loss Mask ---
        # loss_mask: [BS, 2] -> [Loc, Gen]
        if loss_mask is None:
            # loss_mask = torch.ones(hidden_states.shape[0], 2).to(hidden_states.device) # Default all on
            total_loss = torch.nn.MSELoss()(hidden_states, torch.clone(hidden_states.detach()))
        else:
            # ==========================================
            # æŒ‰ç´¢å¼•æ‹†åˆ†ç”Ÿæˆ/å®šä½æ ·æœ¬
            # ==========================================
            # logging.info(f"loc_indices: {len(loc_indices)}, gen_indices: {len(gen_indices)}")

            # ==========================================
            # Branch 1: GENERATION (DiT Path)
            # ==========================================
            if loss_mask[gen_indices][:, 1].sum() > 0: # If any sample needs Generation
                is_gen_task = (task_id == 1)#is_gen_task.sum()=tensor(71, device='cuda:0')
                # TODO_Done å¦‚æœè¿™é‡Œä½¿ç”¨[is_gen_task]è¿‡æ»¤åå†è¿›è¡Œllm_connectorå’Œditï¼Œèƒ½èŠ‚çº¦æ˜¾å­˜ï¼Œä½†æ˜¯ç”±äº[is_gen_task]çš„æ•°ç›®ä¸ä¸€å®šæ°å¥½ç­‰äº2çš„æ¬¡æ–¹ï¼Œæ‰€ä»¥å¯èƒ½ä¼šå½±å“cudaåŠ é€Ÿè¿ç®—ã€‚é™¤éæ•°æ®é›†collactoræ‰‹åŠ¨è®¾ç½®loc:gen=64:64ã€‚
                genbrh_bidr_attention_mask = bidr_attention_mask[gen_indices]
                genbrh_position_ids = position_ids[gen_indices]
                genbrh_hidden_states = hidden_states[gen_indices]
                genbrh_target_image_embeds = target_image_embeds#[gen_indices]
                genbrh_attention_mask = attention_mask[gen_indices]
                genbrh_loss_mask = loss_mask[gen_indices]
                genbrh_aux_loc_input_ids = aux_loc_input_ids[gen_indices]
                genbrh_aux_loc_labels = aux_loc_labels[gen_indices]
                genbrh_aux_loc_attention_mask = aux_loc_attention_mask[gen_indices]
                genbrh_und_image = und_image[gen_indices]
                genbrh_gen_image = gen_image[gen_indices]
                genbrh_task_id = task_id[gen_indices]
                genbrh_actions = actions[gen_indices]

                # 1. Process features via LLM Connector
                img_hidden_states = self.model.llm_connector(
                    attention_mask=genbrh_bidr_attention_mask,#torch.Size([128, 1, 707, 707])
                    position_ids=genbrh_position_ids,#torch.Size([128, 707])
                    inputs_embeds=genbrh_hidden_states,#torch.Size([128, 707, 896])
                    output_hidden_states=True,
                    return_dict=return_dict,#True
                    use_cache=False
                ).hidden_states[-1]

                # 2. Project to DiT Caption Channel
                img_hidden_states = self.get_model().projector(img_hidden_states) #torch.Size([128, 707, 2304])
                # bs=8æ˜¾å­˜å ç”¨=6884MiB
                # 3. Calculate DiT Loss
                if genbrh_target_image_embeds is not None:#target_image_embeds=torch.Size([128, 32, 16, 16])
                    latents = genbrh_target_image_embeds # [BS_Gen, C, H, W]
                    bsz = latents.shape[0] #128
                    noise = torch.randn_like(latents, device=latents.device)#torch.Size([128, 32, 16, 16])
                    u = compute_density_for_timestep_sampling(weighting_scheme="logit_normal", batch_size=bsz, logit_mean=0.0, logit_std=1.0) #torch.Size([128])
                    indices = (u * self.get_model().noise_scheduler.config.num_train_timesteps).long() #torch.Size([128])
                    timesteps = self.get_model().noise_scheduler.timesteps[indices].to(device=latents.device)#torch.Size([128])
                    sigmas = self.get_sigmas(timesteps, latents.device, n_dim=latents.ndim, dtype=latents.dtype)#torch.Size([128, 1, 1, 1])

                    noisy_latents = (1.0 - sigmas) * latents + sigmas * noise #torch.Size([128, 32, 16, 16])

                    # Forward DiT
                    noise_pred = self.get_model().dit(
                        noisy_latents, #torch.Size([128, 32, 16, 16])
                        timestep=timesteps, #128
                        encoder_hidden_states=img_hidden_states, # [BS, Seq, C] ##torch.Size([128, 707, 2304])
                        encoder_attention_mask=genbrh_attention_mask, #torch.Size([128, 707])
                        return_dict=False
                    )[0] #torch.Size([128, 32, 16, 16])

                    target = noise - latents #torch.Size([128, 32, 16, 16])
                    # Compute raw MSE loss per sample [BS, ...]
                    gen_loss = F.mse_loss(noise_pred.float(), target.float(), reduction="none")#torch.Size([128, 32, 16, 16])
                    gen_loss = gen_loss.mean(dim=[1, 2, 3]) # [BS]

                    # Apply Mask: Only count loss for Gen samples
                    # masked_gen_loss = (gen_loss * loss_mask[:, 1]).mean()#loss_mask[:, 1].sum()=tensor(71., device='cuda:0', dtype=torch.bfloat16)
                    masked_gen_loss = (gen_loss * genbrh_loss_mask[:, 1]).mean()
                    # bs=8æ˜¾å­˜å ç”¨=6884MiB
                    # =========================================================
                    # [NEW] Auxiliary Localization Loss (Consistency Check)
                    # =========================================================
                    # ä»…å½“è®­ç»ƒç”Ÿæˆä»»åŠ¡ï¼Œä¸” actions (GT Pose) å­˜åœ¨æ—¶è®¡ç®—
                    # å¹¶ä¸”ä¸ºäº†æ˜¾å­˜å®‰å…¨ï¼Œå¯èƒ½åªå¯¹éƒ¨åˆ†æ ·æœ¬è®¡ç®—ï¼Œæˆ–è€…éœ€è¦ gradient checkpointing
                    if getattr(self.config, 'is_loc_aux_loss', False) and genbrh_actions is not None:
                        # actions [BS, 1, 5] å³ä½¿æ˜¯ Gen ä»»åŠ¡ï¼ŒDataset ä¹Ÿåº”è¯¥æŠŠ pose ä¼ è¿›æ¥
                        masked_loc_aux_loss = self.forward_for_aux_loc_loss(
                            sigmas, #torch.Size([16, 1, 1, 1])
                            noisy_latents, #torch.Size([16, 32, 16, 16])
                            noise_pred, #torch.Size([16, 32, 16, 16])
                            genbrh_aux_loc_input_ids, #torch.Size([16, 617])
                            genbrh_aux_loc_labels, #torch.Size([16, 617])
                            genbrh_aux_loc_attention_mask, #torch.Size([128, 707])
                            genbrh_und_image, #torch.Size([16, 3, 448, 448])
                            genbrh_gen_image, #torch.Size([16, 3, 448, 448])
                            grid_thw, #None
                            i_s_pos, #None
                            image_sizes, #None
                            torch.zeros_like(genbrh_task_id), #torch.Size([16])
                            return_dict, #True
                            genbrh_actions, #torch.Size([16, 1, 5])
                            genbrh_loss_mask #torch.Size([16, 2])
                        )
                    else:
                        masked_loc_aux_loss = torch.nn.MSELoss()(genbrh_hidden_states, torch.clone(genbrh_hidden_states.detach())).to(torch.float32)

            else:
                masked_gen_loss = torch.nn.MSELoss()(hidden_states, torch.clone(hidden_states.detach())).to(torch.float32)
                masked_loc_aux_loss = torch.nn.MSELoss()(hidden_states, torch.clone(hidden_states.detach())).to(torch.float32)
            # bs=8æ˜¾å­˜å ç”¨=13316MiB
            # ==========================================
            # Branch 2: LOCALIZATION (Flow Matching Path)
            # ==========================================
            if loss_mask[loc_indices][:, 0].sum() > 0: # If any sample needs Localization
                # actions: [BS, 1, 5]
                locbrh_actions = actions[loc_indices]#torch.Size([128, 5])
                locbrh_hidden_states = hidden_states[loc_indices]
                locbrh_attention_mask = attention_mask[loc_indices]
                locbrh_position_ids = position_ids[loc_indices]
                locbrh_loss_mask = loss_mask[loc_indices]
                # # TODO è¿™é‡Œä¹Ÿä¸€æ ·ï¼å¦‚æœä½¿ç”¨[is_loc_task]è¿‡æ»¤actionså’Œå…¶ä»–ä¸­é—´tensoråå†è¿›è¡Œembed_action_suffixå’Œaction_ditï¼Œèƒ½èŠ‚çº¦æ˜¾å­˜ï¼Œä½†æ˜¯ç”±äº[is_loc_task]çš„æ•°ç›®ä¸ä¸€å®šæ°å¥½ç­‰äº2çš„æ¬¡æ–¹ï¼Œæ‰€ä»¥å¯èƒ½ä¼šå½±å“cudaåŠ é€Ÿè¿ç®—ã€‚é™¤éæ•°æ®é›†collactoræ‰‹åŠ¨è®¾ç½®loc:gen=64:64ã€‚
                # 1. Flow Matching Setup
                # Sample Noise & Time
                noise = self.sample_noise(locbrh_actions.shape, locbrh_actions.device)#torch.Size([128, 5])
                time = self.sample_time(locbrh_actions.shape[0], locbrh_actions.device)#torch.Size([128])

                # Interpolate: x_t = t * noise + (1-t) * x_1 (Actions)
                time_expanded = time[:, None, None].to(locbrh_actions.dtype) # [BS, 1, 1] #torch.Size([128, 1, 1])
                x_t = time_expanded * noise + (1 - time_expanded) * locbrh_actions # å¸¦å™ªå£°çš„ä¸­é—´å‘é‡x_t #torch.Size([128, 1, 5])

                u_t = noise - locbrh_actions # éœ€è¦æ¨¡å‹é¢„æµ‹çš„ Velocity Target é€Ÿåº¦åœº #torch.Size([128, 5])

                # 2. Prepare Inputs for Action Connector
                # We treat LLM hidden_states as "Prefix" (Context)
                # Embed the suffix (Noisy Action + Time)
                suffix_emb, adarms_cond = self.embed_action_suffix(
                    x_t, #torch.Size([128, 1, 5])
                    time, #torch.Size([128])
                    llm_hidden_size=self.model.config.text_config.hidden_size,
                    device=locbrh_actions.device,
                    dtype=locbrh_hidden_states.dtype
                ) # suffix_emb: [BS, 1, Hidden] #torch.Size([128, 1, 896])

                # é˜²æ­¢ï¼›anguage_modelè¾“å‡ºçš„last_hidden_stateå‡ºç°max=266ï¼Œmin=-256ï¼Œè€Œå¯¼è‡´æ¢¯åº¦nan
                locbrh_hidden_states = self.get_model().action_dit_norm(locbrh_hidden_states)
                # action_emb = self.get_model().action_norm(action_emb)

                # scaler = self.model.config.text_config.hidden_size ** 0.5
                # suffix_emb = suffix_emb * scaler

                # 3. Concatenate & Forward Action Connector
                # Context (LLM Output) + Suffix (Action)
                # We need to construct attention mask so Suffix sees Context, but standard Causal mask is fine usually

                # Concat Embeddings
                # Scatter å¡«å…… # â€œå³ç§»å¡«ç©ºâ€ (Right Shift & Fill) æ“ä½œ # å®ç°"Left Padding" æˆ– "Packing"çš„æ•ˆæœ
                bs, seq_len, hidden_dim = locbrh_hidden_states.shape
                valid_lens = locbrh_attention_mask.sum(dim=1).long()

                # action_dit_inputs = torch.cat([hidden_states, suffix_emb], dim=1) # [BS, Seq+1, Hidden] #hidden_states=torch.Size([128, 707, 896]) #suffix_emb=torch.Size([128, 1, 896]) #action_dit_inputs #torch.Size([128, 708, 896])
                action_dit_inputs = torch.cat([locbrh_hidden_states, torch.zeros_like(suffix_emb)], dim=1)
                target_indices = valid_lens.view(-1, 1, 1).expand(-1, 1, hidden_dim)#æŠŠ suffix_emb æ”¾åˆ° valid_lens çš„ä½ç½® # æ„é€ ç´¢å¼•ï¼šæˆ‘ä»¬éœ€è¦ä¿®æ”¹çš„ä½ç½®æ˜¯ (b, valid_lens[b]) # view(-1, 1, 1) æ˜¯ä¸ºäº†å¹¿æ’­åˆ° hidden_dim
                action_dit_inputs = action_dit_inputs.scatter(1, target_indices, suffix_emb)

                # Extend Masks
                # 1 for Action token (visible)
                # action_mask = torch.ones((bsz, 1, 1, 1), device=attention_mask.device, dtype=attention_mask.dtype)
                # action_mask = torch.ones((bsz, 1), device=attention_mask.device, dtype=attention_mask.dtype)
                # action_dit_att_mask = torch.cat([attention_mask, action_mask], dim=1) #torch.Size([128, 708])
                action_dit_att_mask = torch.cat([locbrh_attention_mask, torch.zeros((bs, 1), device=locbrh_attention_mask.device, dtype=locbrh_attention_mask.dtype)], dim=1)
                mask_indices = valid_lens.view(-1, 1)
                action_dit_att_mask = action_dit_att_mask.scatter(1, mask_indices, 1)

                # Position IDs
                # action_pos_id = position_ids.max(dim=1)[0].unsqueeze(1) + 1
                # action_dit_pos_ids = torch.cat([position_ids, action_pos_id], dim=1) #position_ids=torch.Size([128, 707]) #action_pos_id=torch.Size([128, 1])
                action_dit_pos_ids = torch.cat([locbrh_position_ids, torch.zeros((bs, 1), device=locbrh_position_ids.device, dtype=locbrh_position_ids.dtype)], dim=1)
                # Action çš„ Pos ID åº”è¯¥æ˜¯ä¸Šä¸€ä¸ª token çš„ pos + 1ï¼Œæˆ–è€…ç›´æ¥å°±æ˜¯ valid_lens (å¦‚æœä»0å¼€å§‹)
                # å‡è®¾ä½ çš„ position_ids åœ¨ padding å¤„æ˜¯ 0 æˆ–å…¶ä»–ï¼Œæˆ‘ä»¬è¿™é‡Œæ˜¾å¼è®¡ç®—ä¸€ä¸‹ action çš„ pos
                action_pos_ids = valid_lens.view(-1, 1) # Action çš„ä½ç½®ç´¢å¼•å°±æ˜¯å®ƒçš„åºåˆ—ä½ç½®
                # action_dit_pos_ids = action_dit_pos_ids.scatter(1, mask_indices, action_pos_ids)
                # [æ ¸å¿ƒä¿®æ”¹] ç”Ÿæˆæ©ç å¹¶å¡«å……
                # æˆ‘ä»¬è¦æ‰¾åˆ°æ‰€æœ‰ index >= valid_lens çš„ä½ç½®
                # æ„é€ ä¸€ä¸ª range çŸ©é˜µ: [0, 1, 2, ..., Seq]
                current_seq_len = action_dit_pos_ids.shape[1]
                range_ids = torch.arange(current_seq_len, device=locbrh_position_ids.device).unsqueeze(0) # [1, Seq+1]
                # ç”Ÿæˆæ©ç ï¼šå¦‚æœå½“å‰ä½ç½® index >= valid_lensï¼Œåˆ™ä¸º True
                # [BS, 1] vs [1, Seq+1] -> Broadcast -> [BS, Seq+1]
                mask_after_valid = range_ids >= valid_lens.view(-1, 1)
                # ä½¿ç”¨ torch.where è¿›è¡Œæ‰¹é‡å¡«å……
                # é€»è¾‘ï¼šMask ä¸º True çš„åœ°æ–¹å¡«å…¥ action_pos_idï¼ŒFalse çš„åœ°æ–¹ä¿æŒåŸæ ·
                action_dit_pos_ids = torch.where(
                    mask_after_valid,
                    action_pos_ids,      # å¹¿æ’­å¡«å…… [BS, 1] -> [BS, MaskåŒºåŸŸ]
                    action_dit_pos_ids  # ä¿æŒåŸå€¼
                )

                #### TODO
                # è¿™é‡Œaction_dit_inputså’Œaction_dit_pos_idsæœ‰ä¸€ä¸ªé£é™©ç‚¹ï¼Œä¸ºäº†å°†suffix_embå’Œaction_pos_idsæ‹¼æ¥åˆ°æ­£ç¡®çš„ä½ç½®ï¼Œæˆ‘ä»¬å…ˆä½¿ç”¨zeroså¡«å……åˆ°æ­£ç¡®çš„seqé•¿åº¦ï¼Œç„¶åå†ä½¿ç”¨scatteræ‰¾åˆ°æ­£ç¡®ä½ç½®valid_lenså¡«å……ã€‚è€Œè¿™æ ·åšä¼šå¯¼è‡´åŸå…ˆpaddingä½ç½®çš„tensorè¢«zerosæ›¿ä»£ï¼Œè™½ç„¶action_dit_att_maskä¸å—å½±å“ä¸”ä¼šå¿½ç•¥paddingä½ç½®çš„tensorçš„æ¢¯åº¦è®¡ç®—ï¼Œä½†è¿˜æ˜¯æœ‰é£é™©(å‰§çƒˆæ•°å€¼æ³¢åŠ¨ã€æŸäº›bf16è®¡ç®—ã€ç‰¹å®šFlashAttentionç®—å­ç­‰)ã€‚
                # è¿˜æœ‰ä¸€ä¸ªæ›´ç³Ÿç³•çš„æƒ…å†µï¼Œå¦‚æœæ‰‹åŠ¨å°†attention_maskçš„paddingä½ç½®çš„position_idæ­£ç¡®åœ°æ›¿ä»£åˆ°action_dit_pos_idsä¸Šæ˜¯å¯ä»¥å®ç°çš„ã€‚ä½†æ˜¯å°†hidden_statesåœ¨paddingä½ç½®çš„embedæ›¿ä»£åˆ°æ­£ç¡®çš„action_dit_inputsçš„ä½ç½®æ˜¯ä¸å¯èƒ½çš„ï¼Œå› ä¸ºhidden_statesæ¯ä¸ªä½ç½®çš„embedæ˜¯ç”±æ‰€æœ‰ä½ç½®çš„embedè®¡ç®—å¾—åˆ°çš„ã€‚
                # ç»¼ä¸Šæ‰€è¿°ï¼Œæˆ‘æ‰“ç®—å…ˆä»¿ç…§genä»»åŠ¡ditï¼Œå°†hidden_statesä½œä¸ºencoder_hidden_statesè¾“å…¥action_ditçš„UniLIPæ–¹å¼ã€‚ä»¥æ­¤æ¥æ›¿ä»£hidden_statesæ‹¼æ¥suffix_embä½œä¸ºinputs_embedsè¾“å…¥action_ditçš„Pi05æ–¹å¼ã€‚
                # ä½†æ˜¯genä»»åŠ¡çš„ditæ˜¯ä¸ªsanaï¼Œæ”¯æŒtimestepå‚æ•°çš„è¾“å…¥ï¼›action_ditæ˜¯ä¸ªinternvl.language_modeléœ€è¦æ¨¡ä»¿Pi05çš„gemma_300må°†é™„å¸¦timestepä¿¡æ¯çš„adarms_condä¼ å…¥æ¨¡å‹ï¼Œä¸”ä¿®æ”¹æ¨¡å‹çš„norm layerå·²é€‚é…adarms_condã€‚
                # ç¬¬äºŒæ¬¡ç»¼ä¸Šæ‰€è¿°ï¼Œå…ˆå¿½ç•¥zeroså¡«å……çš„é£é™©ç‚¹ï¼Œè·‘é€šæ¨¡å‹ã€‚åç»­å†ä¿®æ”¹action_ditçš„ä»£ç æ¥é€‚é…adarms_cond
                #### TODO

                if getattr(self.config, "is_action_dit_projector", False):
                    action_dit_inputs = self.get_model().action_dit_projector(action_dit_inputs)
                # bs=8æ˜¾å­˜å ç”¨=13316MiB
                # Forward Action Connector (InternVL Slice)
                # Reuse bidr mask logic or standard causal. Since it's InternVL, it expects eager/causal usually.
                # For simplicity, we assume bidr mask logic handles the sequence extension as default.
                # æ³¨æ„åœ¨OpenPi0.5ä¸­ä½¿ç”¨çš„gemma_expert_modelè¿˜ä¼šæ¥å—adarms_cond(ä¸€ä¸ªè·Ÿtimestepæœ‰å…³çš„embedding)ä½œä¸ºè¾“å…¥
                if getattr(self.config, 'is_action_dit_dense_timestep', False):
                    if getattr(self.config, "is_loc_learnable_query", False):
                        action_outputs = self.action_dit_forward_with_adarmscond(
                            hidden_states=self.loc_learnable_query,
                            encoder_hidden_states=locbrh_hidden_states, #torch.Size([128, 708, 896])
                            encoder_attention_mask=locbrh_attention_mask, #torch.Size([128, 708])
                            encoder_position_ids=locbrh_position_ids, #torch.Size([128, 708])
                            adarms_cond=adarms_cond,
                        )
                    else:
                        action_outputs = self.action_dit_forward_with_adarmscond(
                            hidden_states=action_dit_inputs, #torch.Size([128, 708, 896])
                            attention_mask=action_dit_att_mask, #torch.Size([128, 708])
                            position_ids=action_dit_pos_ids, #torch.Size([128, 708])
                            adarms_cond=adarms_cond,
                            # å’ŒPi05çš„ä¸åŒï¼šé™¤äº†æ²¡æœ‰ä½¿ç”¨adarms_condä¹‹å¤–ï¼Œaction_ditä¹Ÿæ²¡æœ‰ä½¿ç”¨full_att_2d_masks_4d
                                # Pi05çš„language_modelå’ŒDiTéƒ½ä½¿ç”¨äº†prefixåŒå‘ï¼Œsuffixå•å‘çš„maskï¼›
                                # è€ŒUniLIPçš„language_modelå’ŒDiTä½¿ç”¨äº†å•å‘maskï¼Œä½†æ˜¯ä¸­é—´çš„llm_connectorä½¿ç”¨äº†å•å‘maskï¼›
                        )
                    action_hidden = action_outputs
                else:
                    if getattr(self.config, "is_loc_learnable_query", False):
                        action_outputs = self.model.action_dit(
                            inputs_embeds=self.loc_learnable_query,
                            encoder_inputs_embeds=locbrh_hidden_states, #torch.Size([128, 708, 896])
                            encoder_attention_mask=locbrh_attention_mask, #torch.Size([128, 708])
                            encoder_position_ids=locbrh_position_ids, #torch.Size([128, 708])
                            output_hidden_states=True,
                            return_dict=return_dict,
                            use_cache=False
                        )
                    else:
                        action_outputs = self.model.action_dit(
                            inputs_embeds=action_dit_inputs, #torch.Size([128, 708, 896])
                            attention_mask=action_dit_att_mask, #torch.Size([128, 708])
                            position_ids=action_dit_pos_ids, #torch.Size([128, 708])
                            output_hidden_states=True,
                            # adarms_cond=[None, adarms_cond],
                            # å’ŒPi05çš„ä¸åŒï¼šé™¤äº†æ²¡æœ‰ä½¿ç”¨adarms_condä¹‹å¤–ï¼Œaction_ditä¹Ÿæ²¡æœ‰ä½¿ç”¨full_att_2d_masks_4d
                                # Pi05çš„language_modelå’ŒDiTéƒ½ä½¿ç”¨äº†prefixåŒå‘ï¼Œsuffixå•å‘çš„maskï¼›
                                # è€ŒUniLIPçš„language_modelå’ŒDiTä½¿ç”¨äº†å•å‘maskï¼Œä½†æ˜¯ä¸­é—´çš„llm_connectorä½¿ç”¨äº†å•å‘maskï¼›
                            return_dict=return_dict,
                            use_cache=False
                        )

                    # Get output corresponding to the Action Token (Last token)
                    # output: [BS, Seq+1, Hidden]
                    action_hidden = action_outputs.hidden_states[-1] # [BS, seq+1, Hidden] # [:, -1:, :] # [BS, 1, Hidden] #torch.Size([128, 1, 896])

                #### TODO
                # # ç¬¬äºŒæ¬¡ç»¼ä¸Šæ‰€è¿°ï¼Œå…ˆå¿½ç•¥zeroså¡«å……çš„é£é™©ç‚¹ï¼Œè·‘é€šæ¨¡å‹ã€‚åç»­å†ä¿®æ”¹action_ditçš„ä»£ç æ¥é€‚é…adarms_cond
                # action_hidden = self.model.action_dit(
                #     suffix_emb, #torch.Size([128, 32, 16, 16])
                #     timestep=time, #128
                #     encoder_hidden_states=hidden_states, # [BS, Seq, C] ##torch.Size([128, 707, 2304])
                #     encoder_attention_mask=attention_mask, #torch.Size([128, 707])
                #     return_dict=False
                # )[0] #[BS, 1, Hidden]

                if getattr(self.config, "is_loc_learnable_query", False):
                    action_hidden = action_hidden[:,-1,:]
                else:
                    # Gather from the same indices we scattered to
                    gather_indices = valid_lens.view(-1,1,1).expand(-1, -1, hidden_dim)
                    action_hidden = action_hidden.gather(1, gather_indices) # [BS, 1, Hidden]
                # bs=8æ˜¾å­˜å ç”¨=13316MiB
                # 4. Final Projection (Velocity Prediction)
                v_t_pred = self.get_model().action_out_proj(action_hidden) # [BS, 1, 5] #torch.Size([128, 1, 5])
                # # åœ¨ --- Localiztion Branch --- å†…éƒ¨ï¼Œv_t_pred è®¡ç®—å‡ºæ¥åæ’å…¥
                # logging.info(f"  Action Pred Mean: {v_t_pred.mean().item():.4f}, Std: {v_t_pred.std().item():.4f}")
                # logging.info(f"  Action GT Mean: {u_t.mean().item():.4f}, Std: {u_t.std().item():.4f}")

                # 5. Calculate Loss (MSE)
                loc_loss = F.mse_loss(v_t_pred.float(), u_t.float(), reduction="none") #torch.Size([128, 1, 5])
                loc_loss = loc_loss.mean(dim=[1, 2]) # [BS] #torch.Size([128])

                # Apply Mask: Only count loss for Loc samples
                masked_loc_loss = (loc_loss * locbrh_loss_mask[:, 0]).mean()#loss_mask[:, 0].sum()=tensor(57., device='cuda:0', dtype=torch.bfloat16)
            else:
                masked_loc_loss = torch.nn.MSELoss()(hidden_states, torch.clone(hidden_states.detach())).to(torch.float32)

        alpha_loc_aux_loss = torch.tensor(self.model.config.alpha_loc_aux_loss).to(torch.float32)
        alpha_loc_loss = torch.tensor(self.model.config.alpha_loc_loss).to(torch.float32)
        total_loss = masked_gen_loss + masked_loc_loss * alpha_loc_loss + masked_loc_aux_loss * alpha_loc_aux_loss
        # logging.info(f"total_loss: {total_loss.detach().cpu().numpy().item():6f}, masked_loc_loss: {masked_loc_loss.detach().cpu().numpy().item():6f}, alpha_loc_loss: {alpha_loc_loss.detach().cpu().numpy().item():6f}, masked_gen_loss: {masked_gen_loss.detach().cpu().numpy().item():6f}, masked_loc_aux_loss: {masked_loc_aux_loss.detach().cpu().numpy().item():6f}, alpha_loc_aux_loss: {alpha_loc_aux_loss.detach().cpu().numpy().item():6f}")
        # 224 + lora=64
        # bs=8æ˜¾å­˜å ç”¨=13316MiB
        # bs=16æ˜¾å­˜å ç”¨=20846MiB
        # bs=32æ˜¾å­˜å ç”¨=35678MiB
        #
        outputs = CausalLMOutputWithPast(
            loss=total_loss,
            logits=None, # Not used
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )

        outputs.extras = {
            "other_info": {
                "loc_indices": len(loc_indices),
                "gen_indices": len(gen_indices),
                "total_loss": total_loss.detach().cpu().numpy().item(),
                "loc_loss": masked_loc_loss.detach().cpu().numpy().item(),
                "alpha_loc": alpha_loc_loss.detach().cpu().numpy().item(),
                "gen_loss": masked_gen_loss.detach().cpu().numpy().item(),
                "loc_aux_loss": masked_loc_aux_loss.detach().cpu().numpy().item(),
                "alpha_loc_aux": alpha_loc_aux_loss.detach().cpu().numpy().item(),
            }
        }

        return outputs

    def forward_for_aux_loc_loss(
        self,
        sigmas,
        noisy_latents,
        noise_pred,
        aux_loc_input_ids,
        aux_loc_labels,
        attention_mask,
        und_image_map,
        gen_image,
        grid_thw,
        i_s_pos,
        image_sizes,
        task_id,
        return_dict,
        actions,
        loss_mask
    ):
        # 1. Estimate x_0 (Clean Latent) from current prediction
        # Flow Matching (Euler): x_t = (1-t)x_0 + t*x_1; v = x_1 - x_0
        # => x_0 = x_t - t * v (approx)
        # æ³¨æ„: è¿™é‡Œçš„ sigmas å¯¹åº” tã€‚target å¯¹åº” v (å¦‚æœ noise_scheduler æ˜¯ rectified flow)
        # å¦‚æœæ˜¯ standard diffusion, å…¬å¼ä¸åŒã€‚è¿™é‡Œå‡è®¾æ˜¯ Rectified Flow (Sana Default)
        t_broadcast = sigmas
        pred_latents_x0 = noisy_latents - t_broadcast * noise_pred # \hat{x}_0 (Latent Space)

        # 2. VAE Decode (Latent -> Pixel)
        # [Gradient Flow] æ¢¯åº¦éœ€è¦æµç» pred_latents_x0 -> noise_pred -> DiT
        # å› æ­¤è¿™é‡Œä¸èƒ½ç”¨ no_grad
        # scale back: UniLIP Factor
        pred_latents_scaled = pred_latents_x0 / self.model.config.unilip_factor
        # VAE Decode is heavy! Use with caution.
        with torch.no_grad():
            # pred_pixels = self.model.vae_decoder.vae_decode(pred_latents_scaled) # [BS, 3, H, W] (-1~1)

            # [ä¿®æ”¹] ä½¿ç”¨ Mini-Batch å¾ªç¯è§£ç ï¼Œé¿å… Tensor è¿‡å¤§
            mini_batch_size = 64  # å®‰å…¨å€¼ï¼Œæ ¹æ®æ˜¾å­˜è°ƒæ•´ (1, 2, 4, 8)
            pred_pixels_list = []
            # æ˜¾å¼å¾ªç¯è§£ç 
            for i in range(0, pred_latents_scaled.shape[0], mini_batch_size):
                batch_latents = pred_latents_scaled[i : i + mini_batch_size]

                # ä½¿ç”¨ no_grad (å¦‚æœä½ ä¸éœ€è¦ VAE çš„æ¢¯åº¦ï¼Œé€šå¸¸ VAE æ˜¯å†»ç»“çš„)
                # æ³¨æ„ï¼šå¦‚æœ pred_latents_scaled éœ€è¦æ¢¯åº¦å›ä¼ åˆ° DiTï¼Œè¿™é‡Œä¸èƒ½ç”¨ no_gradï¼
                # æ ¹æ®ä½ çš„ä»£ç é€»è¾‘ï¼Œä½ éœ€è¦æ¢¯åº¦æµå‘ DiTï¼Œæ‰€ä»¥å¿…é¡»ä¿ç•™æ¢¯åº¦è®¡ç®—ã€‚

                # è¿™é‡Œçš„ checkpointing éå¸¸å…³é”®ï¼
                # å¦‚æœ VAE æ²¡æœ‰å¼€å¯ GCï¼Œè¿™ä¸€æ­¥ä¼šåƒæ‰å·¨å¤§æ˜¾å­˜ã€‚
                # ä½†é’ˆå¯¹ Int32 Overflowï¼Œæˆ‘ä»¬ä¸»è¦æ˜¯ä¸ºäº†å‡å°‘å•æ¬¡ Conv2d çš„è¾“å…¥è§„æ¨¡ã€‚
                batch_pixels = self.model.vae_decoder.vae_decode(batch_latents)
                pred_pixels_list.append(batch_pixels)
            # é‡æ–°æ‹¼æ¥
            pred_pixels = torch.cat(pred_pixels_list, dim=0)

        # bs=8æ˜¾å­˜å ç”¨=13314MiB
        # 3. Process for Vision Encoder (SigLIP)
        # SigLIP expects [0, 1] and specific normalization
        # pred_pixels is [-1, 1], convert to [0, 1]
        pred_pixels_norm = (pred_pixels + 1.0) / 2.0
        # Resize to Vision Encoder size (e.g. 448) if needed
        # VAE output is usually 512 or 1024. SigLIP is 448.
        if pred_pixels_norm.shape[-1] != gen_image.size(0):
            pred_pixels_norm = F.interpolate(pred_pixels_norm, size=(und_image_map.shape[-2],und_image_map.shape[-1]), mode='bilinear', align_corners=False)
        # SigLIP Normalization (Mean/Std) - Approximate or use processor values
        # mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5] for simplicity/speed in training loop
        # Or use self.image_processor logic
        pred_pixels_input = (pred_pixels_norm - 0.5) / 0.5

        # 4. Forward Localization Branch with Generated Image
        # [Important] Freeze Loc Branch weights to avoid updating them with noisy gradients
        # We only want gradients to flow back to the Image (pred_pixels_input)

        # æ„é€  Loc åˆ†æ”¯è¾“å…¥
        # Und Image = Generated Fps Image (pred_pixels_input)
        # Aux Image = Original Map (und_image passed in forward, which is actually map for Gen task)
        # Prompt = Loc Prompt (aux_loc_input_ids)

        # è·å– Gen ä»»åŠ¡åŸæœ¬çš„ Map è¾“å…¥ (åœ¨ prepare_inputs é‡Œå®ƒæ˜¯ und_image)
        combined_und_images = torch.cat([pred_pixels_input, und_image_map], dim=0)
        ( # return None, position_ids, attention_mask, past_key_values, text_embeds, labels, target_image_embeds, combined_img_idx, combined_image_embeds, bidr_attention_mask
            aux_loc_input_ids,
            position_ids,
            attention_mask,
            past_key_values,
            inputs_embeds,
            aux_loc_labels,
            target_image_embeds, #latents
            combined_img_idx,
            combined_image_embeds,
            bidr_attention_mask
        ) = self.prepare_inputs_labels_for_multimodal(
            aux_loc_input_ids,
            None, #position_ids,
            attention_mask,
            None, #past_key_values,
            aux_loc_labels,
            gen_image,
            combined_und_images, # Pass und_image (which contains all input visuals)
            None, #grid_thw,
            None, #i_s_pos,
            None, #image_sizes,
            task_id,
        )
        und_img_idx = combined_img_idx[:combined_img_idx.size(0)//2, ...] #und_img_idx,sum()=32768 #32768/256=128.0
        aux_img_idx = combined_img_idx[combined_img_idx.size(0)//2:, ...]#aux_img_idx.sum()=tensor(14592, device='cuda:0') #14592/256=57
        und_image_embeds = combined_image_embeds[:combined_image_embeds.size(0)//2, ...]#torch.Size([128, 256, 896])
        aux_image_embeds = combined_image_embeds[combined_image_embeds.size(0)//2:, ...]#torch.Size([128, 256, 896])

        # --- B. Main LLM Forward (Understanding) ---
        position_ids = torch.cumsum(attention_mask, dim=1) - 1
        position_ids[position_ids < 0] = 0
        # bs=8æ˜¾å­˜å ç”¨=13314MiB
        with torch.no_grad():
            outputs = self.model.language_model(
                attention_mask=attention_mask, #torch.Size([2, 617])
                position_ids=position_ids, #torch.Size([2, 617])
                inputs_embeds=inputs_embeds, #torch.Size([2, 617, 896])
                output_hidden_states=True,
                return_dict=return_dict, #True
                use_cache=False
            )

        # Last Hidden State from LLM: [BS, Seq_Len, Hidden_Size]
        # This contains contextualized features of both text and images.
        hidden_states = outputs.hidden_states[-1] #torch.Size([128, 707, 896])

        # Re-fill und_image embeddings (Skip Connection logic from UniLIP)
        if und_image_embeds is not None and und_img_idx is not None:
            hidden_states[und_img_idx] = und_image_embeds.to(hidden_states.device).flatten(0,1)

        # åœ¨ä¼ å…¥forward_for_aux_loc_lossä¹‹å‰ï¼Œå·²ç»å°†æ‰€æœ‰task_idè½¬æ¢ä¸º0ï¼Œå³è¯¥batchä¸­æ‰€æœ‰æ ·æœ¬éƒ½æ˜¯locä»»åŠ¡
        is_loc_task = (task_id == 0)#is_loc_task.shape=torch.Size([128])#is_loc_task.sum()=tensor(57, device='cuda:0')
        if aux_image_embeds is not None and und_img_idx is not None: #aux_img_idx.sum()/128 = 57
            hidden_states[aux_img_idx] = aux_image_embeds[is_loc_task].to(hidden_states.device).flatten(0,1)#hidden_states[aux_img_idx].shape=torch.Size([14592, 896]) #aux_image_embeds[is_loc_task].shape=torch.Size([2, 256, 896])

        # ä¸´æ—¶å†»ç»“ Action Dit
        self.model.action_dit_norm.requires_grad_(False)
        self.model.action_dit_projector.requires_grad_(False)
        self.model.action_dit.requires_grad_(False)
        self.model.action_in_proj.requires_grad_(False)
        self.model.action_out_proj.requires_grad_(False)
        self.model.time_mlp_in.requires_grad_(False)
        self.model.time_mlp_out.requires_grad_(False)
        self.model.action_dit_projector.eval()
        self.model.action_dit.eval()
        self.model.action_dit.eval()
        self.model.action_in_proj.eval()
        self.model.action_out_proj.eval()
        self.model.time_mlp_in.eval()
        self.model.time_mlp_out.eval()
        # bs=8æ˜¾å­˜å ç”¨=13314MiB
        # 5. Original LOCALIZATION Branch (Flow Matching Path)
        with torch.no_grad():
            actions = actions#torch.Size([128, 5])
            noise = self.sample_noise(actions.shape, actions.device)#torch.Size([128, 5])
            time = self.sample_time(actions.shape[0], actions.device)#torch.Size([128])
            time_expanded = time[:, None, None].to(actions.dtype)
            x_t = time_expanded * noise + (1 - time_expanded) * actions
            u_t = noise - actions

            suffix_emb, adarms_cond = self.embed_action_suffix(
                    x_t, #torch.Size([128, 1, 5])
                    time, #torch.Size([128])
                    llm_hidden_size=self.model.config.text_config.hidden_size,
                    device=actions.device,
                    dtype=hidden_states.dtype
                )

            hidden_states = self.get_model().action_dit_norm(hidden_states)

            bs, seq_len, hidden_dim = hidden_states.shape
            valid_lens = attention_mask.sum(dim=1).long()

            action_dit_inputs = torch.cat([hidden_states, torch.zeros_like(suffix_emb)], dim=1)
            target_indices = valid_lens.view(-1, 1, 1).expand(-1, 1, hidden_dim)
            action_dit_inputs = action_dit_inputs.scatter(1, target_indices, suffix_emb)

            action_dit_att_mask = torch.cat([attention_mask, torch.zeros((bs, 1), device=attention_mask.device, dtype=attention_mask.dtype)], dim=1)
            mask_indices = valid_lens.view(-1, 1)
            action_dit_att_mask = action_dit_att_mask.scatter(1, mask_indices, 1)

            action_dit_pos_ids = torch.cat([position_ids, torch.zeros((bs, 1), device=position_ids.device, dtype=position_ids.dtype)], dim=1)
            action_pos_ids = valid_lens.view(-1, 1)
            # action_dit_pos_ids = action_dit_pos_ids.scatter(1, mask_indices, action_pos_ids)
            current_seq_len = action_dit_pos_ids.shape[1]
            range_ids = torch.arange(current_seq_len, device=position_ids.device).unsqueeze(0) # [1, Seq+1]
            mask_after_valid = range_ids >= valid_lens.view(-1, 1)
            action_dit_pos_ids = torch.where(
                mask_after_valid,
                action_pos_ids,
                action_dit_pos_ids
            )

            if getattr(self.config, "is_action_dit_projector", False):
                action_dit_inputs = self.get_model().action_dit_projector(action_dit_inputs)

            if getattr(self.config, 'is_action_dit_dense_timestep', False):
                action_outputs = self.action_dit_forward_with_adarmscond(
                    hidden_states=action_dit_inputs,
                    attention_mask=action_dit_att_mask,
                    position_ids=action_dit_pos_ids,
                    adarms_cond=adarms_cond,
                )
                action_hidden = action_outputs
            else:
                action_outputs = self.model.action_dit(
                    inputs_embeds=action_dit_inputs,
                    attention_mask=action_dit_att_mask,
                    position_ids=action_dit_pos_ids,
                    output_hidden_states=True,
                    return_dict=return_dict,
                    use_cache=False
                )
                # Get output corresponding to the Action Token (Last token)
                # output: [BS, Seq+1, Hidden]
                action_hidden = action_outputs.hidden_states[-1]# [BS, seq+1, Hidden] #[:, -1:, :] # [BS, 1, Hidden] #torch.Size([128, 1, 896])

            # Gather from the same indices we scattered to
            gather_indices = valid_lens.view(-1,1,1).expand(-1, -1, hidden_dim)
            action_hidden = action_hidden.gather(1, gather_indices) # [BS, 1, Hidden]

            v_t_pred = self.get_model().action_out_proj(action_hidden) # [BS, 1, 5] #torch.Size([128, 1, 5])
            loc_loss = F.mse_loss(v_t_pred.float(), u_t.float(), reduction="none") #torch.Size([128, 1, 5])
            loc_loss = loc_loss.mean(dim=[1, 2]) # [BS] #torch.Size([128])

            # åŠ æƒï¼šåªåœ¨ t å°çš„æ—¶å€™ (ç”Ÿæˆæ¥è¿‘å®Œæˆ) è®¡ç®— Loss
            # sigmas è¶Šå¤§å™ªå£°è¶Šå¤§ã€‚æˆ‘ä»¬å¸Œæœ› sigma å°çš„æ—¶å€™æƒé‡é«˜ã€‚
            weight = (1.0 - sigmas).clamp(min=0)
            loc_loss = (loc_loss * weight.squeeze())

            # Apply Mask: Only count aux loc loss for Gen samples
            masked_loc_loss = (loc_loss * loss_mask[:, 1]).mean()

        # æ¢å¤è®­ç»ƒ Action Dit
        self.model.action_dit_norm.requires_grad_(True)
        self.model.action_dit_projector.requires_grad_(True)
        self.model.action_dit.requires_grad_(True)
        self.model.action_in_proj.requires_grad_(True)
        self.model.action_out_proj.requires_grad_(True)
        self.model.time_mlp_in.requires_grad_(True)
        self.model.time_mlp_out.requires_grad_(True)
        self.model.action_dit_norm.train()
        self.model.action_dit_projector.train()
        self.model.action_dit.train()
        self.model.action_in_proj.train()
        self.model.action_out_proj.train()
        self.model.time_mlp_in.train()
        self.model.time_mlp_out.train()
        # bs=8æ˜¾å­˜å ç”¨=13316MiB
        return masked_loc_loss

    def action_dit_forward_with_adarmscond(
        self,
        hidden_states,#torch.Size([128, 708, 896])
        attention_mask, #torch.Size([128, 708])
        position_ids, #torch.Size([128, 708])
        adarms_cond,
        # å’ŒPi05çš„ä¸åŒï¼šé™¤äº†æ²¡æœ‰ä½¿ç”¨adarms_condä¹‹å¤–ï¼Œaction_ditä¹Ÿæ²¡æœ‰ä½¿ç”¨full_att_2d_masks_4d
            # Pi05çš„language_modelå’ŒDiTéƒ½ä½¿ç”¨äº†prefixåŒå‘ï¼Œsuffixå•å‘çš„maskï¼›
            # è€ŒUniLIPçš„language_modelå’ŒDiTä½¿ç”¨äº†å•å‘maskï¼Œä½†æ˜¯ä¸­é—´çš„llm_connectorä½¿ç”¨äº†å•å‘maskï¼›

    ):
        # 1. å‡†å¤‡ Rotary Embedding (Qwen2 éœ€è¦)
        kv_seq_len = hidden_states.shape[1]
        # cos, sin = self.model.action_dit.rotary_emb(hidden_states, position_ids)
        position_embeddings = self.model.action_dit.rotary_emb(hidden_states, position_ids)

        # [å¯é€‰] åœ¨ Loop ä¹‹å‰
        # åˆ©ç”¨ transformers æä¾›çš„ helper æ‰©å±• mask
        # æ³¨æ„ï¼šQwen2 çš„å®ç°å¯èƒ½éœ€è¦ 4D mask
        # extended_attention_mask = self.model.action_dit.get_extended_attention_mask(
        #     attention_mask,
        #     attention_mask.shape,
        #     attention_mask.device
        # )# åœ¨ layer.self_attn ä¸­ä½¿ç”¨ extended_attention_mask

        # from transformers.masking_utils import create_causal_mask
        # mask_kwargs = {
        #         "config": self.model.action_dit.config,
        #         "input_embeds": hidden_states,
        #         "attention_mask": attention_mask,
        #         "cache_position": None,
        #         "past_key_values": None,
        #         "position_ids": position_ids,
        #     }
        # extended_attention_mask = create_causal_mask(**mask_kwargs)
        # ============================================================
        # [ä¿®å¤] ä½¿ç”¨ Qwen2 å†…éƒ¨çš„ _update_causal_mask
        # ============================================================
        # è¿™ä¸ªæ–¹æ³•ä¼šè‡ªåŠ¨å¤„ç†ï¼š
        # 1. Causal Mask (ä¸‹ä¸‰è§’)
        # 2. Padding Mask (æ ¹æ®ä¼ å…¥çš„ attention_mask)
        # 3. ç»´åº¦æ‰©å±• -> [BS, 1, Seq, Seq]
        # 4. Flash Attention å…¼å®¹æ€§ (å¦‚æœæ˜¯ FA2ï¼Œå®ƒå¯èƒ½è¿”å› None æˆ–ç‰¹å®šæ ¼å¼)

        # ä¸ºäº†å…¼å®¹ transformers ç‰ˆæœ¬ï¼Œæˆ‘ä»¬éœ€è¦æ„é€  cache_position (é€šå¸¸æ˜¯ arange)
        # å¦‚æœæŠ¥é”™ç¼ºå°‘ cache_positionï¼Œå¯ä»¥ç”¨ä¸‹é¢çš„ç®€å•é€»è¾‘ç”Ÿæˆ
        # cache_position = torch.arange(
        #     0, kv_seq_len, device=hidden_states.device
        # )
        # causal_mask = self.model.action_dit._update_causal_mask(
        #     attention_mask,
        #     hidden_states,
        #     cache_position,
        #     past_key_values=None,
        #     output_attentions=False
        # )
        # å¤‡ç”¨æ–¹æ¡ˆï¼šæ‰‹åŠ¨æ„å»º 4D Causal Mask
        batch_size, seq_len = hidden_states.shape[:2]

        # 1. åˆ›å»ºä¸‹ä¸‰è§’ Causal Mask [Seq, Seq]
        # min_dtype æ˜¯ float çš„æœ€å°å€¼ (e.g. -65504 for fp16, -3.4e38 for fp32)
        min_dtype = torch.finfo(hidden_states.dtype).min
        causal_mask = torch.full((seq_len, seq_len), min_dtype, device=hidden_states.device, dtype=hidden_states.dtype)
        causal_mask = torch.triu(causal_mask, diagonal=1) # ä¸Šä¸‰è§’ä¸ºè´Ÿæ— ç©·ï¼Œä¸‹ä¸‰è§’ä¸º0

        # 2. æ‰©å±•ç»´åº¦ [1, 1, Seq, Seq]
        causal_mask = causal_mask[None, None, :, :]

        # 3. å¤„ç† Padding Mask [BS, Seq] -> [BS, 1, 1, Seq]
        # æ³¨æ„ï¼šattention_mask æ˜¯ 1=Valid, 0=Pad
        # æˆ‘ä»¬éœ€è¦æŠŠ 0 å˜æˆè´Ÿæ— ç©·ï¼Œ1 å˜æˆ 0
        padding_mask = torch.zeros_like(attention_mask, dtype=hidden_states.dtype)
        padding_mask = padding_mask.masked_fill(attention_mask == 0, min_dtype)
        padding_mask = padding_mask[:, None, None, :]

        # 4. åˆå¹¶ [BS, 1, Seq, Seq]
        # åˆ©ç”¨å¹¿æ’­æœºåˆ¶ï¼šCausal (mask future) + Padding (mask pad tokens)
        combined_mask = causal_mask + padding_mask

        # 2. é€å±‚å‰å‘ä¼ æ’­
        for i, layer in enumerate(self.model.action_dit.layers):

            # --- Attention Block ---
            residual = hidden_states

            # [å…³é”®] è°ƒç”¨ AdaRMS Input Norm (ä¼ å…¥ cond)
            # adarms_cond æ˜¯æˆ‘ä»¬åœ¨ embed_action_suffix é‡Œç®—å‡ºæ¥çš„
            hidden_states, gate_attn = layer.input_layernorm(hidden_states, cond=adarms_cond)

            # Self Attention
            # Qwen2 çš„ self_attn forward ç­¾åé€šå¸¸æ˜¯:
            # (hidden_states, attention_mask, position_ids, past_key_values, output_attentions, use_cache, **kwargs)
            # æ³¨æ„ï¼šæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨å¤„ç† cos/sin çš„ä¼ å…¥ï¼ŒQwen2 å†…éƒ¨é€šå¸¸ä¼šè‡ªåŠ¨å¤„ç†ï¼Œä½†æˆ‘ä»¬éœ€è¦ç¡®ä¿ position_ids æ­£ç¡®
            hidden_states, _ = layer.self_attn(
                hidden_states=hidden_states,#torch.Size([2, 708, 896])
                attention_mask=combined_mask,#causal_mask,#extended_attention_mask[:, None, :, :], # éœ€è¦å¹¿æ’­ä¸º [BS, 1, Seq, Seq] æˆ–è€…æ˜¯ FlashAttn æ ¼å¼
                position_ids=position_ids,
                position_embeddings=position_embeddings,
                past_key_values=None,
                output_attentions=False,
                use_cache=False,
            )
            hidden_states = residual + gate_attn * hidden_states
            # --- MLP Block ---
            residual = hidden_states
            # [å…³é”®] è°ƒç”¨ AdaRMS Post Norm (ä¼ å…¥ cond)
            hidden_states, gate_mlp = layer.post_attention_layernorm(hidden_states, cond=adarms_cond)
            hidden_states = layer.mlp(hidden_states)
            hidden_states = residual + gate_mlp * hidden_states

        # 3. Final Norm
        hidden_states, _ = self.model.action_dit.norm(hidden_states, cond=adarms_cond)
        # 4. æå–è¾“å‡º (ä¿æŒåŸé€»è¾‘)
        action_outputs = hidden_states

        return action_outputs

    @torch.no_grad()
    def generate_image(
        self,
        text: List[str],
        tokenizer: AutoTokenizer,
        pixel_values: Optional[torch.Tensor] = None,
        image_grid_thw: Optional[torch.Tensor] = None,
        max_var: Optional[float] = None,
        generator=None,
        guidance_scale: float = 4.5,
    ):
        dit_path = self.model.config.dit_path
        scheduler = DPMSolverMultistepScheduler.from_pretrained(dit_path, subfolder="scheduler")

        N_QUERY = self.get_n_query()
        inputs = tokenizer(text, padding="longest", return_tensors="pt")
        device = self.get_model().device
        attention_mask = inputs.attention_mask.to(device)
        input_ids = inputs.input_ids.to(device)  # B x N

        text_embeds = self.get_model().language_model.embed_tokens(input_ids)
        latent_queries = self.get_model().latent_queries.repeat(text_embeds.shape[0], 1, 1)

        if pixel_values is not None:
            und_image_idx = (input_ids == UND_IMAGE_TOKEN_IDX)
            pixel_values = pixel_values.type(self.vision_tower.dtype)
            vision_feature_layer = self.config.vision_feature_layer
            vision_feature_select_strategy = self.config.vision_feature_select_strategy
            und_image_embeds = self.model.get_image_features(
                pixel_values=pixel_values,
                vision_feature_layer=vision_feature_layer,
                vision_feature_select_strategy=vision_feature_select_strategy,
                image_sizes=None,
            )

            text_embeds[und_image_idx] = und_image_embeds.to(text_embeds.device).repeat(2,1,1).flatten(0,1)

        text_embeds = torch.cat([text_embeds, latent_queries], dim=1)
        attention_mask = torch.cat([attention_mask, torch.ones_like(latent_queries[:, :, 0])], dim=1).int()
        position_ids = torch.cumsum(attention_mask, dim=1) - 1
        position_ids[position_ids < 0] = 0

        outputs = self.model.language_model(
            inputs_embeds=text_embeds,
            attention_mask=attention_mask.bool(),
            position_ids= position_ids,
            output_hidden_states=True,
            return_dict=True,
        )

        hidden_states = outputs.hidden_states[-1]
        if pixel_values is not None:
            und_image_idx = torch.cat([und_image_idx, torch.zeros_like(latent_queries[:, :, 0]).bool()], dim=1)
            hidden_states[und_image_idx] = und_image_embeds.to(hidden_states.device).repeat(2,1,1).flatten(0,1)

        attention_mask = attention_mask.bool()
        bidr_attention_mask = attention_mask.unsqueeze(2) & attention_mask.unsqueeze(1)
        bidr_attention_mask = bidr_attention_mask.unsqueeze(1)
        bidr_attention_mask = (1-bidr_attention_mask.float())*-100000

        img_hidden_states = self.model.llm_connector(
            inputs_embeds=hidden_states,
            attention_mask=bidr_attention_mask,
            position_ids=position_ids,
            output_hidden_states=True,
            return_dict=True,
        ).hidden_states[-1]

        img_hidden_states = self.get_model().projector(img_hidden_states)

        steps = 20
        logging.info("steps, guiance scale", steps, guidance_scale)
        # null first
        bsz = img_hidden_states.shape[0]
        img_hidden_states = torch.cat([img_hidden_states[bsz//2:], img_hidden_states[:bsz//2]])
        attention_mask = torch.cat([attention_mask[bsz//2:], attention_mask[:bsz//2]])

        output_img = self.sample_image_latents(img_hidden_states, scheduler, encoder_attention_mask=attention_mask, generator=generator, num_inference_steps=steps, guidance_scale=guidance_scale)

        logging.info(self.model.config.unilip_factor)
        output_img = self.model.vae_decoder.vae_decode(output_img.float()/ self.model.config.unilip_factor)

        output_img = ((output_img[0].permute(1,2,0).clamp(-1,1).float().cpu().numpy() + 1)/2)

        original_height, original_width = output_img.shape[:2]
        # å®šä¹‰ç¼©æ”¾æ¯”ä¾‹
        scale_factor = 28 / 32  # 0.875ï¼Œå³ç¼©å°ä¸ºåŸæ¥çš„87.5%
        # è®¡ç®—æ–°çš„å°ºå¯¸
        new_width = int(original_width * scale_factor)
        new_height = int(original_height * scale_factor)
        new_size = (new_width, new_height)
        # ä½¿ç”¨åŒçº¿æ€§æ’å€¼ï¼ˆINTER_LINEARï¼‰è¿›è¡Œç¼©æ”¾
        resized_img = cv2.resize(output_img, new_size, interpolation=cv2.INTER_LINEAR)

        return resized_img

    def sample_image_latents(
        self,
        img_hidden_states,
        scheduler,
        encoder_attention_mask=None,
        guidance_scale: float = 3.0,
        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
        num_inference_steps: int = 30,
        num_images_per_prompt: int = 1,
        return_tensor=False,
        **kwargs,
    ):
        device = img_hidden_states.device
        dtype = img_hidden_states.dtype

        img_hidden_states_input = img_hidden_states
        # here already is cfg feature
        batch_size = img_hidden_states.shape[0]//2
        latent_size = 16
        latent_channels = 32

        latents = randn_tensor(
            shape=(batch_size * num_images_per_prompt, latent_channels, latent_size, latent_size),
            generator=generator,
            device=device,
            dtype=dtype,
        )

        # set step values
        if isinstance(scheduler, FlowMatchEulerDiscreteScheduler):
            sigmas = np.linspace(1.0, 1 / num_inference_steps, num_inference_steps)
            scheduler.set_timesteps(num_inference_steps, sigmas=sigmas)
        else:
            scheduler.set_timesteps(num_inference_steps)

        # Repeat z_latents and conditions for each image per prompt
        img_hidden_states_input = img_hidden_states_input.repeat_interleave(num_images_per_prompt, dim=0)

        for t in scheduler.timesteps:
            latent_model_input = latents.repeat(2, 1, 1, 1)
            if hasattr(scheduler, "scale_model_input"):
                latent_model_input = scheduler.scale_model_input(latent_model_input, t)
            latent_model_input = latent_model_input.to(img_hidden_states_input.dtype)

            # predict noise model_output
            noise_pred = self.get_model().dit(
                latent_model_input,
                timestep=t.unsqueeze(0).expand(latent_model_input.shape[0]).to(latent_model_input.device, torch.long),
                encoder_hidden_states=img_hidden_states_input,
                encoder_attention_mask=encoder_attention_mask,
                return_dict=False,
            )[0]
            noise_pred = noise_pred.float()

            # perform guidance
            noise_pred_uncond, noise_pred = noise_pred.chunk(2)
            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred - noise_pred_uncond)

            # compute previous image: x_t -> x_t-1
            latents = scheduler.step(noise_pred, t, latents, generator=generator).prev_sample

        return latents

    # # ==========================================
    # # 4. [NEW] Inference: Generate Action (Flow Matching)
    # # ==========================================
    # @torch.no_grad()
    # def generate_action(
    #     self,
    #     text: List[str],
    #     tokenizer: AutoTokenizer,
    #     und_images: Optional[torch.Tensor] = None,
    #     aux_images: Optional[torch.Tensor] = None,
    #     num_steps: int = 10
    # ):
    #     """
    #     Run inference for Localization task using Flow Matching Euler Solver.
    #     Analogous to Pi0.5 `sample_actions`.
    #     """
    #     # 1. Precompute Context (LLM Forward)
    #     inputs = tokenizer(text, padding="longest", return_tensors="pt")
    #     device = self.get_model().device
    #     attention_mask = inputs.attention_mask.to(device)
    #     input_ids = inputs.input_ids.to(device)  # B x N
    #     text_embeds = self.get_model().language_model.embed_tokens(input_ids)

    #     vision_feature_layer = self.config.vision_feature_layer
    #     vision_feature_select_strategy = self.config.vision_feature_select_strategy

    #     if und_images is not None:
    #         und_image_embeds = self.model.get_image_features(
    #             pixel_values=und_images,
    #             vision_feature_layer=vision_feature_layer,
    #             vision_feature_select_strategy=vision_feature_select_strategy,
    #             image_sizes=None,
    #         )

    #     if aux_images is not None:
    #         aux_image_embeds = self.model.get_image_features(
    #             pixel_values=aux_images,
    #             vision_feature_layer=vision_feature_layer,
    #             vision_feature_select_strategy=vision_feature_select_strategy,
    #             image_sizes=None,
    #         )

    #     und_image_idx, aux_image_idx = split_image_tokens(input_ids, IMAGE_TOKEN_IDX)
    #     if und_images is not None and und_image_idx.any():
    #         text_embeds[und_image_idx] = und_image_embeds.to(text_embeds.device).repeat(2,1,1).flatten(0,1)
    #     if aux_images is not None and aux_image_idx.any():
    #         text_embeds[aux_image_idx] = aux_image_embeds.to(text_embeds.device).repeat(2,1,1).flatten(0,1)

    #     attention_mask = torch.cat([attention_mask, torch.ones_like(x_t[:, :, 0])], dim=1).int()
    #     position_ids = torch.cumsum(attention_mask, dim=1) - 1
    #     position_ids[position_ids < 0] = 0

    #     # Forward Context (LLM)
    #     outputs = self.model.language_model(
    #         inputs_embeds=text_embeds,
    #         attention_mask=attention_mask.bool(),
    #         position_ids=position_ids,
    #         output_hidden_states=True,
    #         return_dict=True,
    #         use_cache=True # Enable KV Cache for efficiency?
    #         # Note: Action Connector might not share KV cache structure easily if distinct models.
    #         # Pi0.5 uses cache for Prefix. Here Context is separate model.
    #         # We can cache the Context Output (Hidden States).
    #     )
    #     hidden_states = outputs.hidden_states[-1] # [BS, Seq, Hidden]

    #     if und_images is not None and und_image_idx.any():
    #         hidden_states[und_image_idx] = und_image_embeds.to(hidden_states.device).repeat(2,1,1).flatten(0,1)
    #     if aux_images is not None and aux_image_idx.any():
    #         hidden_states[aux_image_idx] = aux_image_embeds.to(hidden_states.device).repeat(2,1,1).flatten(0,1)

    #     # Pre-prepare Context inputs for Action DiT
    #     # Since Action DiT is just layers, we concat inputs every step.
    #     # Optimization: Pi0 uses KV cache for prefix.
    #     # Here, `action_dit` is a separate module instance.
    #     # We can treat `hidden_states` as the "Prefix Embeddings".

    #     # 2. Initialize Noise
    #     bsize = input_ids.shape[0]
    #     action_dim = self.model.config.action_dim
    #     noise = self.sample_noise((bsize, 1, action_dim), device=device)

    #     # 3. Euler Solver Loop

    #     dt = -1.0 / num_steps
    #     dt = torch.tensor(1.0, device=device, dtype=torch.float32)

    #     x_t = noise
    #     t = torch.tensor(1.0, dtype=torch.float32, device=device)
    #     while t >= -dt / 2: # Stop near 0
    #         # Embed Suffix
    #         expanded_time = t.expand(bsize)
    #         suffix_emb, adarms_cond = self.embed_action_suffix(
    #             x_t,
    #             expanded_time,
    #             llm_hidden_size=self.model.config.text_config.hidden_size,
    #             device=device,
    #             dtype=hidden_states.dtype
    #         )

    #         # Concat
    #         action_dit_inputs = torch.cat([hidden_states, suffix_emb], dim=1)

    #         # Masks & Pos IDs (Same as Forward)
    #         action_mask = torch.ones((bsize, 1), device=device, dtype=attention_mask.dtype)
    #         action_dit_att_mask = torch.cat([attention_mask, action_mask], dim=1)
    #         action_pos_id = position_ids.max(dim=1)[0].unsqueeze(1) + 1
    #         action_dit_pos_ids = torch.cat([position_ids, action_pos_id], dim=1)

    #         # Forward Connector
    #         action_out = self.model.action_dit(
    #             inputs_embeds=action_dit_inputs,
    #             attention_mask=action_dit_att_mask,
    #             position_ids=action_dit_pos_ids,
    #             output_hidden_states=True,
    #             # adarms_cond=[None, adarms_cond],
    #             # å’ŒPi05çš„ä¸åŒï¼šé™¤äº†æ²¡æœ‰ä½¿ç”¨adarms_condä¹‹å¤–ï¼Œaction_ditä¹Ÿæ²¡æœ‰ä½¿ç”¨full_att_2d_masks_4d
    #                 # Pi05çš„language_modelå’ŒDiTéƒ½ä½¿ç”¨äº†prefixåŒå‘ï¼Œsuffixå•å‘çš„maskï¼›
    #                 # è€ŒUniLIPçš„language_modelå’ŒDiTä½¿ç”¨äº†å•å‘maskï¼Œä½†æ˜¯ä¸­é—´çš„llm_connectorä½¿ç”¨äº†å•å‘maskï¼›
    #         )

    #         # Predict Velocity
    #         action_feat = action_out.hidden_states[-1][:, -1:, :]
    #         v_t = self.get_model().action_out_proj(action_feat)

    #         # Euler Step
    #         x_t = x_t + dt * v_t
    #         t += dt

    #     return x_t # Final Denoised Action [BS, 1, 5]



    # ==========================================
    # 4. [NEW] Inference: Generate Action (Flow Matching)
    # ==========================================
    @torch.no_grad()
    def generate_action2(
        self,
        input_ids: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None,
        und_image: Optional[torch.Tensor] = None,
        aux_image: Optional[torch.Tensor] = None,
        num_steps: int = 10,
        generator: Optional[torch.Generator] = None
    ):
        """
        Run inference for Localization task using Flow Matching Euler Solver.
        Adapts the logic from `forward` (Right-Shift & Fill) to ensure consistency.
        """

        # 1. Get Vision Features
        vision_feature_layer = self.config.vision_feature_layer
        vision_feature_select_strategy = self.config.vision_feature_select_strategy
        vision_dtype = self.model.vision_tower.embeddings.patch_embedding.weight.dtype

        und_image_embeds = None
        if und_image is not None:
            und_image_embeds = self.model.get_image_features(
                pixel_values=und_image.to(dtype=vision_dtype),
                vision_feature_layer=vision_feature_layer,
                vision_feature_select_strategy=vision_feature_select_strategy,
            )

        aux_image_embeds = None
        if aux_image is not None:
            aux_image_embeds = self.model.get_image_features(
                pixel_values=aux_image.to(dtype=vision_dtype),
                vision_feature_layer=vision_feature_layer,
                vision_feature_select_strategy=vision_feature_select_strategy,
            )

        # 2. Embed Text & Replace Image Tokens
        text_embeds = self.get_model().language_model.embed_tokens(input_ids)
        und_image_idx, aux_image_idx = split_image_tokens(input_ids, IMAGE_TOKEN_IDX)

        # 3. Replace Image Features
        if und_image is not None and und_image_idx.any():
            # Broadcast embeddings to batch size if needed (e.g. 1 image for all prompts)
            # Assuming standard [BS, C, H, W] input for simplicity based on collator
            # If batch sizes match, no repeat needed. If single image for batch, repeat.
            if und_image_embeds.shape[0] == 1 and text_embeds.shape[0] > 1:
                 und_image_embeds = und_image_embeds.repeat(text_embeds.shape[0], 1, 1)
            text_embeds[und_image_idx] = und_image_embeds.flatten(0,1)

        if aux_image is not None and aux_image_idx.any():
            if aux_image_embeds.shape[0] == 1 and text_embeds.shape[0] > 1:
                 aux_image_embeds = aux_image_embeds.repeat(text_embeds.shape[0], 1, 1)
            text_embeds[aux_image_idx] = aux_image_embeds.flatten(0,1)

        # 4. Prepare Context Position IDs
        position_ids = torch.cumsum(attention_mask, dim=1) - 1
        position_ids[position_ids < 0] = 0

        # 5. Forward Context (LLM)
        # We need the last hidden state as the "Context" for the Action Head
        outputs = self.model.language_model(
            inputs_embeds=text_embeds,
            attention_mask=attention_mask.bool(),
            position_ids=position_ids,
            output_hidden_states=True,
            return_dict=True,
            use_cache=False # Inference usually benefits from cache, but here we just need the last state once
        )
        # [BS, Seq, Hidden]
        hidden_states = outputs.hidden_states[-1]

        # Re-fill image embeddings (Skip Connection logic)
        # Important: indices must match flattened structure or batch structure
        if und_image_embeds is not None and und_image_idx.any():
             hidden_states[und_image_idx] = und_image_embeds.flatten(0,1)
        if aux_image_embeds is not None and aux_image_idx.any():
             hidden_states[aux_image_idx] = aux_image_embeds.flatten(0,1)

        # 6. action_dit_norm
        if not getattr(self.config, 'is_exp5_eval_without_aciton_dit_premodules', False):
            hidden_states = self.get_model().action_dit_norm(hidden_states)

        # 7. Initialize Flow Matching Loop
        bsize = input_ids.shape[0]
        action_dim = self.model.config.action_dim

        # Sample initial noise x_1
        noise = self.sample_noise((bsize, 1, action_dim), device=hidden_states.device)

        # dt tensor for calculation
        dt = -1.0 / num_steps
        dt = torch.tensor(dt, device=hidden_states.device, dtype=hidden_states.dtype)

        # Calculate Valid Lengths for Right-Shift Insertion
        valid_lens = attention_mask.sum(dim=1, keepdim=True).long() # [BS, 1]
        bs, seq_len, hidden_dim = hidden_states.shape

        # 8. Euler Solver Loop
        # Stop at t=0 (or close to it, Pi0 uses -dt/2 for safety)
        x_t = noise
        t = torch.tensor(1.0, device=hidden_states.device, dtype=hidden_states.dtype)
        while t >= -dt / 2:
            # 8.1. Embed Suffix (Noisy Action + Time)
            expanded_time = t.expand(bsize)
            suffix_emb, adarms_cond = self.embed_action_suffix(
                x_t,
                expanded_time,
                llm_hidden_size=self.model.config.text_config.hidden_size,
                device=hidden_states.device,
                dtype=hidden_states.dtype
            )

            # 8.2. Construct Action Inputs (Right Shift & Fill Strategy)
            # Reusing the robust logic from forward pass to avoid NaN

            # 8.2.1. Inputs: Concat Hidden + Last Token (Safe Padding)
            # last_token_states = hidden_states[:, -1:, :]
            # extended_inputs = torch.cat([hidden_states, last_token_states], dim=1)
            # 8.2.1. Inputs: Concat Hidden + Zero Token
            extended_inputs = torch.cat([hidden_states, torch.zeros_like(suffix_emb)], dim=1)
            # Scatter Suffix to valid positions
            scatter_indices = valid_lens.unsqueeze(-1).expand(-1, -1, hidden_dim)
            action_dit_inputs = extended_inputs.scatter(1, scatter_indices, suffix_emb)

            # 8.2.2. Mask: Extend and Set True at Action Position
            extended_mask = torch.cat([
                attention_mask,
                torch.zeros((bs, 1), device=hidden_states.device, dtype=attention_mask.dtype)
            ], dim=1)
            mask_indices = valid_lens.view(-1, 1)
            action_dit_att_mask = extended_mask.scatter(1, mask_indices, 1)

            # 8.2.3. Position IDs: Extend and Set to Valid Length Index
            extended_pos_ids = torch.cat([
                position_ids,
                torch.zeros((bs, 1), device=hidden_states.device, dtype=position_ids.dtype)
            ], dim=1)
            action_pos_ids = valid_lens.view(-1, 1)
            if getattr(self.config, 'is_exp5_eval_without_aciton_dit_premodules', False):
                action_dit_pos_ids = extended_pos_ids.scatter(1, mask_indices, action_pos_ids)
            else:
                current_seq_len = extended_pos_ids.shape[1]
                range_ids = torch.arange(current_seq_len, device=position_ids.device).unsqueeze(0) # [1, Seq+1]
                # ç”Ÿæˆæ©ç ï¼šå¦‚æœå½“å‰ä½ç½® index >= valid_lensï¼Œåˆ™ä¸º True
                # [BS, 1] vs [1, Seq+1] -> Broadcast -> [BS, Seq+1]
                mask_after_valid = range_ids >= valid_lens.view(-1, 1)
                # ä½¿ç”¨ torch.where è¿›è¡Œæ‰¹é‡å¡«å……,ä¿è¯pos_idsåœ¨valid_lensä»¥åçš„ä½ç½®ç»§æ‰¿action_pos_idsçš„å€¼ä½œä¸ºpadding_pos_ids
                # é€»è¾‘ï¼šMask ä¸º True çš„åœ°æ–¹å¡«å…¥ action_pos_idï¼ŒFalse çš„åœ°æ–¹ä¿æŒåŸæ ·
                extended_pos_ids = torch.where(
                mask_after_valid,
                action_pos_ids,      # å¹¿æ’­å¡«å…… [BS, 1] -> [BS, MaskåŒºåŸŸ]
                extended_pos_ids  # ä¿æŒåŸå€¼
            )

            # 8.3 action_dit_projector
            if not getattr(self.config, 'is_exp5_eval_without_aciton_dit_premodules', False):
                if getattr(self.config, 'is_action_dit_projector', False):
                    action_dit_inputs = self.get_model().action_dit_projector(action_dit_inputs)

            # 8.4. Forward Action DiT
            if getattr(self.config, 'is_action_dit_dense_timestep', False):
                # Use custom forward loop with AdaRMS
                action_outputs = self.action_dit_forward_with_adarmscond(
                    hidden_states=action_dit_inputs,
                    attention_mask=action_dit_att_mask,
                    position_ids=extended_pos_ids,
                    adarms_cond=adarms_cond
                )
                # Note: custom forward returns hidden_states directly
                all_hidden_states = action_outputs
            else:
                # Use standard model forward
                action_outputs = self.model.action_dit(
                    inputs_embeds=action_dit_inputs,
                    attention_mask=action_dit_att_mask,
                    position_ids=extended_pos_ids,
                    output_hidden_states=True,
                    return_dict=True,
                    use_cache=False
                )
                all_hidden_states = action_outputs.hidden_states[-1]

            # 8.5. Extract Action Token Output
            # Gather from the same indices we scattered to
            gather_indices = valid_lens.unsqueeze(-1).expand(-1, -1, hidden_dim)
            action_feat = all_hidden_states.gather(1, gather_indices) # [BS, 1, Hidden]

            # 8.6. Predict Velocity & Step
            v_t = self.get_model().action_out_proj(action_feat)
            x_t = x_t + dt * v_t

            t += dt

        return x_t # Final Denoised Action [BS, 1, 5]



AutoConfig.register("unified_unilip", Unified_UniLIP_InternVLConfig)
AutoModelForCausalLM.register(Unified_UniLIP_InternVLConfig, Unified_UniLIP_InternVLForCausalLM)