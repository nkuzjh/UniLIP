import matplotlib.pyplot as plt
import numpy as np
import torch
import os

def visualize_dataset_samples(dataset, processor, num_samples=20, save_path="_debug_dataset_samples.jpg", is_multi_task=False):
    """
    ç›´æ¥å¯¹å·²åŠ è½½çš„ Dataset è¿›è¡ŒæŠ½æ ·å¯è§†åŒ–ï¼ŒéªŒè¯ Radar(Input) -> FPS(Target) åŠ Prompt å¯¹é½æƒ…å†µã€‚

    Args:
        dataset: å·²ç»åˆå§‹åŒ–å¥½çš„ CSGOWorldModelDataset å®ä¾‹
        processor: å¯¹åº”çš„ ImageProcessor (ç”¨äºè·å– mean/std è¿›è¡Œé€†å½’ä¸€åŒ–)
        num_samples: æŠ½æ ·æ•°é‡
        save_path: å›¾ç‰‡ä¿å­˜è·¯å¾„
    """
    print(f"ğŸ‘€ Visualizing first {num_samples} samples from dataset...")

    # è·å–åå½’ä¸€åŒ–æ‰€éœ€çš„å‡å€¼å’Œæ–¹å·®
    # å¦‚æœ processor æ˜¯ AutoProcessorï¼Œé€šå¸¸åœ¨ processor.image_processor ä¸­
    # å¦‚æœä¼ å…¥çš„æ˜¯ image_processor ç›´æ¥ä½¿ç”¨å³å¯
    img_processor = getattr(processor, "image_processor", processor)
    mean = np.array(img_processor.image_mean)
    std = np.array(img_processor.image_std)

    if is_multi_task:
        fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))
    else:
        fig, axes = plt.subplots(num_samples, 2, figsize=(10, 4 * num_samples))
    plt.subplots_adjust(hspace=0.4, wspace=0.1)

    # å…¼å®¹ num_samples=1 çš„æƒ…å†µ
    if num_samples == 1: axes = np.array([axes])

    for i in range(num_samples):
        # 1. è·å–æ•°æ® (__getitem__)
        sample = dataset[i]

        map_name = sample['map_name']
        min_z = dataset.map_z_range[map_name]['min_z']
        max_z = dataset.map_z_range[map_name]['max_z']
        fps_img_name = sample['ids']
        # 2. æå–å›¾åƒ Tensor [1, C, H, W] -> Squeeze -> [C, H, W]
        # æ³¨æ„: dataset è¿”å›çš„æ˜¯ 'und_image' (Radar) å’Œ 'gen_image' (FPS)
        radar_tensor = sample['und_image'].squeeze(0)
        aux_tensor = sample['aux_image'].squeeze(0) if sample.get('aux_image') is not None else None
        fps_tensor = sample['gen_image'].squeeze(0)
        loc_coords = sample['loc_coords'] if sample.get('loc_coords') is not None else sample['actions'][0]
        x,y,z,v,h = loc_coords[0],loc_coords[1],loc_coords[2],loc_coords[3],loc_coords[4]
        x=x*1024
        y=y*1024
        z=z*(max_z-min_z)+min_z
        v=v* 360
        h=h* 360

        # 3. é€†å½’ä¸€åŒ– (Tensor -> Numpy Image)
        def denorm(tensor):
            img = tensor.permute(1, 2, 0).cpu().numpy() # [H, W, C]
            img = img * std + mean # åå½’ä¸€åŒ–
            return np.clip(img, 0, 1)

        img_radar = denorm(radar_tensor)
        img_aux = denorm(aux_tensor) if aux_tensor is not None else None
        img_fps = denorm(fps_tensor)

        # 4. è§£ç æ–‡æœ¬ Prompt (éªŒè¯åæ ‡æ˜¯å¦æ­£ç¡®)
        # sample['input_ids'] æ˜¯ Tensorï¼Œéœ€è¦è½¬å› list æ‰èƒ½ decode
        input_ids = sample['input_ids']
        if isinstance(input_ids, torch.Tensor):
            input_ids = input_ids.tolist()

        # ä½¿ç”¨ dataset ä¸­çš„ tokenizer è¿›è¡Œè§£ç 
        decoded_text = dataset.tokenizer.decode(input_ids, skip_special_tokens=False)

        # æå–å…³é”®ä¿¡æ¯ç”¨äºæ ‡é¢˜ (æˆªå– Prompt ä¸­çš„ Pose éƒ¨åˆ†)
        try:
            # ç®€å•æå– Pose(...) éƒ¨åˆ†ï¼Œé¿å…æ ‡é¢˜å¤ªé•¿
            pose_str = decoded_text.split("Current Camera Pose:")[-1].split("<img>")[0].strip()
            # å¦‚æœå¤ªé•¿ï¼Œæ¢è¡Œæ˜¾ç¤º
            if len(pose_str) > 50:
                # pose_str = pose_str[:50] + "\n" + pose_str[50:]
                # pose_str = '\n'.join([pose_str[i:i+50] for i in range(0, len(pose_str), 50)])
                import textwrap
                pose_str = textwrap.fill(pose_str, width=50)
        except:
            pose_str = "Prompt parsing failed"

        # 5. ç»˜å›¾
        # å·¦ä¾§: Radar Map
        axes[i, 0].imshow(img_radar)
        axes[i, 0].set_title(f"Sample {i} | Input: Radar, \npose {pose_str}", fontsize=10, fontweight='bold')
        axes[i, 0].axis('off')

        if img_aux is not None:
            axes[i, 1].imshow(img_aux)
            axes[i, 1].set_title(f"Auxilliary for \nMulti-Task Training", fontsize=9, color='green')
            axes[i, 1].axis('off')

        # å³ä¾§: FPS View
        axes[i, 2].imshow(img_fps)
        axes[i, 2].set_title(f"Target: FPS {fps_img_name}, \ngt_pose{x,y,z,v,h}", fontsize=9, color='darkblue')
        axes[i, 2].axis('off')



    # ä¿å­˜
    plt.savefig(save_path, bbox_inches='tight', dpi=120)
    print(f"âœ¨ Visualization saved to: {os.path.abspath(save_path)}")
    plt.close()


### MultiTaskDataset:
# sample.keys()
# dict_keys(['task_id', 'und_image', 'aux_image', 'gen_image', 'input_ids', 'labels', 'raw_prompt', 'actions', 'loss_mask', 'map_id', 'map_name', 'pose_dict'])



# print(train_dataset[0].keys())
# dict_keys(['input_ids', 'labels', 'und_image', 'gen_image', 'ids', 'loc_coords'])

# print(train_dataset[0]['input_ids'].shape)
# torch.Size([423])

# print(train_dataset[0]['labels'].shape)
# torch.Size([423])

# print(train_dataset[0]['und_image'].shape)
# torch.Size([1, 3, 448, 448])

# print(train_dataset[0]['gen_image'].shape)
# torch.Size([1, 3, 448, 448])

# print(train_dataset[0]['ids'])
# file_num159_frame_191

# loc_array = np.array([data['x'] / 1024, data['y'] / 1024, z_norm, pitch_deg, yaw_deg])
# loc_coords = torch.tensor(loc_array, dtype=torch.float32)




def visualize_dataset_samples_paired(dataset, processor, num_samples=5, save_path="_debug_dataset_paired.jpg"):
    """
    å¯è§†åŒ–æˆå¯¹é‡‡æ · (Pair-wise Sampling) çš„æ•°æ®é›†ã€‚
    éªŒè¯åŒä¸€ä¸ª Index è¿”å›çš„ Loc å’Œ Gen ä»»åŠ¡æ•°æ®æ˜¯å¦å…±äº«åŒä¸€åœºæ™¯ï¼Œä½†è¾“å…¥/è¾“å‡ºäº’é€†ã€‚

    Args:
        dataset: è¿”å› [sample_loc, sample_gen] çš„ Dataset
        processor: ImageProcessor (ç”¨äºåå½’ä¸€åŒ–)
    """
    print(f"ğŸ‘€ Visualizing first {num_samples} pairs from dataset...")

    # 1. è·å–åå½’ä¸€åŒ–å‚æ•°
    img_processor = getattr(processor, "image_processor", processor)
    mean = np.array(img_processor.image_mean)
    std = np.array(img_processor.image_std)

    def denorm(tensor):
        """Tensor [C, H, W] -> Numpy [H, W, C] (0-1)"""
        if tensor is None: return np.zeros((448, 448, 3))
        # å¤„ç†å¯èƒ½å­˜åœ¨çš„ Batch ç»´åº¦ [1, C, H, W]
        if tensor.dim() == 4: tensor = tensor.squeeze(0)

        img = tensor.permute(1, 2, 0).cpu().numpy()
        img = img * std + mean
        return np.clip(img, 0, 1)

    # 2. è®¾ç½®ç”»å¸ƒ: æ¯è¡Œ 3 åˆ— (Map, FPS, Info Panel)
    fig, axes = plt.subplots(num_samples, 3, figsize=(18, 6 * num_samples))
    plt.subplots_adjust(hspace=0.3, wspace=0.1)
    if num_samples == 1: axes = axes.reshape(1, -1)

    for i in range(num_samples):
        # --- A. è·å–æˆå¯¹æ•°æ® ---
        # dataset[i] ç°åœ¨è¿”å›çš„æ˜¯ä¸€ä¸ª list: [sample_loc, sample_gen]
        pair_data = dataset[i]
        sample_loc = pair_data[0]
        sample_gen = pair_data[1]

        # æ ¡éªŒ Task ID
        assert sample_loc['task_id'] == 0, f"Index 0 should be Loc, got {sample_loc['task_id']}"
        assert sample_gen['task_id'] == 1, f"Index 1 should be Gen, got {sample_gen['task_id']}"

        # --- B. æå–å›¾åƒ ---
        # é€»è¾‘éªŒè¯ï¼š
        # Loc Task: Input(Und)=FPS, Aux=Map
        # Gen Task: Input(Und)=Map, Target(Gen)=FPS

        # æˆ‘ä»¬ä» Gen ä»»åŠ¡å– Map (Und)ï¼Œä» Loc ä»»åŠ¡å– FPS (Und)
        # è¿™æ ·èƒ½åŒæ—¶éªŒè¯ä¸¤ä¸ªä»»åŠ¡çš„ Input ä¹Ÿæ˜¯æ­£ç¡®çš„
        tensor_map = sample_gen['und_image']
        tensor_fps = sample_loc['und_image']

        # ä¹Ÿå¯ä»¥éªŒè¯ Loc çš„ Aux æ˜¯å¦ç­‰äº Map
        tensor_map_aux = sample_loc['aux_image']

        img_map = denorm(tensor_map)
        img_fps = denorm(tensor_fps)
        img_map_aux = denorm(tensor_map_aux)

        # --- C. æå– Meta ä¿¡æ¯ ---
        map_name = sample_loc['map_name']
        pose = sample_loc['pose_dict']

        # è§£ç  Prompt
        input_ids_loc = sample_loc['input_ids']
        if isinstance(input_ids_loc, torch.Tensor): input_ids_loc = input_ids_loc.tolist()
        text_loc = dataset.tokenizer.decode(input_ids_loc, skip_special_tokens=False)

        input_ids_gen = sample_gen['input_ids']
        if isinstance(input_ids_gen, torch.Tensor): input_ids_gen = input_ids_gen.tolist()
        text_gen = dataset.tokenizer.decode(input_ids_gen, skip_special_tokens=False)

        # --- D. ç»˜å›¾ ---

        # Column 1: Radar Map (Gen Input / Loc Aux)
        # ä¸ºäº†å±•ç¤º Aux æ˜¯å¦æ­£ç¡®ï¼Œæˆ‘ä»¬åœ¨ Map ä¸Šå åŠ ä¸€ä¸ªå°å›¾æˆ–è€…æ ‡é¢˜è¯´æ˜
        ax_map = axes[i, 0]
        ax_map.imshow(img_map)
        diff = np.mean(np.abs(img_map - img_map_aux)) # éªŒè¯ä¸€è‡´æ€§
        ax_map.set_title(f"Map '{map_name}'\nGen Input / Loc Aux\n(Consistency Diff: {diff:.1e})", fontsize=11, fontweight='bold')
        ax_map.axis('off')

        # Column 2: FPS View (Loc Input / Gen Target)
        ax_fps = axes[i, 1]
        ax_fps.imshow(img_fps)
        ax_fps.set_title(f"FPS View\nLoc Input / Gen Target\nID: {sample_loc['ids']}", fontsize=11, fontweight='bold')
        ax_fps.axis('off')

        # Column 3: Info Panel (Text & Pose)
        ax_text = axes[i, 2]
        ax_text.axis('off')

        # æ„é€ ä¿¡æ¯æ–‡æœ¬
        pose_str = f"x={pose['x']:.1f}, y={pose['y']:.1f}, z={pose['z']:.2f}\npitch={pose['angle_v']:.1f}, yaw={pose['angle_h']:.1f}"

        # æˆªå– Prompt å…³é”®éƒ¨åˆ†
        prompt_loc_short = text_loc.split("Task:")[-1].split("Predict")[0].strip()[:100] + "..."
        prompt_gen_short = text_gen.split("Task:")[-1].split("Coordinate")[0].strip()[:100] + "..."

        info_text = (
            f"Sample Pair Index: {i}\n"
            f"--------------------------------\n"
            f"Map: {map_name}\n"
            f"GT Pose:\n{pose_str}\n"
            f"--------------------------------\n"
            f"[Task Loc] Raw Prompt Snippet:\n...{textwrap.fill(prompt_loc_short, 35)}\n"
            f"Loss Mask: {sample_loc['loss_mask'].tolist()}\n"
            f"--------------------------------\n"
            f"[Task Gen] Raw Prompt Snippet:\n...{textwrap.fill(prompt_gen_short, 35)}\n"
            f"Loss Mask: {sample_gen['loss_mask'].tolist()}\n"
        )

        ax_text.text(0, 0.95, info_text, fontsize=10, verticalalignment='top', fontfamily='monospace',
                     bbox=dict(boxstyle="round,pad=0.5", fc="#f9f9f9", ec="gray", alpha=0.5))

    # ä¿å­˜
    plt.savefig(save_path, bbox_inches='tight', dpi=100)
    print(f"âœ¨ Paired visualization saved to: {os.path.abspath(save_path)}")
    plt.close()