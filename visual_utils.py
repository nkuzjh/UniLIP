import matplotlib.pyplot as plt
import numpy as np
import torch
import os

def visualize_dataset_samples(dataset, processor, num_samples=20, save_path="_debug_dataset_samples.jpg", is_multi_task=False):
    """
    ç›´æ¥å¯¹å·²åŠ è½½çš„ Dataset è¿›è¡ŒæŠ½æ ·å¯è§†åŒ–ï¼ŒéªŒè¯ Radar(Input) -> FPS(Target) åŠ Prompt å¯¹é½æƒ…å†µã€‚

    Args:
        dataset: å·²ç»åˆå§‹åŒ–å¥½çš„ CSGOWorldModelDataset å®ä¾‹
        processor: å¯¹åº”çš„ ImageProcessor (ç”¨äºè·å– mean/std è¿›è¡Œé€†å½’ä¸€åŒ–)
        num_samples: æŠ½æ ·æ•°é‡
        save_path: å›¾ç‰‡ä¿å­˜è·¯å¾„
    """
    print(f"ğŸ‘€ Visualizing first {num_samples} samples from dataset...")

    # è·å–åå½’ä¸€åŒ–æ‰€éœ€çš„å‡å€¼å’Œæ–¹å·®
    # å¦‚æœ processor æ˜¯ AutoProcessorï¼Œé€šå¸¸åœ¨ processor.image_processor ä¸­
    # å¦‚æœä¼ å…¥çš„æ˜¯ image_processor ç›´æ¥ä½¿ç”¨å³å¯
    img_processor = getattr(processor, "image_processor", processor)
    mean = np.array(img_processor.image_mean)
    std = np.array(img_processor.image_std)

    if is_multi_task:
        fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))
    else:
        fig, axes = plt.subplots(num_samples, 2, figsize=(10, 4 * num_samples))
    plt.subplots_adjust(hspace=0.4, wspace=0.1)

    # å…¼å®¹ num_samples=1 çš„æƒ…å†µ
    if num_samples == 1: axes = np.array([axes])

    for i in range(num_samples):
        # 1. è·å–æ•°æ® (__getitem__)
        sample = dataset[i]

        map_name = sample['map_name']
        min_z = dataset.map_z_range[map_name]['min_z']
        max_z = dataset.map_z_range[map_name]['max_z']
        fps_img_name = sample['ids']
        # 2. æå–å›¾åƒ Tensor [1, C, H, W] -> Squeeze -> [C, H, W]
        # æ³¨æ„: dataset è¿”å›çš„æ˜¯ 'und_image' (Radar) å’Œ 'gen_image' (FPS)
        radar_tensor = sample['und_image'].squeeze(0)
        aux_tensor = sample['aux_image'].squeeze(0) if sample.get('aux_image') is not None else None
        fps_tensor = sample['gen_image'].squeeze(0)
        loc_coords = sample['loc_coords'] if sample.get('loc_coords') is not None else sample['actions'][0]
        x,y,z,v,h = loc_coords[0],loc_coords[1],loc_coords[2],loc_coords[3],loc_coords[4]
        x=x*1024
        y=y*1024
        z=z*(max_z-min_z)+min_z
        v=v* 360
        h=h* 360

        # 3. é€†å½’ä¸€åŒ– (Tensor -> Numpy Image)
        def denorm(tensor):
            img = tensor.permute(1, 2, 0).cpu().numpy() # [H, W, C]
            img = img * std + mean # åå½’ä¸€åŒ–
            return np.clip(img, 0, 1)

        img_radar = denorm(radar_tensor)
        img_aux = denorm(aux_tensor) if aux_tensor is not None else None
        img_fps = denorm(fps_tensor)

        # 4. è§£ç æ–‡æœ¬ Prompt (éªŒè¯åæ ‡æ˜¯å¦æ­£ç¡®)
        # sample['input_ids'] æ˜¯ Tensorï¼Œéœ€è¦è½¬å› list æ‰èƒ½ decode
        input_ids = sample['input_ids']
        if isinstance(input_ids, torch.Tensor):
            input_ids = input_ids.tolist()

        # ä½¿ç”¨ dataset ä¸­çš„ tokenizer è¿›è¡Œè§£ç 
        decoded_text = dataset.tokenizer.decode(input_ids, skip_special_tokens=False)

        # æå–å…³é”®ä¿¡æ¯ç”¨äºæ ‡é¢˜ (æˆªå– Prompt ä¸­çš„ Pose éƒ¨åˆ†)
        try:
            # ç®€å•æå– Pose(...) éƒ¨åˆ†ï¼Œé¿å…æ ‡é¢˜å¤ªé•¿
            pose_str = decoded_text.split("Current Camera Pose:")[-1].split("<img>")[0].strip()
            # å¦‚æœå¤ªé•¿ï¼Œæ¢è¡Œæ˜¾ç¤º
            if len(pose_str) > 50:
                # pose_str = pose_str[:50] + "\n" + pose_str[50:]
                # pose_str = '\n'.join([pose_str[i:i+50] for i in range(0, len(pose_str), 50)])
                import textwrap
                pose_str = textwrap.fill(pose_str, width=50)
        except:
            pose_str = "Prompt parsing failed"

        # 5. ç»˜å›¾
        # å·¦ä¾§: Radar Map
        axes[i, 0].imshow(img_radar)
        axes[i, 0].set_title(f"Sample {i} | Input: Radar, \npose {pose_str}", fontsize=10, fontweight='bold')
        axes[i, 0].axis('off')

        # å³ä¾§: FPS View
        axes[i, 1].imshow(img_fps)
        axes[i, 1].set_title(f"Target: FPS {fps_img_name}, \ngt_pose{x,y,z,v,h}", fontsize=9, color='darkblue')
        axes[i, 1].axis('off')

        if img_aux is not None:
            axes[i, 2].imshow(img_aux)
            axes[i, 2].set_title(f"Auxilliary for Multi-Task Training", fontsize=9, color='green')
            axes[i, 2].axis('off')

    # ä¿å­˜
    plt.savefig(save_path, bbox_inches='tight', dpi=120)
    print(f"âœ¨ Visualization saved to: {os.path.abspath(save_path)}")
    plt.close()


### MultiTaskDataset:
# sample.keys()
# dict_keys(['task_id', 'und_image', 'aux_image', 'gen_image', 'input_ids', 'labels', 'raw_prompt', 'actions', 'loss_mask', 'map_id', 'map_name', 'pose_dict'])



# print(train_dataset[0].keys())
# dict_keys(['input_ids', 'labels', 'und_image', 'gen_image', 'ids', 'loc_coords'])

# print(train_dataset[0]['input_ids'].shape)
# torch.Size([423])

# print(train_dataset[0]['labels'].shape)
# torch.Size([423])

# print(train_dataset[0]['und_image'].shape)
# torch.Size([1, 3, 448, 448])

# print(train_dataset[0]['gen_image'].shape)
# torch.Size([1, 3, 448, 448])

# print(train_dataset[0]['ids'])
# file_num159_frame_191

# loc_array = np.array([data['x'] / 1024, data['y'] / 1024, z_norm, pitch_deg, yaw_deg])
# loc_coords = torch.tensor(loc_array, dtype=torch.float32)

