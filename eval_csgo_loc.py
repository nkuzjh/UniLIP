import argparse
import os
import json
import torch
import torch.nn.functional as F
import numpy as np
import yaml
import random
import datetime
from PIL import Image, ImageDraw
from tqdm import tqdm
from torch.utils.data import DataLoader, Dataset
from transformers import AutoProcessor
from safetensors.torch import load_file as safe_load_file
from types import SimpleNamespace
from collections import defaultdict

# å¼•å…¥ UniLIP æ ¸å¿ƒæ¨¡å—
from unilip.utils import disable_torch_init
from unilip.model import Unified_UniLIP_InternVLForCausalLM

# ==========================================
# 0. åŸºç¡€å·¥å…· & Pi0.5 å¤ç”¨å‡½æ•°
# ==========================================
def set_seed(seed=42):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    print(f"éšæœºç§å­å·²è®¾ç½®ä¸º: {seed}")

def vertical_concat(images):
    """å¤ç”¨ Pi0.5: å°†å¤šå¼  PIL Image çºµå‘æ‹¼æ¥"""
    if not images: return None
    width = images[0].width
    total_height = sum(img.height for img in images)
    new_image = Image.new(images[0].mode, (width, total_height))
    y_offset = 0
    for img in images:
        if img.width != width:
            img = img.resize((width, img.height), Image.LANCZOS)
        new_image.paste(img, (0, y_offset))
        y_offset += img.height
    return new_image

def concat_images_horizontal_resize(img1: Image.Image, img2: Image.Image) -> Image.Image:
    """å¤ç”¨ Pi0.5: æ¨ªå‘æ‹¼æ¥ï¼Œå¼ºåˆ¶ img2 é«˜åº¦é€‚é… img1"""
    if img1 is None or img2 is None: return img1 if img1 else img2
    w1, h1 = img1.size
    w2, h2 = img2.size
    aspect_ratio = h1 / h2
    new_w2 = int(w2 * aspect_ratio)
    new_h2 = h1
    img2_resized = img2.resize((new_w2, new_h2), Image.Resampling.LANCZOS)
    total_width = w1 + new_w2
    dst = Image.new(img1.mode, (total_width, h1))
    dst.paste(img1, (0, 0))
    dst.paste(img2_resized, (w1, 0))
    return dst


# ==========================================
# 1. æ ¸å¿ƒé€»è¾‘: Metric è®¡ç®— (å‡çº§ç‰ˆ)
# ==========================================
def calculate_metrics(results):
    """
    è®¡ç®—å®šä½ä»»åŠ¡çš„æ ¸å¿ƒæŒ‡æ ‡ (åŒ…å« L2 Norm, SmoothL1, MSE)
    results: List[Dict], åŒ…å« 'gt', 'pred' (å·²ç»æ˜¯åå½’ä¸€åŒ–åçš„ç‰©ç†åæ ‡)
    """
    gt_norm = torch.tensor([item['gt_norm'] for item in results], dtype=torch.float32)
    pred_norm = torch.tensor([item['pred_norm'] for item in results], dtype=torch.float32)

    # --- 1. è®¡ç®—ç»å¯¹å·®å€¼ (Abs Diff) ---
    # abs_diff = |pred - gt|
    norm_abs_diff = torch.abs(pred_norm - gt_norm)

    # --- 2. å¤„ç† Yaw (Index 4) çš„å‘¨æœŸæ€§ ---
    # åœ¨ 0~1 ç©ºé—´ä¸­ï¼Œå‘¨æœŸæ˜¯ 1.0
    # ä¿®æ­£åçš„è¯¯å·® = min(raw_diff, 1.0 - raw_diff)
    yaw_diff = norm_abs_diff[:, 4]
    yaw_diff_wrapped = torch.min(yaw_diff, 1.0 - yaw_diff)
    norm_abs_diff[:, 4] = yaw_diff_wrapped # æ›´æ–° Diff Tensor

    norm_l2_xy = torch.norm(norm_abs_diff[:, :2], p=2, dim=1).mean().item()
    # 2. 5D L2 Norm (Normalized Space)
    # sqrt(dx^2 + dy^2 + dz^2 + dpitch^2 + dyaw^2)
    norm_l2_5d = torch.norm(norm_abs_diff, p=2, dim=1).mean().item()

    # ==========================================
    # Metric B: Losses (MSE & SmoothL1)
    # ==========================================
    zeros_target = torch.zeros_like(norm_abs_diff)
    # 1. MSE Loss (5D)
    # mean(error^2)
    norm_mse_loss_5d = F.mse_loss(norm_abs_diff, zeros_target, reduction='mean').item()
    # 2. SmoothL1 Loss (5D)
    # è¿™é‡Œçš„ beta=1.0 æ˜¯é»˜è®¤å€¼ï¼Œå¯æ ¹æ®è®­ç»ƒæ—¶çš„è®¾å®šè°ƒæ•´
    norm_smooth_l1_loss_5d = F.smooth_l1_loss(norm_abs_diff, zeros_target, reduction='mean', beta=1.0).item()

    # 1. æå–æ•°æ®å¹¶è½¬æ¢ä¸º Tensor ä»¥åˆ©ç”¨ PyTorch çš„ Loss å‡½æ•°
    gt_list = []
    pred_list = []
    for item in results:
        gt = item['gt']
        pred = item['pred']
        # æ„é€ å‘é‡ [x, y, z, pitch, yaw]
        gt_vec = [gt['x'], gt['y'], gt['z'], gt['angle_v'], gt['angle_h']]
        pred_vec = [pred['x'], pred['y'], pred['z'], pred['angle_v'], pred['angle_h']]
        gt_list.append(gt_vec)
        pred_list.append(pred_vec)

    gt_tensor = torch.tensor(gt_list, dtype=torch.float32)     # [N, 5]
    pred_tensor = torch.tensor(pred_list, dtype=torch.float32) # [N, 5]

    # --- é¢„å¤„ç†: å¤„ç† Yaw (Index 4) çš„ 360 åº¦å‘¨æœŸæ€§ ---
    # æˆ‘ä»¬è®¡ç®—å·®å€¼ diffï¼Œè€Œä¸æ˜¯ç›´æ¥ç”¨ predï¼Œè¿™æ · Loss è®¡ç®—æ‰ç¬¦åˆç‰©ç†ç›´è§‰
    # diff = pred - gt
    diff_tensor = pred_tensor - gt_tensor
    abs_diff = torch.abs(diff_tensor)

    # å¯¹ Yaw (ç¬¬4åˆ—) åš wrap å¤„ç†: diff = (diff + 180) % 360 - 180
    # æˆ–è€…æ›´ç®€å•çš„ min(|d|, 360-|d|) é€»è¾‘ï¼Œä½†ä¸ºäº†ä¿ç•™ç¬¦å·ç»™ Loss ç”¨ï¼Œé€šå¸¸å–æœ€å°å¤¹è§’
    # è¿™é‡Œä¸ºäº† L2/MSE/SmoothL1 è®¡ç®—è·ç¦»ï¼ˆå³è¯¯å·®å¤§å°ï¼‰ï¼Œæˆ‘ä»¬å–ç»å¯¹è¯¯å·®
    # ä¿®æ­£ Yaw çš„ç»å¯¹è¯¯å·®: min(err, 360 - err)
    yaw_err = abs_diff[:, 4]
    yaw_err = torch.min(yaw_err, 360.0 - yaw_err)
    abs_diff[:, 4] = yaw_err

    # --- Metric 1: å•é¡¹è¯¯å·® ---
    xy_dist = torch.sqrt(abs_diff[:, 0]**2 + abs_diff[:, 1]**2).mean().item()
    z_dist = abs_diff[:, 2].mean().item()
    pitch_dist = abs_diff[:, 3].mean().item()
    yaw_dist = abs_diff[:, 4].mean().item()

    # --- Metric 2: XY L2 Norm (åŒ Mean XY Error) ---
    l2_xy = torch.norm(abs_diff[:, :2], p=2, dim=1).mean().item()
    # --- Metric 3: 5D L2 Norm ---
    # å®šä¹‰ï¼šsqrt(dx^2 + dy^2 + dz^2 + dp^2 + dyaw^2)
    l2_5d = torch.norm(abs_diff, p=2, dim=1).mean().item()
    # --- Metric 4: 5D Loss (MSE & SmoothL1) ---
    # è¿™é‡Œçš„ Loss æ˜¯åŸºäºç‰©ç†åæ ‡çš„ï¼Œæ‰€ä»¥æ•°å€¼ä¼šå¾ˆå¤§ï¼Œä½†èƒ½åæ˜ ç‰©ç†åç¦»ç¨‹åº¦
    # ä½¿ç”¨ abs_diff ä½œä¸ºè¾“å…¥ï¼Œä¹Ÿå°±æ˜¯è®¡ç®— Loss(abs_diff, 0)
    mse_loss_5d = F.mse_loss(abs_diff, torch.zeros_like(abs_diff))
    smooth_l1_loss_5d = F.smooth_l1_loss(abs_diff, torch.zeros_like(abs_diff), beta=1.0)

    metrics = {
        "Norm_L2_XY": norm_l2_xy,
        "Norm_L2_5D": norm_l2_5d,
        "Norm_MSE_5D": norm_mse_loss_5d,
        "Norm_SmoothL1_5D": norm_smooth_l1_loss_5d,

        "XY_Dist": xy_dist,
        "Z_Dist": z_dist,
        "Pitch_Dist": pitch_dist,
        "Yaw_Dist": yaw_dist,

        # é¡ºä¾¿ä¿ç•™å•é¡¹çš„å¹³å‡ç»å¯¹è¯¯å·® (L1)
        "Norm_L1_X": abs_diff[:, 0].mean().item(),
        "Norm_L1_Y": abs_diff[:, 1].mean().item(),
        "Norm_L1_Z": abs_diff[:, 2].mean().item(),
        "Norm_L1_Pitch": abs_diff[:, 3].mean().item(),
        "Norm_L1_Yaw": abs_diff[:, 4].mean().item(),

        "L2_XY": l2_xy,       # å®é™…ä¸Šç­‰äº XY_Dist
        "L2_5D": l2_5d,
        "MSE_Loss_5D": mse_loss_5d.item(),
        "SmoothL1_Loss_5D": smooth_l1_loss_5d.item(),
    }
    return metrics

# ==========================================
# 2. å¯è§†åŒ–é€»è¾‘ (Pi0.5 é£æ ¼)
# ==========================================
def visualize_map_results(vis_data_grouped, output_dir, map_path_dict, data_dir_base, vis_num_per_map=5):
    """
    æŒ‰åœ°å›¾åˆ†ç»„å¯è§†åŒ–ï¼šå·¦ä¾§ Radar å åŠ  GT/Pred ç‚¹ï¼Œå³ä¾§ FPS åˆ—è¡¨
    """
    print(f"ğŸ“Š Generating Pi0.5 Style Visualizations for {len(vis_data_grouped)} maps...")

    # é¢œè‰²åˆ—è¡¨ (ç”¨äºåŒºåˆ†ä¸åŒæ ·æœ¬)
    color_list = [
        (255, 0, 0, 180),   # Red
        (0, 255, 0, 180),   # Green
        (0, 0, 255, 180),   # Blue
        (255, 255, 0, 180), # Yellow
        (255, 0, 255, 180)  # Magenta
    ]

    for map_name, items in vis_data_grouped.items():
        # é™åˆ¶æ¯å¼ å›¾æ˜¾ç¤ºçš„æ ·æœ¬æ•°
        items_to_show = items[:vis_num_per_map]

        # 1. åŠ è½½åº•å›¾ (Radar)
        map_filename = map_path_dict.get(map_name, 'de_dust2_radar_psd.png')
        map_path = os.path.join(data_dir_base, map_name, map_filename)
        if not os.path.exists(map_path):
            print(f"âš ï¸ Map image not found: {map_path}")
            continue

        base_map = Image.open(map_path).convert('RGBA')

        # åˆ›å»ºç»˜åˆ¶å±‚
        overlay = Image.new("RGBA", base_map.size, (0, 0, 0, 0))
        draw = ImageDraw.Draw(overlay)

        fps_img_list = []

        for i, item in enumerate(items_to_show):
            color = color_list[i % len(color_list)]

            # --- Draw GT (Solid Circle) ---
            # GT x,y å·²ç»æ˜¯ 0-1024 èŒƒå›´ï¼Œç›´æ¥å¯¹åº”å›¾ç‰‡åƒç´  (å‡è®¾åœ°å›¾æ˜¯ 1024x1024)
            # å¦‚æœåœ°å›¾å°ºå¯¸ä¸æ˜¯ 1024ï¼Œéœ€è¦ç¼©æ”¾
            scale_x = base_map.width / 1024.0
            scale_y = base_map.height / 1024.0

            gt_x, gt_y = item['gt']['x'] * scale_x, item['gt']['y'] * scale_y
            pred_x, pred_y = item['pred']['x'] * scale_x, item['pred']['y'] * scale_y

            # GT: Solid Circle
            r_gt = 6
            draw.ellipse((gt_x - r_gt, gt_y - r_gt, gt_x + r_gt, gt_y + r_gt), fill=color, outline='black', width=1)

            # --- Draw Pred (Hollow/X or different style) ---
            r_pred = 8
            # ç»˜åˆ¶é¢„æµ‹ç‚¹ (å¸¦é»‘æ¡†çš„åŠé€æ˜ç‚¹)
            draw.ellipse((pred_x - r_pred, pred_y - r_pred, pred_x + r_pred, pred_y + r_pred), fill=None, outline=color, width=3)
            # å¯é€‰ï¼šç»˜åˆ¶è¿çº¿
            draw.line([(gt_x, gt_y), (pred_x, pred_y)], fill='white', width=2)

            # --- Collect FPS Image ---
            # åœ¨ FPS å›¾ä¸Šç”»ä¸ªæ¡†æ ‡è®°é¢œè‰²å¯¹åº”å…³ç³»
            fps_img = item['fps'].convert('RGB').copy()
            fps_draw = ImageDraw.Draw(fps_img)
            fps_draw.rectangle((0, 0, 40, 40), fill=color[:3]) # å·¦ä¸Šè§’è‰²å—
            fps_img_list.append(fps_img)

        # 2. åˆæˆ Radar + Overlay
        combined_map = Image.alpha_composite(base_map, overlay).convert('RGB')

        # 3. åˆæˆ FPS Strip
        fps_strip = vertical_concat(fps_img_list)

        # 4. æœ€ç»ˆæ¨ªå‘æ‹¼æ¥
        final_vis = concat_images_horizontal_resize(combined_map, fps_strip)

        # ä¿å­˜
        save_path = os.path.join(output_dir, f"vis_map_{map_name}.jpg")
        final_vis.save(save_path)
        print(f"   -> Saved: {save_path}")


####### ä»unified_task_dataset.pyå¼•å…¥å’Œè®­ç»ƒä¸€è‡´çš„é¢„å¤„ç†å‡½æ•°
####### ç®€å•çš„é¢„å¤„ç†è¾…åŠ©ç±»æ— æ³•å‡†ç¡®æ‹¼æ¥UniLIPçš„ <image> token
import copy
from csgo_datasets.unified_task_dataset import get_loc_prompt, preprocess_multimodal, preprocess, img_process
# ==========================================
# 3. è¾…åŠ©ç±»ä¸ Dataset (ç²¾ç®€ç‰ˆ)
# ==========================================
# # ... (Prompt æ¨¡æ¿ä»£ç åŒå‰) ...
# def get_loc_prompt(map_name):
#     # return f"Task: The following visual data of CS2 map '{map_name}' has been fused... Predict the 5D pose...\n<image>\n<image>"
#     LOC_PROMPT_TEMPLATE = (
#         f"Task: The following visual data of CS2 map '{map_name}' has been fused and inserted into this sequence:\n"
#         "1. First-Person View (FPV) Features.\n"
#         "2. Overhead Radar Map (RADAR) Features.\n"
#         "Analyze the spatial relationship between the FPV and the RADAR Map to determine the precise camera pose. "
#         "Predict the 5D pose (x, y, z, pitch, yaw) in the required format.\n"
#         "<image>\n<image>"
#     )
#     return LOC_PROMPT_TEMPLATE

# def add_template_for_loc(prompt_text):
#     return f"<|im_start|>user\n{prompt_text}<|im_end|>\n<|im_start|>assistant\n"

# æ¨¡æ‹Ÿ ModelArguments
class InferenceArgs:
    def __init__(self, config_dict):
        self.__dict__.update(config_dict)
        ##### model_args
        # è¡¥å…… initialize_vision_modules éœ€è¦çš„é»˜è®¤å‚æ•° (å¦‚æœ yaml é‡Œæ²¡æœ‰)
        self.unilip_path = config_dict.get("unilip_path", "")
        self.unilip_factor = config_dict.get("unilip_factor", 5.85)
        self.fix_dit = False
        self.fix_connect = False
        self.mllm_path = config_dict.get("mllm_path", "")
        self.mllm_hf_path = config_dict.get("mllm_hf_path", "OpenGVLab/InternVL3-1B-hf")
        self.vae_path = config_dict.get("vae_path", "")
        self.dit_path = config_dict.get("dit_path", "")
        self.n_query = 256
        self.connect_layer = 6
        # è¡¥å…… initialize_localization_modules éœ€è¦çš„å‚æ•°
        self.action_horizon = 1
        self.action_dim = 5
        self.is_action_dit_dense_timestep = config_dict.get("is_action_dit_dense_timestep", False)
        self.action_dit_layer = config_dict.get("action_dit_layer", 3)
        # å…¶ä»–æ‚é¡¹
        self.mm_use_im_patch_token = config_dict.get("mm_use_im_patch_token", False)
        self.mm_use_im_start_end = config_dict.get("mm_use_im_start_end", False)
        self.tune_mm_mlp_adapter = False
        self.pretrain_mm_mlp_adapter = None
        self.version = "internvl"

        ##### data_args
        self.image_aspect_ratio = "square"
        ##### training_args
        self.model_max_length = 1024
        self.is_action_dit_projector = config_dict.get("is_action_dit_projector", False)
        self.is_loc_learnable_query = config_dict.get("is_loc_learnable_query", False)


# Dataset
class CSGOLocInferenceDataset(Dataset):
    def __init__(self, config, tokenizer, map_path_dict, image_processor, image_aspect_ratio):
        self.config = config
        self.tokenizer = tokenizer
        self.data_dir = config['data_dir']
        self.map_names = config['val_maps']
        self.map_path_dict = map_path_dict
        self.image_processor = image_processor
        self.image_aspect_ratio = image_aspect_ratio
        self.data_entries = []
        self.map_z_range = {}

        for map_name in self.map_names:
            split_path = f"{self.data_dir}/{map_name}/splits_20000_5000/test_split.json"
            if not os.path.exists(split_path): continue
            with open(split_path, "r", encoding="utf-8") as f: positions_data = json.load(f)
            zs = [d['z'] for d in positions_data]
            self.map_z_range[map_name] = {'max_z': max(zs), 'min_z': min(zs)}
            for pos_data in positions_data:
                self.data_entries.append({
                    'map': map_name, 'file_frame': pos_data['file_frame'],
                    'x': pos_data['x'], 'y': pos_data['y'], 'z': pos_data['z'],
                    'angle_v': pos_data['angle_v'], 'angle_h': pos_data['angle_h']
                })

        if config['debug'] and config.get('debug_num_val_data', False):
            sampled_num = config.get('debug_num_val_data', len(self.data_entries))
            self.data_entries = self.data_entries[:sampled_num]
            print([data['file_frame'] for data in self.data_entries])
        elif config['debug'] and config.get('debug_num_val_data', False) == False:
            indices = [335, 535, 707, 288, 21, 240, 20, 30, 809, 423, 857, 459, 557, 882, 893, 406, 24, 477, 407, 427, 453, 923, 925, 399, 752, 867, 547, 563, 424, 217, 789, 681]
            self.data_entries = [self.data_entries[i] for i in indices]
            print([data['file_frame'] for data in self.data_entries])
        elif config['debug']==False and config.get('debug_num_val_data', False):
            sampled_num = config.get('debug_num_val_data', len(self.data_entries))
            self.data_entries = random.sample(self.data_entries, sampled_num)
            print([data['file_frame'] for data in self.data_entries])

        # é¢„åŠ è½½æ‰€æœ‰åœ°å›¾å›¾ç‰‡åˆ°å†…å­˜
        self.map_images = {}
        for map_name, filename in map_path_dict.items():
            path = f"{config['data_dir']}/{map_name}/{filename}"
            if os.path.exists(path):
                img = Image.open(path).convert('RGB')
                # é¢„å…ˆåš Resize ä»¥çœå†…å­˜ (å¦‚æœ processor éœ€è¦ 448)
                # img = img.resize((448, 448))
                self.map_images[map_name] = img
            else:
                print(f"Map image not found: {path}")

        # ä»…å–å‰Nä¸ªåšæµ‹è¯•ï¼Œé¿å…è·‘å¤ªä¹… (å¯é€‰)
        # self.data_entries = self.data_entries[:50]
        print(f"âœ… Loaded {len(self.data_entries)} test samples.")

    def __len__(self): return len(self.data_entries)

    def __getitem__(self, i):
        data = self.data_entries[i]
        map_name = data['map']
        # map_img_path = f"{self.data_dir}/{map_name}/{self.map_path_dict.get(map_name, 'de_dust2_radar_psd.png')}"
        # map_img = Image.open(map_img_path).convert('RGB')
        map_img = self.map_images.get(map_name).copy()
        ext = ".jpg" if "preprocessed" in self.data_dir else ".png"
        fps_path = f"{self.data_dir}/{map_name}/imgs/{data['file_frame']}{ext}"
        fps_img = Image.open(fps_path).convert('RGB')
        all_images = [fps_img, map_img]
        process_images = img_process(
            all_images,
            self.image_processor,
            self.image_aspect_ratio
        ) # shape: [2, C, H, W]
        # æ‹†åˆ†å‡º Tensor
        tensor_fps = process_images[:-1] # [1, C, H, W]
        tensor_map = process_images[-1:] # [1, C, H, W]

        # --- å‡†å¤‡ Prompt æ•°æ® ---
        z_min = self.map_z_range[map_name]['min_z']
        z_max = self.map_z_range[map_name]['max_z']
        z_norm = (data['z'] - z_min) / (z_max - z_min + 1e-6)

        # è§’åº¦å½’ä¸€åŒ– (0 ~ 1)
        angle_h_norm = data['angle_h'] / (2 * np.pi) # Yaw
        angle_v_norm = data['angle_v'] / (2 * np.pi) # Pitch

        # æ„é€  normalized GT Tensor [x, y, z, pitch, yaw]
        # x, y åŸæœ¬æ˜¯ 0-1024ï¼Œå½’ä¸€åŒ–é™¤ä»¥ 1024
        loc_list = [data['x'] / 1024.0, data['y'] / 1024.0, z_norm, angle_v_norm, angle_h_norm]
        gt_norm_tensor = torch.tensor(loc_list, dtype=torch.float32)

        pose_dict = {
            'x': data['x'], 'y': data['y'], 'z': data['z'],
            'angle_v': (data['angle_v'] / (2 * np.pi)) * 360.0,
            'angle_h': (data['angle_h'] / (2 * np.pi)) * 360.0
        }

        # Text Prompt
        user_text_loc = get_loc_prompt(map_name)
        sources_loc = {
            "conversations": [
                {"from": "human", "value": user_text_loc},
                {"from": "gpt", "value": ""} # Assistant å›å¤ä½ç½®Token
            ]
        }
        # Tokenize Loc
        sources_loc, _ = preprocess_multimodal(copy.deepcopy([sources_loc["conversations"]]))
        pre_dict_loc = preprocess(sources_loc, self.tokenizer, has_image=True)

        return {
            "task_id": 0,
            "map_name": map_name,
            "ids": data['file_frame'],
            "und_image": tensor_fps,   # Input: FPS
            "aux_image": tensor_map,   # Aux: Map
            "input_ids": pre_dict_loc["input_ids"][0],
            "labels": pre_dict_loc["labels"][0],
            "actions": gt_norm_tensor,

            "raw_prompt": user_text_loc,
            "map_img": map_img,
            "fps_img": fps_img,
            "pose_dict": pose_dict,
            "z_range": self.map_z_range[map_name]
        }

# def collate_fn(batch): return batch

from typing import Dict, Sequence, Union, List
from dataclasses import dataclass
import transformers
from unilip.constants import IGNORE_INDEX, DEFAULT_IMAGE_TOKEN, IMAGE_TOKEN_IDX

@dataclass
class DataCollatorForLoc(object):
    """
    Collate examples for UniLIP Multi-Task (Localization + Generation).
    Adapts inputs based on task_id.
    """
    tokenizer: transformers.PreTrainedTokenizer

    def __call__(self, instances: Sequence[Union[Dict, List[Dict]]]) -> Dict[str, torch.Tensor]:
        # 1. æå–åŸºç¡€æ•°æ®
        task_id_list = []
        input_ids_list = []
        labels_list = []
        ids_list = []
        batch_und_images = []
        batch_aux_images = []
        batch_actions = []
        batch_raw_prompt_list = []
        batch_map_img_list = []
        batch_fps_img_list = []
        batch_z_range_list = []
        batch_map_name_list = []
        batch_pose_dict_list = []

        # 2. é€æ ·æœ¬å¤„ç†
        for instance in instances:
            task_id = instance.get("task_id", 0) # é»˜è®¤ä¸º0(å®šä½)ä»¥é˜²ä¸‡ä¸€
            _input_id = instance["input_ids"]
            _label = instance["labels"]
            _id = instance.get("ids", "unknown")

            # === Token å¤„ç†é€»è¾‘ ===
            # ä¸ºäº†é˜²æ­¢è¶…è¿‡æ¨¡å‹æœ€å¤§é•¿åº¦ï¼Œå…ˆåšæˆªæ–­ (é¢„ç•™ 257 ä¸ªä½ç½®ç»™ç”Ÿæˆ Token)
            # UniLIP åŸé€»è¾‘ï¼šinput_id[: max_len - 257]
            safe_len = self.tokenizer.model_max_length - 257
            _input_id = _input_id[:safe_len]
            _label = _label[:safe_len]

            task_id_list.append(task_id)
            input_ids_list.append(_input_id)
            labels_list.append(_label)
            ids_list.append(_id)

            # === æ”¶é›† Tensor æ•°æ® ===
            if "und_image" in instance and instance["und_image"] is not None:
                batch_und_images.append(instance["und_image"])
            # [NEW] æ”¶é›† Aux Image (è¾…åŠ©å›¾/Map)
            if "aux_image" in instance and instance["aux_image"] is not None:
                batch_aux_images.append(instance["aux_image"])
            # [NEW] æ”¶é›† Actions
            if "actions" in instance and instance["actions"] is not None:
                batch_actions.append(instance["actions"]) # [1, 5]
            if "raw_prompt" in instance:
                batch_raw_prompt_list.append(instance["raw_prompt"])
            if "map_name" in instance:
                batch_map_name_list.append(instance["map_name"])
            if "pose_dict" in instance:
                batch_pose_dict_list.append(instance["pose_dict"])
            if "map_img" in instance:
                batch_map_img_list.append(instance["map_img"])
            if "fps_img" in instance:
                batch_fps_img_list.append(instance["fps_img"])
            if "z_range" in instance:
                batch_z_range_list.append(instance["z_range"])

        # 3. Padding (Pad Input Ids & Labels)
        # batch_first=True -> [BS, Seq]
        input_ids = torch.nn.utils.rnn.pad_sequence(
            input_ids_list, batch_first=True, padding_value=self.tokenizer.pad_token_id
        )
        labels = torch.nn.utils.rnn.pad_sequence(
            labels_list, batch_first=True, padding_value=IGNORE_INDEX
        )

        # å†æ¬¡æ£€æŸ¥æœ€å¤§é•¿åº¦ (Padding åå¯èƒ½ä¼šå˜é•¿)
        if input_ids.shape[1] > self.tokenizer.model_max_length:
            print(f"Input length {input_ids.shape[1]} > {self.tokenizer.model_max_length}, truncating.")
            input_ids = input_ids[:, :self.tokenizer.model_max_length]
            labels = labels[:, :self.tokenizer.model_max_length]

        # 4. æ„å»º Batch å­—å…¸
        batch = dict(
            input_ids=input_ids,
            labels=labels,
            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),
            ids=ids_list,
        )

        # 5. å †å å›¾åƒ (Stack Images)
        # è¾…åŠ©å‡½æ•°ï¼šå®‰å…¨å †å 
        def stack_images(img_list):
            if len(img_list) == 0:
                return None
            # æ£€æŸ¥å½¢çŠ¶ä¸€è‡´æ€§
            if all(x is not None and x.shape == img_list[0].shape for x in img_list):
                return torch.cat(img_list, dim=0) # [BS, C, H, W] (å› ä¸º Dataset è¿”å›çš„æ˜¯ [1, C, H, W])
            else:
                print("Image shapes inconsistent in batch, returning list instead of tensor.")
                return img_list

        batch["und_image"] = stack_images(batch_und_images)
        batch["aux_image"] = stack_images(batch_aux_images)

        # 6. å †å å…¶ä»– Tensor (Actions, Mask)
        if len(batch_actions) > 0:
            batch["actions"] = torch.stack(batch_actions, dim=0) # [BS, 1, 5]
        else:
            batch["actions"] = None

        if len(task_id_list) > 0:
            batch["task_id"] = torch.tensor(task_id_list, dtype=torch.long)
        if len(batch_raw_prompt_list) > 0:
            batch["raw_prompt"] = batch_raw_prompt_list
        if len(batch_map_name_list) > 0:
            batch["map_name"] = batch_map_name_list
        if len(batch_pose_dict_list) > 0:
            batch["pose_dict"] = batch_pose_dict_list
        if len(batch_map_img_list) > 0:
            batch["map_img"] = batch_map_img_list
        if len(batch_fps_img_list) > 0:
            batch["fps_img"] = batch_fps_img_list
        if len(batch_z_range_list) > 0:
            batch["z_range"] = batch_z_range_list

        return batch



def unnormalize_pose(pred_tensor, z_range):
    x_norm, y_norm, z_norm, v_norm, h_norm = pred_tensor.cpu().numpy()
    return {
        'x': x_norm * 1024.0, 'y': y_norm * 1024.0,
        'z': z_norm * (z_range['max_z'] - z_range['min_z'] + 1e-6) + z_range['min_z'],
        'angle_v': v_norm * 360.0, 'angle_h': h_norm * 360.0
    }

def smart_tokenizer_and_embedding_resize(special_tokens_dict, tokenizer, model):
    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)
    model.resize_token_embeddings(len(tokenizer))
    if num_new_tokens > 0:
        input_embeddings = model.get_input_embeddings().weight.data
        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)
        input_embeddings[-num_new_tokens:] = input_embeddings_avg

map_path_dict = {
    'de_dust2': 'de_dust2_radar_psd.png',
    'de_inferno': 'de_inferno_radar_psd.png',
    'de_mirage': 'de_mirage_radar_psd.png',
    'de_nuke': 'de_nuke_blended_radar_psd.png',
    'de_ancient': 'de_ancient_radar_psd.png',
    'de_anubis': 'de_anubis_radar_psd.png',
    'de_golden': 'de_golden_radar_tga.png',
    'de_overpass': 'de_overpass_radar_psd.png',
    'de_palacio': 'de_palacio_radar_tga.png',
    'de_train': 'de_train_blended_radar_psd.png',
    'de_vertigo': 'de_vertigo_blended_radar_psd.png',
    'cs_agency': 'cs_agency_radar_tga.png',
    'cs_italy': 'cs_italy_radar_psd.png',
    'cs_office': 'cs_office_radar_psd.png',
}

# ==========================================
# 4. ä¸»ç¨‹åº
# ==========================================
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--csgo_config", type=str, required=True)
    args = parser.parse_args()
    with open(args.csgo_config, 'r') as f:
        csgo_config = yaml.safe_load(f)

    disable_torch_init()
    inference_args = InferenceArgs(csgo_config)

    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    set_seed()

    cur_time_str = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = f"outputs_loc/{args.csgo_config.split('/')[-1][:-5]}/test_{cur_time_str}"
    os.makedirs(output_dir, exist_ok=True)

    # 1. Init Model
    model = Unified_UniLIP_InternVLForCausalLM.from_pretrained(
        csgo_config.get('model_name_or_path', 'UniLIP-1B'),
        torch_dtype=torch.bfloat16, low_cpu_mem_usage=True, trust_remote_code=True
    )

    # 2. Init Tokenizer
    tokenizer = AutoProcessor.from_pretrained(inference_args.mllm_hf_path).tokenizer
    tokenizer.model_max_length = inference_args.model_max_length
    if tokenizer.pad_token is None:
        smart_tokenizer_and_embedding_resize(
            special_tokens_dict=dict(
                pad_token="<pad>",
                additional_special_tokens=["[IMG]", "[/IMG]", "<image>"],
            ),
            tokenizer=tokenizer,
            model=model,
        )
    elif not "<image>" in tokenizer.get_added_vocab():
        smart_tokenizer_and_embedding_resize(
            special_tokens_dict=dict(additional_special_tokens=["[IMG]", "[/IMG]", "<image>"]),
            tokenizer=tokenizer,
            model=model,
        )
    from unilip import conversation as conversation_lib
    if inference_args.version in conversation_lib.conv_templates:
        conversation_lib.default_conversation = conversation_lib.conv_templates[inference_args.version]
    else:
        conversation_lib.default_conversation = conversation_lib.conv_templates["llama3"]
    print(f"Using conversation format: {conversation_lib.default_conversation.version}")

    # 3. Init Action Modules
    model.config.is_action_dit_dense_timestep = inference_args.is_action_dit_dense_timestep
    model.config.is_exp5_eval_without_aciton_dit_premodules = inference_args.is_exp5_eval_without_aciton_dit_premodules
    model.config.is_action_dit_projector =  inference_args.is_action_dit_projector
    model.config.is_loc_learnable_query =  inference_args.is_loc_learnable_query

    model.get_model().initialize_vision_modules(model_args=inference_args)
    model.get_model().initialize_localization_modules(model_args=inference_args)

    # 4. Init å…¶ä»–é…ç½®
    image_processor = AutoProcessor.from_pretrained(inference_args.mllm_hf_path).image_processor
    image_aspect_ratio = inference_args.image_aspect_ratio

    # 5. Init vision tokenizer
    # è®­ç»ƒæ—¶ï¼Œé»˜è®¤é…ç½®ä½¿å¾—initialize_vision_tokenizer()ä¸­æ‰€æœ‰é…ç½®éƒ½ä¸ç”Ÿæ•ˆï¼Œå› æ­¤çœç•¥evalè¿™é‡Œçš„åˆå§‹åŒ–
    # model.initialize_vision_tokenizer(model_args, tokenizer=tokenizer)

    # 6. Load Weights
    ckpt_path = csgo_config['ckpt_path']
    print(f"ğŸ“¥ Loading Checkpoint: {ckpt_path}")
    if ckpt_path.endswith(".safetensors"):
        state_dict = safe_load_file(ckpt_path, device="cpu")
    else:
        state_dict = torch.load(ckpt_path, map_location="cpu")
    msg = model.load_state_dict(state_dict, strict=False)
    print(msg)

    # 7. Set Model Eval
    model.to(device=device, dtype=torch.bfloat16)
    model.eval()
    model.config.use_cache = True

    # 8. Init Localization Inference Dataset
    test_dataset = CSGOLocInferenceDataset(csgo_config, tokenizer, map_path_dict, image_processor, image_aspect_ratio)
    data_collator = DataCollatorForLoc(tokenizer=tokenizer)
    dataloader = DataLoader(test_dataset, batch_size=csgo_config["batch_size"], shuffle=False, collate_fn=data_collator)

    # 9. Inference Loop
    results_json = []
    vis_data_grouped = defaultdict(list) # æŒ‰åœ°å›¾åˆ†ç»„å­˜å‚¨å¯è§†åŒ–æ•°æ®

    print("ğŸš€ Starting Localization Inference...")
    for batch in tqdm(dataloader):
        map_name = batch["map_name"]
        ids = batch["ids"]

        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)
        und_image = batch["und_image"].to(device)
        aux_image = batch["aux_image"].to(device)

        actions = batch["actions"]
        pose_dict = batch["pose_dict"]

        raw_prompt = batch["raw_prompt"]
        map_img = batch["map_img"]
        fps_img = batch["fps_img"]
        z_range = batch["z_range"]

        with torch.no_grad():
            pred_norm_loc_tensor = model.generate_action2(
                input_ids,
                attention_mask,
                labels,
                und_image,
                aux_image,
                num_steps=10
            ).squeeze(1).float().cpu()

        for sample_idx in range(csgo_config["batch_size"]):
            # 1. å½’ä¸€åŒ–æ•°æ®
            pred_norm = pred_norm_loc_tensor[sample_idx]
            gt_norm = actions[sample_idx]

            # 2. ç‰©ç†æ•°æ® (ç”¨äºå¯è§†åŒ–)
            pred_phys = unnormalize_pose(pred_norm, z_range[sample_idx])
            gt_phys = pose_dict[sample_idx]

            # 3. å­˜å‚¨
            results_json.append({
                "file_frame": ids[sample_idx],
                "map": map_name[sample_idx],
                "pred_norm": pred_norm.tolist(), # Save as list
                "gt_norm": gt_norm.tolist(),   # Save as list
                "pred": pred_phys,
                "gt": gt_phys,
            })

            vis_data_grouped[map_name[sample_idx]].append({
                "fps": fps_img[sample_idx], "gt": gt_phys, "pred": pred_phys
            })

    # 3. Calculate Metrics (L2 5D, SmoothL1, etc.)
    print("ğŸ“ˆ Calculating Metrics...")
    metrics = calculate_metrics(results_json)
    print(json.dumps(metrics, indent=4))

    # Save Metrics & Results
    with open(os.path.join(output_dir, "loc_metrics.json"), "w") as f:
        json.dump(metrics, f, indent=4)
    with open(os.path.join(output_dir, "loc_results.json"), "w") as f:
        json.dump(results_json, f, indent=4)

    # 4. Visualization (Pi0.5 Style)
    visualize_map_results(
        vis_data_grouped,
        output_dir,
        map_path_dict,
        csgo_config['data_dir'],
        vis_num_per_map=10 # æ¯å¼ åœ°å›¾éšæœºé€‰5ä¸ªæ ·æœ¬å¯è§†åŒ–
    )

    print(f"âœ… Finished. Results at: {output_dir}")

if __name__ == "__main__":
    main()

# python eval_csgo_loc.py --csgo_config csgo_configs/test/exp3_1_loc.yaml

